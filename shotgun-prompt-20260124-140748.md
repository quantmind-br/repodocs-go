# Performance Optimizations Analysis

<!--
  Analyzes codebase to identify performance bottlenecks, optimization
  opportunities, and efficiency improvements across bundle size,
  runtime, memory, database, and network.

  VARIABLES:
  - └── repodocs-go/
    ├── build/
    ├── cmd/
    │   └── repodocs/
    │       └── main.go [15.2KB]
    ├── configs/
    │   └── config.yaml.template [4.6KB]
    ├── coverage/
    ├── e2e_output/
    ├── examples/
    │   └── manifests/
    │       ├── error-tolerant.yaml [180B]
    │       ├── full-options.yaml [573B]
    │       ├── multi-source.yaml [347B]
    │       └── simple.yaml [43B]
    ├── internal/
    │   ├── app/
    │   │   ├── detector.go [5.0KB]
    │   │   └── orchestrator.go [9.6KB]
    │   ├── cache/
    │   │   ├── badger.go [3.3KB]
    │   │   ├── interface.go [1.0KB]
    │   │   └── keys.go [1.8KB]
    │   ├── config/
    │   │   ├── config.go [4.9KB]
    │   │   ├── defaults.go [3.6KB]
    │   │   └── loader.go [4.9KB]
    │   ├── converter/
    │   │   ├── content_type.go [1.6KB]
    │   │   ├── encoding.go [2.3KB]
    │   │   ├── markdown.go [4.8KB]
    │   │   ├── markdown_reader.go [6.6KB]
    │   │   ├── pipeline.go [6.1KB]
    │   │   ├── plaintext_reader.go [3.3KB]
    │   │   ├── readability.go [6.7KB]
    │   │   └── sanitizer.go [5.5KB]
    │   ├── domain/
    │   │   ├── errors.go [5.9KB]
    │   │   ├── interfaces.go [3.3KB]
    │   │   ├── models.go [8.7KB]
    │   │   └── options.go [392B]
    │   ├── fetcher/
    │   │   ├── client.go [6.2KB]
    │   │   ├── retry.go [3.7KB]
    │   │   ├── stealth.go [4.7KB]
    │   │   └── transport.go [1.6KB]
    │   ├── git/
    │   │   ├── client.go [470B]
    │   │   └── interface.go [257B]
    │   ├── llm/
    │   │   ├── anthropic.go [6.6KB]
    │   │   ├── circuit_breaker.go [3.4KB]
    │   │   ├── google.go [6.1KB]
    │   │   ├── metadata.go [6.7KB]
    │   │   ├── openai.go [5.2KB]
    │   │   ├── provider.go [2.2KB]
    │   │   ├── provider_wrapper.go [3.9KB]
    │   │   ├── ratelimit.go [2.2KB]
    │   │   └── retry.go [4.1KB]
    │   ├── manifest/
    │   │   ├── doc.go [1.2KB]
    │   │   ├── errors.go [768B]
    │   │   ├── loader.go [1.6KB]
    │   │   └── types.go [1.9KB]
    │   ├── output/
    │   │   ├── collector.go [2.5KB]
    │   │   └── writer.go [3.1KB]
    │   ├── renderer/
    │   │   ├── detector.go [3.9KB]
    │   │   ├── pool.go [2.2KB]
    │   │   ├── rod.go [6.8KB]
    │   │   └── stealth.go [1.7KB]
    │   ├── state/
    │   │   ├── errors.go [415B]
    │   │   ├── manager.go [3.3KB]
    │   │   └── models.go [1.6KB]
    │   ├── strategies/
    │   │   ├── git/
    │   │   │   ├── archive.go [3.7KB]
    │   │   │   ├── clone.go [2.0KB]
    │   │   │   ├── doc.go [690B]
    │   │   │   ├── fetcher.go [167B]
    │   │   │   ├── parser.go [3.4KB]
    │   │   │   ├── processor.go [4.0KB]
    │   │   │   ├── strategy.go [5.9KB]
    │   │   │   └── types.go [1.4KB]
    │   │   ├── crawler.go [6.9KB]
    │   │   ├── docsrs.go [8.9KB]
    │   │   ├── docsrs_json.go [3.9KB]
    │   │   ├── docsrs_renderer.go [13.2KB]
    │   │   ├── docsrs_types.go [15.6KB]
    │   │   ├── git_compat.go [3.9KB]
    │   │   ├── github_pages.go [14.2KB]
    │   │   ├── github_pages_discovery.go [8.9KB]
    │   │   ├── llms.go [6.3KB]
    │   │   ├── pkggo.go [4.7KB]
    │   │   ├── sitemap.go [9.2KB]
    │   │   ├── strategy.go [9.8KB]
    │   │   ├── wiki.go [7.3KB]
    │   │   └── wiki_parser.go [7.6KB]
    │   ├── tui/
    │   │   ├── app.go [7.6KB]
    │   │   ├── categories.go [1.6KB]
    │   │   ├── config_adapter.go [9.6KB]
    │   │   ├── forms.go [10.5KB]
    │   │   ├── styles.go [2.0KB]
    │   │   └── validation.go [3.4KB]
    │   └── utils/
    │       ├── fs.go [6.6KB]
    │       ├── logger.go [2.2KB]
    │       ├── progress.go [1.5KB]
    │       ├── url.go [10.6KB]
    │       └── workerpool.go [5.0KB]
    ├── pkg/
    │   └── version/
    │       └── version.go [1.0KB]
    ├── scripts/
    │   └── release.sh [2.3KB]
    ├── tests/
    ├── AGENTS.md [4.1KB]
    ├── CLAUDE.md [1.7KB]
    ├── LICENSE [994B]
    ├── Makefile [2.4KB]
    ├── README.md [10.2KB]
    ├── TESTING.md [6.4KB]
    ├── go.mod [5.9KB]
    ├── go.sum [35.7KB]
    └── opencode.json [338B]

<file path="cmd/repodocs/main.go">
package main

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"os/exec"
	"os/signal"
	"syscall"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/app"
	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/manifest"
	"github.com/quantmind-br/repodocs-go/internal/tui"
	"github.com/quantmind-br/repodocs-go/internal/utils"
	"github.com/quantmind-br/repodocs-go/pkg/version"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"gopkg.in/yaml.v3"
)

var (
	cfgFile      string
	verbose      bool
	manifestPath string
	log          *utils.Logger

	// Dependencies for testing
	osStat       = os.Stat
	execLookPath = exec.LookPath
)

func main() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		os.Exit(1)
	}
}

var rootCmd = &cobra.Command{
	Use:   "repodocs [url]",
	Short: "Extract documentation from any source",
	Long: `RepoDocs is a CLI tool for extracting documentation from websites,
git repositories, sitemaps, pkg.go.dev, and llms.txt files.

It supports stealth mode for avoiding bot detection and JavaScript rendering
for single-page applications.`,
	Version: version.Short(),
	Args:    cobra.MaximumNArgs(1),
	RunE:    run,
}

func init() {
	cobra.OnInitialize(initConfig)

	// Global flags
	rootCmd.PersistentFlags().StringVar(&cfgFile, "config", "", "config file (default is ~/.repodocs/config.yaml)")
	rootCmd.PersistentFlags().StringP("output", "o", "./docs", "Output directory")
	rootCmd.PersistentFlags().IntP("concurrency", "j", 5, "Number of concurrent workers")
	rootCmd.PersistentFlags().IntP("limit", "l", 0, "Max pages to process (0=unlimited)")
	rootCmd.PersistentFlags().IntP("max-depth", "d", 4, "Max crawl depth")
	rootCmd.PersistentFlags().StringSlice("exclude", nil, "Regex patterns to exclude")
	rootCmd.PersistentFlags().String("filter", "", "Path filter (web: base URL; git: subdirectory)")
	rootCmd.PersistentFlags().Bool("nofolders", false, "Flat output structure")
	rootCmd.PersistentFlags().Bool("force", false, "Overwrite existing files")
	rootCmd.PersistentFlags().BoolVarP(&verbose, "verbose", "v", false, "Verbose output")

	// Cache flags
	rootCmd.PersistentFlags().Bool("no-cache", false, "Disable caching")
	rootCmd.PersistentFlags().Duration("cache-ttl", 24*time.Hour, "Cache TTL")
	rootCmd.PersistentFlags().Bool("refresh-cache", false, "Force cache refresh")

	// Rendering flags
	rootCmd.PersistentFlags().Bool("render-js", false, "Force JS rendering")
	rootCmd.PersistentFlags().Duration("timeout", 30*time.Second, "Request timeout")

	// Output flags
	rootCmd.PersistentFlags().Bool("json-meta", false, "Generate JSON metadata files")
	rootCmd.PersistentFlags().Bool("dry-run", false, "Simulate without writing files")

	// Specific flags
	rootCmd.PersistentFlags().Bool("split", false, "Split output by sections (pkg.go.dev)")
	rootCmd.PersistentFlags().Bool("include-assets", false, "Include referenced images (git)")
	rootCmd.PersistentFlags().String("user-agent", "", "Custom User-Agent")
	rootCmd.PersistentFlags().String("content-selector", "", "CSS selector for main content")
	rootCmd.PersistentFlags().String("exclude-selector", "", "CSS selector for elements to exclude from content")
	rootCmd.PersistentFlags().StringVar(&manifestPath, "manifest", "", "Path to manifest file (YAML/JSON) for batch processing")

	// Sync flags
	rootCmd.PersistentFlags().Bool("sync", false, "Enable incremental sync mode (skip unchanged pages)")
	rootCmd.PersistentFlags().Bool("full-sync", false, "Force full re-processing (ignore state)")
	rootCmd.PersistentFlags().Bool("prune", false, "Remove files for deleted pages")

	// Bind flags to viper
	_ = viper.BindPFlag("output.directory", rootCmd.PersistentFlags().Lookup("output"))
	_ = viper.BindPFlag("concurrency.workers", rootCmd.PersistentFlags().Lookup("concurrency"))
	_ = viper.BindPFlag("concurrency.max_depth", rootCmd.PersistentFlags().Lookup("max-depth"))
	_ = viper.BindPFlag("concurrency.timeout", rootCmd.PersistentFlags().Lookup("timeout"))
	_ = viper.BindPFlag("output.flat", rootCmd.PersistentFlags().Lookup("nofolders"))
	_ = viper.BindPFlag("output.overwrite", rootCmd.PersistentFlags().Lookup("force"))
	_ = viper.BindPFlag("cache.enabled", rootCmd.PersistentFlags().Lookup("no-cache"))
	_ = viper.BindPFlag("cache.ttl", rootCmd.PersistentFlags().Lookup("cache-ttl"))
	_ = viper.BindPFlag("rendering.force_js", rootCmd.PersistentFlags().Lookup("render-js"))
	_ = viper.BindPFlag("output.json_metadata", rootCmd.PersistentFlags().Lookup("json-meta"))
	_ = viper.BindPFlag("stealth.user_agent", rootCmd.PersistentFlags().Lookup("user-agent"))

	// Add subcommands
	rootCmd.AddCommand(doctorCmd)
	rootCmd.AddCommand(versionCmd)
	rootCmd.AddCommand(configCmd)
}

func initConfig() {
	if cfgFile != "" {
		viper.SetConfigFile(cfgFile)
	}
}

func run(cmd *cobra.Command, args []string) error {
	// Initialize logger
	logLevel := "info"
	if verbose {
		logLevel = "debug"
	}
	log = utils.NewLogger(utils.LoggerOptions{
		Level:   logLevel,
		Format:  "pretty",
		Verbose: verbose,
	})

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	if manifestPath != "" {
		if len(args) > 0 {
			return fmt.Errorf("cannot specify both --manifest and URL argument")
		}
		return runManifest(cmd, cfg)
	}

	if len(args) == 0 {
		return cmd.Help()
	}

	url := args[0]

	// Handle output directory:
	// 1. If user explicitly provided -o flag, use that value
	// 2. Otherwise, generate from URL
	if cmd.Flags().Changed("output") {
		// User explicitly set output directory via -o flag
		cfg.Output.Directory, _ = cmd.Flags().GetString("output")
	} else {
		// Use URL-based output directory
		cfg.Output.Directory = utils.GenerateOutputDirFromURL(url)
	}

	// Create context with cancellation
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Handle graceful shutdown
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigCh
		log.Info().Msg("Shutting down gracefully...")
		cancel()
	}()

	limit, _ := cmd.Flags().GetInt("limit")
	dryRun, _ := cmd.Flags().GetBool("dry-run")
	split, _ := cmd.Flags().GetBool("split")
	includeAssets, _ := cmd.Flags().GetBool("include-assets")
	contentSelector, _ := cmd.Flags().GetString("content-selector")
	excludeSelector, _ := cmd.Flags().GetString("exclude-selector")
	excludePatterns, _ := cmd.Flags().GetStringSlice("exclude")
	renderJS, _ := cmd.Flags().GetBool("render-js")
	force, _ := cmd.Flags().GetBool("force")
	filterURL, _ := cmd.Flags().GetString("filter")
	syncEnabled, _ := cmd.Flags().GetBool("sync")
	fullSync, _ := cmd.Flags().GetBool("full-sync")
	prune, _ := cmd.Flags().GetBool("prune")

	orchOpts := app.OrchestratorOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  verbose,
			DryRun:   dryRun,
			Force:    force,
			RenderJS: renderJS,
			Limit:    limit,
			Sync:     syncEnabled,
			FullSync: fullSync,
			Prune:    prune,
		},
		Config:          cfg,
		Split:           split,
		IncludeAssets:   includeAssets,
		ContentSelector: contentSelector,
		ExcludeSelector: excludeSelector,
		ExcludePatterns: excludePatterns,
		FilterURL:       filterURL,
	}

	// Create orchestrator
	orchestrator, err := app.NewOrchestrator(orchOpts)
	if err != nil {
		return fmt.Errorf("failed to create orchestrator: %w", err)
	}
	defer orchestrator.Close()

	// Validate URL
	if err := orchestrator.ValidateURL(url); err != nil {
		return err
	}

	return orchestrator.Run(ctx, url, orchOpts)
}

func runManifest(cmd *cobra.Command, cfg *config.Config) error {
	loader := manifest.NewLoader()
	manifestCfg, err := loader.Load(manifestPath)
	if err != nil {
		return fmt.Errorf("failed to load manifest: %w", err)
	}

	if manifestCfg.Options.Output != "" {
		cfg.Output.Directory = manifestCfg.Options.Output
	}
	if manifestCfg.Options.Concurrency > 0 {
		cfg.Concurrency.Workers = manifestCfg.Options.Concurrency
	}
	if manifestCfg.Options.CacheTTL > 0 {
		cfg.Cache.TTL = manifestCfg.Options.CacheTTL
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigCh
		log.Info().Msg("Shutting down gracefully...")
		cancel()
	}()

	limit, _ := cmd.Flags().GetInt("limit")
	dryRun, _ := cmd.Flags().GetBool("dry-run")
	split, _ := cmd.Flags().GetBool("split")
	includeAssets, _ := cmd.Flags().GetBool("include-assets")
	contentSelector, _ := cmd.Flags().GetString("content-selector")
	excludeSelector, _ := cmd.Flags().GetString("exclude-selector")
	excludePatterns, _ := cmd.Flags().GetStringSlice("exclude")
	renderJS, _ := cmd.Flags().GetBool("render-js")
	force, _ := cmd.Flags().GetBool("force")
	filterURL, _ := cmd.Flags().GetString("filter")
	syncEnabled, _ := cmd.Flags().GetBool("sync")
	fullSync, _ := cmd.Flags().GetBool("full-sync")
	prune, _ := cmd.Flags().GetBool("prune")

	orchOpts := app.OrchestratorOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  verbose,
			DryRun:   dryRun,
			Force:    force,
			RenderJS: renderJS,
			Limit:    limit,
			Sync:     syncEnabled,
			FullSync: fullSync,
			Prune:    prune,
		},
		Config:          cfg,
		Split:           split,
		IncludeAssets:   includeAssets,
		ContentSelector: contentSelector,
		ExcludeSelector: excludeSelector,
		ExcludePatterns: excludePatterns,
		FilterURL:       filterURL,
	}

	orchestrator, err := app.NewOrchestrator(orchOpts)
	if err != nil {
		return fmt.Errorf("failed to create orchestrator: %w", err)
	}
	defer orchestrator.Close()

	return orchestrator.RunManifest(ctx, manifestCfg, orchOpts)
}

var doctorCmd = &cobra.Command{
	Use:   "doctor",
	Short: "Check system dependencies",
	Long:  "Verifies that all system dependencies are properly installed and configured.",
	RunE: func(cmd *cobra.Command, args []string) error {
		fmt.Println("Checking system dependencies...")
		allPassed := true

		// Check 1: Internet connection
		fmt.Print("  Internet connection: ")
		if checkInternet() {
			fmt.Println("OK")
		} else {
			fmt.Println("FAILED")
			allPassed = false
		}

		// Check 2: Chrome/Chromium
		fmt.Print("  Chrome/Chromium: ")
		if chromePath := checkChrome(); chromePath != "" {
			fmt.Printf("OK (%s)\n", chromePath)
		} else {
			fmt.Println("NOT FOUND (JS rendering will be unavailable)")
		}

		// Check 3: Write permissions for output dir
		fmt.Print("  Write permissions: ")
		if checkWritePermissions() {
			fmt.Println("OK")
		} else {
			fmt.Println("FAILED")
			allPassed = false
		}

		// Check 4: Config file
		fmt.Print("  Config file: ")
		_, err := config.Load()
		if err != nil {
			fmt.Printf("WARN (%v)\n", err)
		} else {
			fmt.Println("OK")
		}

		// Check 5: Cache directory
		fmt.Print("  Cache directory: ")
		cacheDir := utils.ExpandPath("~/.repodocs/cache")
		if checkCacheDir(cacheDir) {
			fmt.Printf("OK (%s)\n", cacheDir)
		} else {
			fmt.Println("WARN (will be created on first use)")
		}

		fmt.Println()
		if allPassed {
			fmt.Println("All critical checks passed!")
		} else {
			fmt.Println("Some checks failed. Please resolve the issues above.")
		}
		return nil
	},
}

// checkInternet checks if there's an internet connection
func checkInternet() bool {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, http.MethodHead, "https://www.google.com", nil)
	if err != nil {
		return false
	}

	client := &http.Client{Timeout: 5 * time.Second}
	resp, err := client.Do(req)
	if err != nil {
		return false
	}
	resp.Body.Close()
	return resp.StatusCode < 400
}

// checkChrome checks if Chrome/Chromium is available
func checkChrome() string {
	// Common Chrome/Chromium paths
	paths := []string{
		// Linux
		"/usr/bin/google-chrome",
		"/usr/bin/google-chrome-stable",
		"/usr/bin/chromium",
		"/usr/bin/chromium-browser",
		"/snap/bin/chromium",
		// macOS
		"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
		"/Applications/Chromium.app/Contents/MacOS/Chromium",
		// Windows (via PATH)
		"chrome.exe",
		"chromium.exe",
	}

	for _, path := range paths {
		if _, err := osStat(path); err == nil {
			return path
		}
	}

	// Try to find via which/where command
	if path, err := execLookPath("google-chrome"); err == nil {
		return path
	}
	if path, err := execLookPath("chromium"); err == nil {
		return path
	}
	if path, err := execLookPath("chromium-browser"); err == nil {
		return path
	}

	return ""
}

// checkWritePermissions checks if we can write to the current directory
func checkWritePermissions() bool {
	tmpFile := ".repodocs_test_write"
	f, err := os.Create(tmpFile)
	if err != nil {
		return false
	}
	f.Close()
	os.Remove(tmpFile)
	return true
}

// checkCacheDir checks if the cache directory exists or can be created
func checkCacheDir(path string) bool {
	info, err := os.Stat(path)
	if err != nil {
		return false
	}
	return info.IsDir()
}

var versionCmd = &cobra.Command{
	Use:   "version",
	Short: "Print version information",
	Run: func(cmd *cobra.Command, args []string) {
		fmt.Println(version.Full())
	},
}

var configCmd = &cobra.Command{
	Use:   "config",
	Short: "Manage RepoDocs configuration",
	Long:  "View, edit, and manage RepoDocs configuration interactively.",
	RunE:  runConfigEdit,
}

var configEditCmd = &cobra.Command{
	Use:   "edit",
	Short: "Edit configuration interactively",
	Long:  "Open an interactive TUI to edit RepoDocs configuration.",
	RunE:  runConfigEdit,
}

var configShowCmd = &cobra.Command{
	Use:   "show",
	Short: "Show current configuration",
	Long:  "Display the current configuration in YAML format.",
	RunE:  runConfigShow,
}

var configInitCmd = &cobra.Command{
	Use:   "init",
	Short: "Initialize configuration file",
	Long:  "Create a default configuration file at ~/.repodocs/config.yaml.",
	RunE:  runConfigInit,
}

var configPathCmd = &cobra.Command{
	Use:   "path",
	Short: "Show configuration file path",
	Long:  "Display the path to the configuration file.",
	Run: func(cmd *cobra.Command, args []string) {
		fmt.Println(config.ConfigFilePath())
	},
}

var accessibleMode bool

func init() {
	configCmd.PersistentFlags().BoolVar(&accessibleMode, "accessible", false, "Enable accessible mode for screen readers")
	configCmd.AddCommand(configEditCmd)
	configCmd.AddCommand(configShowCmd)
	configCmd.AddCommand(configInitCmd)
	configCmd.AddCommand(configPathCmd)
}

func runConfigEdit(cmd *cobra.Command, args []string) error {
	cfg, err := config.Load()
	if err != nil {
		cfg = config.Default()
	}

	accessible := accessibleMode || os.Getenv("ACCESSIBLE") != ""

	return tui.Run(tui.Options{
		Config:     cfg,
		SaveFunc:   config.Save,
		Accessible: accessible,
	})
}

func runConfigShow(cmd *cobra.Command, args []string) error {
	cfg, err := config.Load()
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	fmt.Println(string(data))
	return nil
}

func runConfigInit(cmd *cobra.Command, args []string) error {
	path := config.ConfigFilePath()

	if _, err := os.Stat(path); err == nil {
		return fmt.Errorf("config file already exists at %s (use --force to overwrite)", path)
	}

	cfg := config.Default()
	if err := config.Save(cfg); err != nil {
		return fmt.Errorf("failed to create config: %w", err)
	}

	fmt.Printf("Created default configuration at %s\n", path)
	return nil
}
</file>
<file path="configs/config.yaml.template">
# repodocs configuration file
# Location: ~/.repodocs/config.yaml
#
# All settings can be overridden via environment variables with REPODOCS_ prefix
# Example: REPODOCS_LLM_API_KEY, REPODOCS_CACHE_ENABLED

# =============================================================================
# LLM Configuration
# =============================================================================
# Supports: OpenAI-compatible (OpenAI, Ollama, vLLM, Groq, Together AI, etc.),
#           Anthropic, and Google Gemini APIs
#
llm:
  # Provider type: openai, anthropic, google
  # provider: openai

  # API key (recommended: use REPODOCS_LLM_API_KEY env var instead)
  # api_key: sk-...

  # Base URL of the API endpoint
  # Examples:
  #   OpenAI:      https://api.openai.com/v1
  #   Ollama:      http://localhost:11434/v1
  #   vLLM:        http://localhost:8000/v1
  #   Groq:        https://api.groq.com/openai/v1
  #   Together AI: https://api.together.xyz/v1
  #   Anthropic:   https://api.anthropic.com
  #   Google:      https://generativelanguage.googleapis.com
  # base_url: http://localhost:11434/v1

  # Model ID (no default - must be specified by user)
  # model: llama3.2

  # Optional parameters (have sensible defaults)
  max_tokens: 4096
  temperature: 0.7
  timeout: 60s
  max_retries: 3

  # Enable LLM-enhanced metadata generation (summary, tags, category)
  enhance_metadata: false

  # Rate limiting configuration for LLM API requests
  rate_limit:
    # Enable rate limiting (recommended for API quotas)
    enabled: true

    # Maximum requests per minute (adjust based on your API tier)
    # OpenAI: 60-10000+ RPM depending on tier
    # Google Gemini: 60-1000+ RPM depending on plan
    # Anthropic: 60-4000 RPM depending on tier
    requests_per_minute: 60

    # Burst size (max concurrent requests before throttling)
    burst_size: 10

    # Retry configuration for transient errors (429, 5xx)
    max_retries: 3
    initial_delay: 1s
    max_delay: 60s
    multiplier: 2.0

    # Circuit breaker configuration (prevents cascading failures)
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      success_threshold_half_open: 1
      reset_timeout: 30s

# =============================================================================
# Output Configuration
# =============================================================================
output:
  # Default output directory (can be overridden with -o flag)
  directory: "./docs"

  # Flat structure (no subdirectories)
  flat: false

  # Generate JSON metadata files
  json_metadata: false

  # Overwrite existing files
  overwrite: false

# =============================================================================
# Concurrency Configuration
# =============================================================================
concurrency:
  # Number of concurrent workers
  workers: 5

  # Request timeout
  timeout: 30s

  # Maximum crawl depth
  max_depth: 4

# =============================================================================
# Cache Configuration
# =============================================================================
cache:
  # Enable caching (recommended)
  enabled: true

  # Cache time-to-live
  ttl: 24h

  # Cache directory (default: ~/.cache/repodocs)
  # directory: ~/.cache/repodocs

# =============================================================================
# JavaScript Rendering Configuration
# =============================================================================
rendering:
  # Force JavaScript rendering for all pages
  force_js: false

  # Timeout for JavaScript execution
  js_timeout: 10s

  # Scroll to end of page (for infinite scroll sites)
  scroll_to_end: false

# =============================================================================
# Stealth Configuration
# =============================================================================
stealth:
  # Custom User-Agent (empty = use default)
  user_agent: ""

  # Random delay between requests (anti-detection)
  random_delay_min: 500ms
  random_delay_max: 2s

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Log level: debug, info, warn, error
  level: info

  # Log format: text, json
  format: text

# =============================================================================
# URL Exclusion Patterns
# =============================================================================
# Glob patterns for URLs to skip during crawling
exclude:
  - "*/login*"
  - "*/logout*"
  - "*/admin/*"
  - "*.pdf"
  - "*.zip"
  - "*.tar.gz"
</file>
<file path="examples/manifests/error-tolerant.yaml">
sources:
  - url: https://docs.example.com
  - url: https://unstable-service.example.com
  - url: https://api.example.com/docs

options:
  continue_on_error: true
  output: ./docs
</file>
<file path="examples/manifests/full-options.yaml">
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    exclude_selector: ".sidebar, .footer, .nav"
    exclude:
      - "*/changelog/*"
      - "*/archive/*"
    max_depth: 4
    render_js: true
    limit: 100

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "*.md"
    exclude:
      - "docs/internal/*"

  - url: https://pkg.go.dev/github.com/org/repo
    strategy: pkggo

options:
  continue_on_error: true
  output: ./knowledge-base
  concurrency: 5
  cache_ttl: 24h
</file>
<file path="examples/manifests/multi-source.yaml">
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    max_depth: 3

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "README.md"

  - url: https://api.example.com/docs
    strategy: sitemap

options:
  output: ./knowledge-base
  concurrency: 10
</file>
<file path="examples/manifests/simple.yaml">
sources:
  - url: https://docs.example.com
</file>
<file path="internal/app/detector.go">
package app

import (
	"net/url"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/strategies"
)

// StrategyType represents the type of extraction strategy
type StrategyType string

const (
	StrategyLLMS        StrategyType = "llms"
	StrategyPkgGo       StrategyType = "pkggo"
	StrategyDocsRS      StrategyType = "docsrs"
	StrategySitemap     StrategyType = "sitemap"
	StrategyWiki        StrategyType = "wiki"
	StrategyGitHubPages StrategyType = "github_pages"
	StrategyGit         StrategyType = "git"
	StrategyCrawler     StrategyType = "crawler"
	StrategyUnknown     StrategyType = "unknown"
)

var validStrategies = map[StrategyType]bool{
	StrategyLLMS:        true,
	StrategyPkgGo:       true,
	StrategyDocsRS:      true,
	StrategySitemap:     true,
	StrategyWiki:        true,
	StrategyGitHubPages: true,
	StrategyGit:         true,
	StrategyCrawler:     true,
}

func IsValidStrategy(s StrategyType) bool {
	return validStrategies[s]
}

// DetectStrategy determines the appropriate strategy based on URL patterns
func DetectStrategy(rawURL string) StrategyType {
	// Trim whitespace
	rawURL = strings.TrimSpace(rawURL)
	if rawURL == "" {
		return StrategyUnknown
	}

	lower := strings.ToLower(rawURL)

	// Check for SSH Git URLs first (git@host:path/repo.git)
	// These don't parse with url.Parse, so handle them before parsing
	if strings.HasPrefix(rawURL, "git@") || strings.HasPrefix(rawURL, "git+ssh://") {
		return StrategyGit
	}

	// Parse URL to strip query and fragment for path-based matching
	parsed, err := url.Parse(rawURL)
	if err != nil {
		// If URL parsing fails, do basic checks on the raw string
		if strings.HasPrefix(lower, "http://") || strings.HasPrefix(lower, "https://") {
			return StrategyCrawler
		}
		return StrategyUnknown
	}

	// Check if the URL has a valid host (for cases like "https://")
	if parsed.Host == "" && (parsed.Scheme == "http" || parsed.Scheme == "https") {
		return StrategyUnknown
	}

	// For path-based matching, use the path without query/fragment
	path := parsed.Path
	lowerPath := strings.ToLower(path)

	// Check for git:// protocol (unsupported)
	if parsed.Scheme == "git" {
		return StrategyUnknown
	}

	// Check for llms.txt first (using path without query/fragment)
	if strings.HasSuffix(lowerPath, "/llms.txt") || strings.HasSuffix(lowerPath, "llms.txt") {
		return StrategyLLMS
	}

	if strings.Contains(lower, "pkg.go.dev") {
		return StrategyPkgGo
	}

	if strings.Contains(lower, "docs.rs") {
		if !strings.Contains(lowerPath, "/src/") && !strings.Contains(lowerPath, "/source/") {
			return StrategyDocsRS
		}
	}

	if strings.HasSuffix(lowerPath, "sitemap.xml") ||
		strings.HasSuffix(lowerPath, "sitemap.xml.gz") ||
		strings.Contains(lowerPath, "sitemap") && strings.HasSuffix(lowerPath, ".xml") {
		return StrategySitemap
	}

	// Check for Wiki (before generic Git) - pass raw URL to support all wiki patterns
	if strategies.IsWikiURL(rawURL) {
		return StrategyWiki
	}

	// Check for GitHub Pages (*.github.io) - after Wiki, before Git
	if strategies.IsGitHubPagesURL(rawURL) {
		return StrategyGitHubPages
	}

	// Check for Git repository
	// Exclude known documentation/pages subdomains
	isDocsSubdomain := strings.Contains(lower, "docs.github.com") ||
		strings.Contains(lower, "pages.github.io") ||
		strings.Contains(lower, "github.io")

	if !isDocsSubdomain && (strings.HasSuffix(lowerPath, ".git") ||
		(strings.Contains(lower, "github.com") && !strings.Contains(lowerPath, "/blob/")) ||
		(strings.Contains(lower, "gitlab.com") && !strings.Contains(lowerPath, "/-/blob/")) ||
		strings.Contains(lower, "bitbucket.org")) {
		return StrategyGit
	}

	// Default to crawler for HTTP URLs
	if parsed.Scheme == "http" || parsed.Scheme == "https" {
		return StrategyCrawler
	}

	return StrategyUnknown
}

func CreateStrategy(strategyType StrategyType, deps *strategies.Dependencies) strategies.Strategy {
	switch strategyType {
	case StrategyLLMS:
		return strategies.NewLLMSStrategy(deps)
	case StrategyPkgGo:
		return strategies.NewPkgGoStrategy(deps)
	case StrategyDocsRS:
		return strategies.NewDocsRSStrategy(deps)
	case StrategySitemap:
		return strategies.NewSitemapStrategy(deps)
	case StrategyWiki:
		return strategies.NewWikiStrategy(deps)
	case StrategyGitHubPages:
		return strategies.NewGitHubPagesStrategy(deps)
	case StrategyGit:
		return strategies.NewGitStrategy(deps)
	case StrategyCrawler:
		return strategies.NewCrawlerStrategy(deps)
	default:
		return nil
	}
}

func GetAllStrategies(deps *strategies.Dependencies) []strategies.Strategy {
	return []strategies.Strategy{
		strategies.NewLLMSStrategy(deps),
		strategies.NewPkgGoStrategy(deps),
		strategies.NewDocsRSStrategy(deps),
		strategies.NewSitemapStrategy(deps),
		strategies.NewWikiStrategy(deps),
		strategies.NewGitHubPagesStrategy(deps),
		strategies.NewGitStrategy(deps),
		strategies.NewCrawlerStrategy(deps),
	}
}

func FindMatchingStrategy(url string, deps *strategies.Dependencies) strategies.Strategy {
	for _, strategy := range GetAllStrategies(deps) {
		if strategy.CanHandle(url) {
			return strategy
		}
	}
	return nil
}
</file>
<file path="internal/app/orchestrator.go">
package app

import (
	"context"
	"fmt"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/manifest"
	"github.com/quantmind-br/repodocs-go/internal/strategies"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// Orchestrator coordinates the documentation extraction process
type Orchestrator struct {
	config          *config.Config
	deps            *strategies.Dependencies
	logger          *utils.Logger
	strategyFactory func(StrategyType, *strategies.Dependencies) strategies.Strategy
}

// OrchestratorOptions contains options for creating an orchestrator
type OrchestratorOptions struct {
	domain.CommonOptions
	Config           *config.Config
	Split            bool
	IncludeAssets    bool
	ContentSelector  string
	ExcludeSelector  string
	ExcludePatterns  []string
	FilterURL        string
	StrategyFactory  func(StrategyType, *strategies.Dependencies) strategies.Strategy
	StrategyOverride string
}

// NewOrchestrator creates a new orchestrator with the given configuration
func NewOrchestrator(opts OrchestratorOptions) (*Orchestrator, error) {
	cfg := opts.Config

	// Validate config
	if cfg == nil {
		return nil, fmt.Errorf("config is required")
	}

	// Create logger
	logLevel := "info"
	logFormat := "pretty"
	if cfg.Logging.Level != "" {
		logLevel = cfg.Logging.Level
	}
	if cfg.Logging.Format != "" {
		logFormat = cfg.Logging.Format
	}
	if opts.Verbose {
		logLevel = "debug"
	}

	logger := utils.NewLogger(utils.LoggerOptions{
		Level:   logLevel,
		Format:  logFormat,
		Verbose: opts.Verbose,
	})

	// Determine cache directory
	cacheDir := cfg.Cache.Directory
	if cacheDir == "" {
		cacheDir = "~/.repodocs/cache"
	}
	cacheDir = utils.ExpandPath(cacheDir)

	// Create dependencies
	deps, err := strategies.NewDependencies(strategies.DependencyOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  opts.Verbose,
			DryRun:   opts.DryRun,
			Force:    opts.Force || cfg.Output.Overwrite,
			RenderJS: opts.RenderJS,
			Limit:    opts.Limit,
			Sync:     opts.Sync,
			FullSync: opts.FullSync,
			Prune:    opts.Prune,
		},
		Timeout:         cfg.Concurrency.Timeout,
		EnableCache:     cfg.Cache.Enabled,
		CacheTTL:        cfg.Cache.TTL,
		CacheDir:        cacheDir,
		UserAgent:       cfg.Stealth.UserAgent,
		EnableRenderer:  cfg.Rendering.ForceJS || opts.RenderJS,
		RendererTimeout: cfg.Rendering.JSTimeout,
		Concurrency:     cfg.Concurrency.Workers,
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
		OutputDir:       cfg.Output.Directory,
		Flat:            cfg.Output.Flat,
		JSONMetadata:    cfg.Output.JSONMetadata,
		LLMConfig:       &cfg.LLM,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create dependencies: %w", err)
	}

	// Set default strategy factory if none provided
	strategyFactory := opts.StrategyFactory
	if strategyFactory == nil {
		strategyFactory = func(st StrategyType, d *strategies.Dependencies) strategies.Strategy {
			return CreateStrategy(st, d)
		}
	}

	return &Orchestrator{
		config:          cfg,
		deps:            deps,
		logger:          logger,
		strategyFactory: strategyFactory,
	}, nil
}

// Run executes the documentation extraction for the given URL
func (o *Orchestrator) Run(ctx context.Context, url string, opts OrchestratorOptions) error {
	startTime := time.Now()

	o.logger.Info().
		Str("url", url).
		Str("output", o.config.Output.Directory).
		Int("concurrency", o.config.Concurrency.Workers).
		Msg("Starting documentation extraction")

	var strategyType StrategyType
	if opts.StrategyOverride != "" {
		strategyType = StrategyType(opts.StrategyOverride)
		o.logger.Debug().
			Str("strategy", string(strategyType)).
			Msg("Using strategy override from manifest")

		if !IsValidStrategy(strategyType) {
			return fmt.Errorf("unknown strategy override: %s", opts.StrategyOverride)
		}
	} else {
		strategyType = DetectStrategy(url)
		o.logger.Debug().
			Str("strategy", string(strategyType)).
			Msg("Detected strategy type")

		if strategyType == StrategyUnknown {
			return fmt.Errorf("unable to determine strategy for URL: %s", url)
		}
	}

	// Create strategy using strategy factory (allows injection for testing)
	strategy := o.strategyFactory(strategyType, o.deps)
	if strategy == nil {
		return fmt.Errorf("failed to create strategy for URL: %s", url)
	}

	o.logger.Info().
		Str("strategy", strategy.Name()).
		Msg("Using extraction strategy")

	o.deps.SetSourceURL(url)
	o.deps.SetStrategy(strategy.Name())

	strategyOpts := strategies.Options{
		CommonOptions: domain.CommonOptions{
			Verbose:  opts.Verbose,
			DryRun:   opts.DryRun,
			Force:    opts.Force || o.config.Output.Overwrite,
			RenderJS: opts.RenderJS || o.config.Rendering.ForceJS,
			Limit:    opts.Limit,
		},
		Output:          o.config.Output.Directory,
		Concurrency:     o.config.Concurrency.Workers,
		MaxDepth:        o.config.Concurrency.MaxDepth,
		Exclude:         append(o.config.Exclude, opts.ExcludePatterns...),
		NoFolders:       o.config.Output.Flat,
		Split:           opts.Split,
		IncludeAssets:   opts.IncludeAssets,
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
		FilterURL:       opts.FilterURL,
	}

	if err := strategy.Execute(ctx, url, strategyOpts); err != nil {
		if ctx.Err() != nil {
			o.logger.Warn().Msg("Extraction cancelled")
			return ctx.Err()
		}
		return fmt.Errorf("strategy execution failed: %w", err)
	}

	if err := o.deps.FlushMetadata(); err != nil {
		o.logger.Warn().Err(err).Msg("Failed to flush metadata")
	}

	if opts.Prune {
		pruned, err := o.deps.PruneDeletedFiles(ctx)
		if err != nil {
			o.logger.Warn().Err(err).Msg("Failed to prune deleted files")
		} else if pruned > 0 {
			o.logger.Info().Int("pruned", pruned).Msg("Removed deleted pages")
		}
	}

	if err := o.deps.SaveState(ctx); err != nil {
		o.logger.Warn().Err(err).Msg("Failed to save state")
	}

	duration := time.Since(startTime)
	o.logger.Info().
		Dur("duration", duration).
		Msg("Documentation extraction completed")

	return nil
}

// Close releases all resources held by the orchestrator
func (o *Orchestrator) Close() error {
	if o.deps != nil {
		return o.deps.Close()
	}
	return nil
}

// GetStrategyName returns the detected strategy name for a URL
func (o *Orchestrator) GetStrategyName(url string) string {
	return string(DetectStrategy(url))
}

// ValidateURL checks if the URL can be processed
func (o *Orchestrator) ValidateURL(url string) error {
	strategyType := DetectStrategy(url)
	if strategyType == StrategyUnknown {
		return fmt.Errorf("unsupported URL format: %s", url)
	}
	return nil
}

// ManifestResult represents the result of processing one manifest source
type ManifestResult struct {
	Source   manifest.Source
	Error    error
	Duration time.Duration
}

// RunManifest executes all sources defined in the manifest
func (o *Orchestrator) RunManifest(
	ctx context.Context,
	manifestCfg *manifest.Config,
	baseOpts OrchestratorOptions,
) error {
	startTime := time.Now()
	totalSources := len(manifestCfg.Sources)

	o.logger.Info().
		Int("sources", totalSources).
		Bool("continue_on_error", manifestCfg.Options.ContinueOnError).
		Str("output", manifestCfg.Options.Output).
		Msg("Starting manifest execution")

	var results []ManifestResult
	var firstError error

	for i, source := range manifestCfg.Sources {
		if ctx.Err() != nil {
			o.logger.Warn().Msg("Manifest execution cancelled")
			return ctx.Err()
		}

		sourceStart := time.Now()

		o.logger.Info().
			Int("source", i+1).
			Int("total", totalSources).
			Str("url", source.URL).
			Str("strategy", source.Strategy).
			Msg("Processing source")

		opts := o.buildSourceOptions(source, baseOpts)

		err := o.Run(ctx, source.URL, opts)
		sourceDuration := time.Since(sourceStart)

		result := ManifestResult{
			Source:   source,
			Error:    err,
			Duration: sourceDuration,
		}
		results = append(results, result)

		if err != nil {
			o.logger.Error().
				Err(err).
				Str("url", source.URL).
				Dur("duration", sourceDuration).
				Msg("Source extraction failed")

			if firstError == nil {
				firstError = err
			}

			if !manifestCfg.Options.ContinueOnError {
				o.logger.Warn().Msg("Stopping execution (continue_on_error=false)")
				return fmt.Errorf("source %s failed: %w", source.URL, err)
			}
		} else {
			o.logger.Info().
				Str("url", source.URL).
				Dur("duration", sourceDuration).
				Msg("Source extraction completed")
		}
	}

	duration := time.Since(startTime)
	successCount := 0
	for _, r := range results {
		if r.Error == nil {
			successCount++
		}
	}

	o.logger.Info().
		Dur("total_duration", duration).
		Int("total", totalSources).
		Int("success", successCount).
		Int("failed", totalSources-successCount).
		Msg("Manifest execution completed")

	if firstError != nil {
		return fmt.Errorf("manifest completed with %d/%d failures: %w",
			totalSources-successCount, totalSources, firstError)
	}

	return nil
}

func (o *Orchestrator) buildSourceOptions(source manifest.Source, baseOpts OrchestratorOptions) OrchestratorOptions {
	opts := baseOpts

	if source.Strategy != "" {
		opts.StrategyOverride = source.Strategy
	}

	if source.ContentSelector != "" {
		opts.ContentSelector = source.ContentSelector
	}
	if source.ExcludeSelector != "" {
		opts.ExcludeSelector = source.ExcludeSelector
	}

	if len(source.Exclude) > 0 {
		opts.ExcludePatterns = append(opts.ExcludePatterns, source.Exclude...)
	}

	if source.RenderJS != nil {
		opts.RenderJS = *source.RenderJS
	}

	if source.Limit > 0 {
		opts.Limit = source.Limit
	}

	if source.MaxDepth > 0 {
		o.logger.Debug().
			Int("max_depth", source.MaxDepth).
			Str("url", source.URL).
			Msg("Source max_depth specified but config override not implemented")
	}

	return opts
}
</file>
<file path="internal/cache/badger.go">
package cache

import (
	"context"
	"os"
	"time"

	"github.com/dgraph-io/badger/v4"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// BadgerCache is a cache implementation using BadgerDB
type BadgerCache struct {
	db *badger.DB
}

// NewBadgerCache creates a new BadgerDB cache
func NewBadgerCache(opts Options) (*BadgerCache, error) {
	var badgerOpts badger.Options

	if opts.InMemory {
		badgerOpts = badger.DefaultOptions("").WithInMemory(true)
	} else {
		if opts.Directory == "" {
			homeDir, err := os.UserHomeDir()
			if err != nil {
				return nil, err
			}
			opts.Directory = homeDir + "/.repodocs/cache"
		}

		// Ensure directory exists
		if err := os.MkdirAll(opts.Directory, 0755); err != nil {
			return nil, err
		}

		badgerOpts = badger.DefaultOptions(opts.Directory)
	}

	// Disable logging unless explicitly enabled
	if !opts.Logger {
		badgerOpts = badgerOpts.WithLogger(nil)
	}

	db, err := badger.Open(badgerOpts)
	if err != nil {
		return nil, err
	}

	// Start background garbage collection
	go func() {
		ticker := time.NewTicker(5 * time.Minute)
		defer ticker.Stop()
		for range ticker.C {
			_ = db.RunValueLogGC(0.5)
		}
	}()

	return &BadgerCache{db: db}, nil
}

// Get retrieves a value from cache
func (c *BadgerCache) Get(ctx context.Context, key string) ([]byte, error) {
	// Generate cache key from URL
	cacheKey := GenerateKey(key)

	var value []byte
	err := c.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(cacheKey))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return domain.ErrCacheMiss
			}
			return err
		}

		value, err = item.ValueCopy(nil)
		return err
	})

	if err != nil {
		return nil, err
	}

	return value, nil
}

// Set stores a value in cache with TTL
func (c *BadgerCache) Set(ctx context.Context, key string, value []byte, ttl time.Duration) error {
	// Generate cache key from URL
	cacheKey := GenerateKey(key)

	return c.db.Update(func(txn *badger.Txn) error {
		e := badger.NewEntry([]byte(cacheKey), value)
		if ttl > 0 {
			e = e.WithTTL(ttl)
		}
		return txn.SetEntry(e)
	})
}

// Has checks if a key exists in cache
func (c *BadgerCache) Has(ctx context.Context, key string) bool {
	cacheKey := GenerateKey(key)

	err := c.db.View(func(txn *badger.Txn) error {
		_, err := txn.Get([]byte(cacheKey))
		return err
	})

	return err == nil
}

// Delete removes a key from cache
func (c *BadgerCache) Delete(ctx context.Context, key string) error {
	cacheKey := GenerateKey(key)

	return c.db.Update(func(txn *badger.Txn) error {
		return txn.Delete([]byte(cacheKey))
	})
}

// Close releases cache resources
func (c *BadgerCache) Close() error {
	return c.db.Close()
}

// Clear removes all entries from the cache
func (c *BadgerCache) Clear() error {
	return c.db.DropAll()
}

// Size returns the number of entries in the cache
func (c *BadgerCache) Size() int64 {
	var count int64
	_ = c.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			count++
		}
		return nil
	})
	return count
}

// Stats returns cache statistics
func (c *BadgerCache) Stats() map[string]interface{} {
	lsm, vlog := c.db.Size()
	return map[string]interface{}{
		"entries":   c.Size(),
		"lsm_size":  lsm,
		"vlog_size": vlog,
	}
}
</file>
<file path="internal/cache/interface.go">
package cache

import (
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Ensure BadgerCache implements domain.Cache
var _ domain.Cache = (*BadgerCache)(nil)

// Entry represents a cached entry with metadata
type Entry struct {
	URL         string    `json:"url"`
	Content     []byte    `json:"content"`
	ContentType string    `json:"content_type"`
	FetchedAt   time.Time `json:"fetched_at"`
	ExpiresAt   time.Time `json:"expires_at"`
}

// IsExpired returns true if the entry has expired
func (e *Entry) IsExpired() bool {
	return time.Now().After(e.ExpiresAt)
}

// TTL returns the remaining time-to-live
func (e *Entry) TTL() time.Duration {
	remaining := time.Until(e.ExpiresAt)
	if remaining < 0 {
		return 0
	}
	return remaining
}

// Options contains cache configuration options
type Options struct {
	Directory string
	InMemory  bool
	Logger    bool
}

// DefaultOptions returns default cache options
func DefaultOptions() Options {
	return Options{
		Directory: "",
		InMemory:  false,
		Logger:    false,
	}
}
</file>
<file path="internal/cache/keys.go">
package cache

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"path"
	"strings"
)

// GenerateKey generates a cache key from a URL
// The key is a SHA256 hash of the normalized URL
func GenerateKey(rawURL string) string {
	normalized := normalizeForKey(rawURL)
	hash := sha256.Sum256([]byte(normalized))
	return hex.EncodeToString(hash[:])
}

// GenerateKeyWithPrefix generates a cache key with a prefix
func GenerateKeyWithPrefix(prefix, rawURL string) string {
	key := GenerateKey(rawURL)
	return prefix + ":" + key
}

// normalizeForKey normalizes a URL for consistent key generation
func normalizeForKey(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return rawURL
	}

	// Normalize scheme
	if u.Scheme == "" {
		u.Scheme = "https"
	}

	// Normalize host
	u.Host = strings.ToLower(u.Host)

	// Remove default ports
	if (u.Scheme == "http" && u.Port() == "80") ||
		(u.Scheme == "https" && u.Port() == "443") {
		u.Host = u.Hostname()
	}

	// Clean path
	if u.Path == "" {
		u.Path = "/"
	} else {
		u.Path = path.Clean(u.Path)
	}

	// Remove trailing slash except for root
	if u.Path != "/" && strings.HasSuffix(u.Path, "/") {
		u.Path = strings.TrimSuffix(u.Path, "/")
	}

	// Remove fragment
	u.Fragment = ""

	return u.String()
}

// KeyPrefix constants for different cache types
const (
	PrefixPage     = "page"
	PrefixSitemap  = "sitemap"
	PrefixGit      = "git"
	PrefixMetadata = "meta"
)

// PageKey generates a cache key for a page
func PageKey(url string) string {
	return GenerateKeyWithPrefix(PrefixPage, url)
}

// SitemapKey generates a cache key for a sitemap
func SitemapKey(url string) string {
	return GenerateKeyWithPrefix(PrefixSitemap, url)
}

// MetadataKey generates a cache key for metadata
func MetadataKey(url string) string {
	return GenerateKeyWithPrefix(PrefixMetadata, url)
}
</file>
<file path="internal/config/config.go">
package config

import "time"

// Config represents the application configuration
type Config struct {
	Output      OutputConfig      `mapstructure:"output" yaml:"output"`
	Concurrency ConcurrencyConfig `mapstructure:"concurrency" yaml:"concurrency"`
	Cache       CacheConfig       `mapstructure:"cache" yaml:"cache"`
	Rendering   RenderingConfig   `mapstructure:"rendering" yaml:"rendering"`
	Stealth     StealthConfig     `mapstructure:"stealth" yaml:"stealth"`
	Exclude     []string          `mapstructure:"exclude" yaml:"exclude"`
	Logging     LoggingConfig     `mapstructure:"logging" yaml:"logging"`
	LLM         LLMConfig         `mapstructure:"llm" yaml:"llm"`
}

// LLMConfig contains LLM provider settings
type LLMConfig struct {
	Provider        string          `mapstructure:"provider" yaml:"provider"`
	APIKey          string          `mapstructure:"api_key" yaml:"api_key"`
	BaseURL         string          `mapstructure:"base_url" yaml:"base_url"`
	Model           string          `mapstructure:"model" yaml:"model"`
	MaxTokens       int             `mapstructure:"max_tokens" yaml:"max_tokens"`
	Temperature     float64         `mapstructure:"temperature" yaml:"temperature"`
	Timeout         time.Duration   `mapstructure:"timeout" yaml:"timeout"`
	MaxRetries      int             `mapstructure:"max_retries" yaml:"max_retries"` // Deprecated: use RateLimit.MaxRetries
	EnhanceMetadata bool            `mapstructure:"enhance_metadata" yaml:"enhance_metadata"`
	RateLimit       RateLimitConfig `mapstructure:"rate_limit" yaml:"rate_limit"`
}

// RateLimitConfig contains rate limiting settings for LLM requests
type RateLimitConfig struct {
	Enabled           bool                 `mapstructure:"enabled" yaml:"enabled"`
	RequestsPerMinute int                  `mapstructure:"requests_per_minute" yaml:"requests_per_minute"`
	BurstSize         int                  `mapstructure:"burst_size" yaml:"burst_size"`
	MaxRetries        int                  `mapstructure:"max_retries" yaml:"max_retries"`
	InitialDelay      time.Duration        `mapstructure:"initial_delay" yaml:"initial_delay"`
	MaxDelay          time.Duration        `mapstructure:"max_delay" yaml:"max_delay"`
	Multiplier        float64              `mapstructure:"multiplier" yaml:"multiplier"`
	CircuitBreaker    CircuitBreakerConfig `mapstructure:"circuit_breaker" yaml:"circuit_breaker"`
}

// CircuitBreakerConfig contains circuit breaker settings
type CircuitBreakerConfig struct {
	Enabled                  bool          `mapstructure:"enabled" yaml:"enabled"`
	FailureThreshold         int           `mapstructure:"failure_threshold" yaml:"failure_threshold"`
	SuccessThresholdHalfOpen int           `mapstructure:"success_threshold_half_open" yaml:"success_threshold_half_open"`
	ResetTimeout             time.Duration `mapstructure:"reset_timeout" yaml:"reset_timeout"`
}

// OutputConfig contains output-related settings
type OutputConfig struct {
	Directory    string `mapstructure:"directory" yaml:"directory"`
	Flat         bool   `mapstructure:"flat" yaml:"flat"`
	JSONMetadata bool   `mapstructure:"json_metadata" yaml:"json_metadata"`
	Overwrite    bool   `mapstructure:"overwrite" yaml:"overwrite"`
}

// ConcurrencyConfig contains concurrency settings
type ConcurrencyConfig struct {
	Workers  int           `mapstructure:"workers" yaml:"workers"`
	Timeout  time.Duration `mapstructure:"timeout" yaml:"timeout"`
	MaxDepth int           `mapstructure:"max_depth" yaml:"max_depth"`
}

// CacheConfig contains cache settings
type CacheConfig struct {
	Enabled   bool          `mapstructure:"enabled" yaml:"enabled"`
	TTL       time.Duration `mapstructure:"ttl" yaml:"ttl"`
	Directory string        `mapstructure:"directory" yaml:"directory"`
}

// RenderingConfig contains JavaScript rendering settings
type RenderingConfig struct {
	ForceJS     bool          `mapstructure:"force_js" yaml:"force_js"`
	JSTimeout   time.Duration `mapstructure:"js_timeout" yaml:"js_timeout"`
	ScrollToEnd bool          `mapstructure:"scroll_to_end" yaml:"scroll_to_end"`
}

// StealthConfig contains stealth mode settings
type StealthConfig struct {
	UserAgent      string        `mapstructure:"user_agent" yaml:"user_agent"`
	RandomDelayMin time.Duration `mapstructure:"random_delay_min" yaml:"random_delay_min"`
	RandomDelayMax time.Duration `mapstructure:"random_delay_max" yaml:"random_delay_max"`
}

// LoggingConfig contains logging settings
type LoggingConfig struct {
	Level  string `mapstructure:"level" yaml:"level"`
	Format string `mapstructure:"format" yaml:"format"`
}

// Validate validates the configuration
func (c *Config) Validate() error {
	if c.Concurrency.Workers < 1 {
		c.Concurrency.Workers = DefaultWorkers
	}
	if c.Concurrency.MaxDepth < 1 {
		c.Concurrency.MaxDepth = DefaultMaxDepth
	}
	if c.Concurrency.Timeout < time.Second {
		c.Concurrency.Timeout = DefaultTimeout
	}
	if c.Cache.TTL < time.Minute {
		c.Cache.TTL = DefaultCacheTTL
	}
	if c.Rendering.JSTimeout < time.Second {
		c.Rendering.JSTimeout = DefaultJSTimeout
	}
	return nil
}
</file>
<file path="internal/config/defaults.go">
package config

import (
	"os"
	"path/filepath"
	"time"
)

// Default values
const (
	// Output defaults
	DefaultOutputDir = "./docs"

	// Concurrency defaults
	DefaultWorkers  = 5
	DefaultTimeout  = 30 * time.Second
	DefaultMaxDepth = 3

	// Cache defaults
	DefaultCacheEnabled = true
	DefaultCacheTTL     = 24 * time.Hour

	// Rendering defaults
	DefaultJSTimeout   = 60 * time.Second
	DefaultScrollToEnd = true

	// Stealth defaults
	DefaultRandomDelayMin = 1 * time.Second
	DefaultRandomDelayMax = 3 * time.Second

	// Logging defaults
	DefaultLogLevel  = "info"
	DefaultLogFormat = "pretty"

	// LLM defaults (only for optional parameters)
	DefaultLLMMaxTokens   = 4096
	DefaultLLMTemperature = 0.7
	DefaultLLMTimeout     = 60 * time.Second
	DefaultLLMMaxRetries  = 3

	// Rate limiting defaults
	DefaultRateLimitEnabled           = true
	DefaultRateLimitRequestsPerMinute = 60
	DefaultRateLimitBurstSize         = 10
	DefaultRateLimitMaxRetries        = 3
	DefaultRateLimitInitialDelay      = 1 * time.Second
	DefaultRateLimitMaxDelay          = 60 * time.Second
	DefaultRateLimitMultiplier        = 2.0

	// Circuit breaker defaults
	DefaultCircuitBreakerEnabled                  = true
	DefaultCircuitBreakerFailureThreshold         = 5
	DefaultCircuitBreakerSuccessThresholdHalfOpen = 1
	DefaultCircuitBreakerResetTimeout             = 30 * time.Second
)

// Default exclude patterns
var DefaultExcludePatterns = []string{
	`.*\.pdf$`,
	`.*/login.*`,
	`.*/logout.*`,
	`.*/admin.*`,
	`.*/sign-in.*`,
	`.*/sign-up.*`,
}

// ConfigDir returns the config directory path
func ConfigDir() string {
	home, err := os.UserHomeDir()
	if err != nil {
		return ".repodocs"
	}
	return filepath.Join(home, ".repodocs")
}

// CacheDir returns the cache directory path
func CacheDir() string {
	return filepath.Join(ConfigDir(), "cache")
}

// ConfigFilePath returns the config file path
func ConfigFilePath() string {
	return filepath.Join(ConfigDir(), "config.yaml")
}

// Default returns the default configuration
func Default() *Config {
	return &Config{
		Output: OutputConfig{
			Directory:    DefaultOutputDir,
			Flat:         false,
			JSONMetadata: false,
			Overwrite:    false,
		},
		Concurrency: ConcurrencyConfig{
			Workers:  DefaultWorkers,
			Timeout:  DefaultTimeout,
			MaxDepth: DefaultMaxDepth,
		},
		Cache: CacheConfig{
			Enabled:   DefaultCacheEnabled,
			TTL:       DefaultCacheTTL,
			Directory: CacheDir(),
		},
		Rendering: RenderingConfig{
			ForceJS:     false,
			JSTimeout:   DefaultJSTimeout,
			ScrollToEnd: DefaultScrollToEnd,
		},
		Stealth: StealthConfig{
			UserAgent:      "",
			RandomDelayMin: DefaultRandomDelayMin,
			RandomDelayMax: DefaultRandomDelayMax,
		},
		Exclude: DefaultExcludePatterns,
		Logging: LoggingConfig{
			Level:  DefaultLogLevel,
			Format: DefaultLogFormat,
		},
		LLM: LLMConfig{
			MaxTokens:   DefaultLLMMaxTokens,
			Temperature: DefaultLLMTemperature,
			Timeout:     DefaultLLMTimeout,
			MaxRetries:  DefaultLLMMaxRetries,
			RateLimit: RateLimitConfig{
				Enabled:           DefaultRateLimitEnabled,
				RequestsPerMinute: DefaultRateLimitRequestsPerMinute,
				BurstSize:         DefaultRateLimitBurstSize,
				MaxRetries:        DefaultRateLimitMaxRetries,
				InitialDelay:      DefaultRateLimitInitialDelay,
				MaxDelay:          DefaultRateLimitMaxDelay,
				Multiplier:        DefaultRateLimitMultiplier,
				CircuitBreaker: CircuitBreakerConfig{
					Enabled:                  DefaultCircuitBreakerEnabled,
					FailureThreshold:         DefaultCircuitBreakerFailureThreshold,
					SuccessThresholdHalfOpen: DefaultCircuitBreakerSuccessThresholdHalfOpen,
					ResetTimeout:             DefaultCircuitBreakerResetTimeout,
				},
			},
		},
	}
}
</file>
<file path="internal/config/loader.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/spf13/viper"
	"gopkg.in/yaml.v3"
)

// Load loads configuration from file, environment, and defaults
// Uses the global viper instance to access CLI flag bindings
func Load() (*Config, error) {
	// Use global viper instance to get CLI flag bindings
	v := viper.GetViper()

	// Set defaults
	setDefaults(v)

	// Config file settings
	// Search order: current directory first (project-specific), then user config
	v.SetConfigName("config")
	v.SetConfigType("yaml")
	v.AddConfigPath(".")
	v.AddConfigPath(ConfigDir())

	// Read config file (ignore if not found)
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); !ok {
			return nil, err
		}
	}

	// Environment variables (REPODOCS_*)
	v.SetEnvPrefix("REPODOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	// Unmarshal config
	var cfg Config
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, err
	}

	// Validate and apply defaults for invalid values
	if err := cfg.Validate(); err != nil {
		return nil, err
	}

	return &cfg, nil
}

// LoadWithViper loads configuration and returns the viper instance
// This is useful for merging CLI flags later
func LoadWithViper() (*Config, *viper.Viper, error) {
	v := viper.New()

	// Set defaults
	setDefaults(v)

	// Config file settings
	// Search order: current directory first (project-specific), then user config
	v.SetConfigName("config")
	v.SetConfigType("yaml")
	v.AddConfigPath(".")
	v.AddConfigPath(ConfigDir())

	// Read config file (ignore if not found)
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); !ok {
			return nil, nil, err
		}
	}

	// Environment variables (REPODOCS_*)
	v.SetEnvPrefix("REPODOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	// Unmarshal config
	var cfg Config
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, nil, err
	}

	// Validate
	if err := cfg.Validate(); err != nil {
		return nil, nil, err
	}

	return &cfg, v, nil
}

// setDefaults sets default values in viper
func setDefaults(v *viper.Viper) {
	// Output defaults
	v.SetDefault("output.directory", DefaultOutputDir)
	v.SetDefault("output.flat", false)
	v.SetDefault("output.json_metadata", false)
	v.SetDefault("output.overwrite", false)

	// Concurrency defaults
	v.SetDefault("concurrency.workers", DefaultWorkers)
	v.SetDefault("concurrency.timeout", DefaultTimeout)
	v.SetDefault("concurrency.max_depth", DefaultMaxDepth)

	// Cache defaults
	v.SetDefault("cache.enabled", DefaultCacheEnabled)
	v.SetDefault("cache.ttl", DefaultCacheTTL)
	v.SetDefault("cache.directory", CacheDir())

	// Rendering defaults
	v.SetDefault("rendering.force_js", false)
	v.SetDefault("rendering.js_timeout", DefaultJSTimeout)
	v.SetDefault("rendering.scroll_to_end", DefaultScrollToEnd)

	// Stealth defaults
	v.SetDefault("stealth.user_agent", "")
	v.SetDefault("stealth.random_delay_min", DefaultRandomDelayMin)
	v.SetDefault("stealth.random_delay_max", DefaultRandomDelayMax)

	// Exclude defaults
	v.SetDefault("exclude", DefaultExcludePatterns)

	// Logging defaults
	v.SetDefault("logging.level", DefaultLogLevel)
	v.SetDefault("logging.format", DefaultLogFormat)

	// LLM defaults (all keys must be registered for env var binding)
	v.SetDefault("llm.provider", "")
	v.SetDefault("llm.api_key", "")
	v.SetDefault("llm.base_url", "")
	v.SetDefault("llm.model", "")
	v.SetDefault("llm.max_tokens", DefaultLLMMaxTokens)
	v.SetDefault("llm.temperature", DefaultLLMTemperature)
	v.SetDefault("llm.timeout", DefaultLLMTimeout)
	v.SetDefault("llm.max_retries", DefaultLLMMaxRetries)
	v.SetDefault("llm.enhance_metadata", false)
}

// EnsureConfigDir creates the config directory if it doesn't exist
func EnsureConfigDir() error {
	dir := ConfigDir()
	return os.MkdirAll(dir, 0755)
}

// EnsureCacheDir creates the cache directory if it doesn't exist
func EnsureCacheDir() error {
	dir := CacheDir()
	return os.MkdirAll(dir, 0755)
}

// Save writes the configuration to the config file at ~/.repodocs/config.yaml
func Save(cfg *Config) error {
	if err := EnsureConfigDir(); err != nil {
		return fmt.Errorf("failed to create config directory: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	path := ConfigFilePath()
	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write config file: %w", err)
	}

	return nil
}

// SaveTo writes the configuration to a specific path
func SaveTo(cfg *Config, path string) error {
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write config file: %w", err)
	}

	return nil
}
</file>
<file path="internal/converter/content_type.go">
package converter

import "strings"

func IsMarkdownContent(contentType, url string) bool {
	ct := strings.ToLower(contentType)
	if strings.Contains(ct, "text/markdown") ||
		strings.Contains(ct, "text/x-markdown") ||
		strings.Contains(ct, "application/markdown") {
		return true
	}

	lowerURL := strings.ToLower(url)

	if idx := strings.Index(lowerURL, "?"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	if idx := strings.Index(lowerURL, "#"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	if strings.HasSuffix(lowerURL, ".md") ||
		strings.HasSuffix(lowerURL, ".mdx") ||
		strings.HasSuffix(lowerURL, ".markdown") ||
		strings.HasSuffix(lowerURL, ".mdown") {
		return true
	}

	return false
}

// IsPlainTextContent checks if the content is plain text.
// Returns true for text/plain content type or .txt URL extension.
// Query strings and fragments are stripped before checking the extension.
func IsPlainTextContent(contentType, url string) bool {
	ct := strings.ToLower(contentType)
	if strings.Contains(ct, "text/plain") {
		return true
	}

	lowerURL := strings.ToLower(url)

	if idx := strings.Index(lowerURL, "?"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}
	if idx := strings.Index(lowerURL, "#"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	return strings.HasSuffix(lowerURL, ".txt")
}

// IsHTMLContent checks if the content type indicates HTML content.
// Returns true for empty content type (assumes HTML for backward compatibility).
func IsHTMLContent(contentType string) bool {
	if contentType == "" {
		return true
	}
	ct := strings.ToLower(contentType)
	return strings.Contains(ct, "text/html") ||
		strings.Contains(ct, "application/xhtml")
}
</file>
<file path="internal/converter/encoding.go">
package converter

import (
	"bytes"
	"io"
	"strings"

	"golang.org/x/net/html/charset"
	"golang.org/x/text/encoding"
	"golang.org/x/text/encoding/htmlindex"
	"golang.org/x/text/transform"
)

// DetectEncoding detects the character encoding of HTML content
func DetectEncoding(content []byte) string {
	// Try to detect from content-type meta tag or charset attribute
	contentStr := string(content[:min(1024, len(content))])

	// Look for charset in meta tag
	if enc := extractCharsetFromMeta(contentStr); enc != "" {
		return enc
	}

	// Use golang.org/x/net/html/charset for detection
	_, name, _ := charset.DetermineEncoding(content, "")
	if name != "" && name != "windows-1252" {
		return name
	}

	// Default to UTF-8
	return "utf-8"
}

// extractCharsetFromMeta extracts charset from meta tag
func extractCharsetFromMeta(html string) string {
	html = strings.ToLower(html)

	// Look for <meta charset="...">
	if idx := strings.Index(html, "charset="); idx != -1 {
		start := idx + 8
		end := start

		// Skip quote if present
		if start < len(html) && (html[start] == '"' || html[start] == '\'') {
			start++
		}

		// Find end of charset value
		for end = start; end < len(html); end++ {
			c := html[end]
			if c == '"' || c == '\'' || c == ';' || c == '>' || c == ' ' {
				break
			}
		}

		if end > start {
			return strings.TrimSpace(html[start:end])
		}
	}

	return ""
}

// ConvertToUTF8 converts content from detected encoding to UTF-8
func ConvertToUTF8(content []byte) ([]byte, error) {
	enc := DetectEncoding(content)

	// Already UTF-8
	if enc == "utf-8" || enc == "utf8" {
		return content, nil
	}

	// Get encoder for the detected charset
	e, err := htmlindex.Get(enc)
	if err != nil {
		// Unknown encoding, return as-is
		return content, nil
	}

	// Decode to UTF-8
	reader := transform.NewReader(bytes.NewReader(content), e.NewDecoder())
	return io.ReadAll(reader)
}

// IsUTF8 checks if content is valid UTF-8
func IsUTF8(content []byte) bool {
	enc := DetectEncoding(content)
	return enc == "utf-8" || enc == "utf8"
}

// GetEncoder returns the encoding for a charset name
func GetEncoder(charsetName string) (encoding.Encoding, error) {
	return htmlindex.Get(charsetName)
}

// min returns the minimum of two integers
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
</file>
<file path="internal/converter/markdown.go">
package converter

import (
	"fmt"
	"regexp"
	"strings"

	md "github.com/JohannesKaufmann/html-to-markdown/v2"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"gopkg.in/yaml.v3"
)

// Pre-compiled regex patterns for markdown stripping
var (
	linkRegex                = regexp.MustCompile(`\[([^\]]+)\]\([^)]+\)`)
	imageRegex               = regexp.MustCompile(`!\[([^\]]*)\]\([^)]+\)`)
	boldAsterisksRegex       = regexp.MustCompile(`\*\*([^*]+)\*\*`)
	italicAsterisksRegex     = regexp.MustCompile(`\*([^*]+)\*`)
	boldUnderscoresRegex     = regexp.MustCompile(`__([^_]+)__`)
	italicUnderscoresRegex   = regexp.MustCompile(`_([^_]+)_`)
	headersRegex             = regexp.MustCompile(`(?m)^#{1,6}\s+`)
	stripHorizontalRuleRegex = regexp.MustCompile(`(?m)^[\-*_]{3,}$`)
	blockquoteRegex          = regexp.MustCompile(`(?m)^>\s+`)
	unorderedListRegex       = regexp.MustCompile(`(?m)^[\s]*[\-*+]\s+`)
	orderedListRegex         = regexp.MustCompile(`(?m)^[\s]*\d+\.\s+`)
	fencedCodeBlockRegex     = regexp.MustCompile("(?s)```[^`]*```")
	indentedCodeBlockRegex   = regexp.MustCompile("(?m)^(    |\t).*$")
)

// MarkdownConverter converts HTML to Markdown
type MarkdownConverter struct {
	domain string
}

// MarkdownOptions contains options for Markdown conversion
type MarkdownOptions struct {
	Domain          string
	CodeBlockStyle  string // "fenced" or "indented"
	HeadingStyle    string // "atx" or "setext"
	BulletListStyle string // "-", "*", or "+"
}

// DefaultMarkdownOptions returns default Markdown options
func DefaultMarkdownOptions() MarkdownOptions {
	return MarkdownOptions{
		CodeBlockStyle:  "fenced",
		HeadingStyle:    "atx",
		BulletListStyle: "-",
	}
}

// NewMarkdownConverter creates a new Markdown converter
func NewMarkdownConverter(opts MarkdownOptions) *MarkdownConverter {
	return &MarkdownConverter{
		domain: opts.Domain,
	}
}

// Convert converts HTML to Markdown
func (c *MarkdownConverter) Convert(html string) (string, error) {
	// html-to-markdown v2 uses ConvertString directly
	markdown, err := md.ConvertString(html)
	if err != nil {
		return "", fmt.Errorf("failed to convert HTML to Markdown: %w", err)
	}

	// Clean up the markdown
	markdown = c.cleanMarkdown(markdown)

	return markdown, nil
}

// cleanMarkdown cleans up the converted markdown
func (c *MarkdownConverter) cleanMarkdown(markdown string) string {
	// Remove excessive blank lines (more than 2 consecutive)
	for strings.Contains(markdown, "\n\n\n\n") {
		markdown = strings.ReplaceAll(markdown, "\n\n\n\n", "\n\n\n")
	}

	// Trim leading/trailing whitespace
	markdown = strings.TrimSpace(markdown)

	return markdown
}

// GenerateFrontmatter generates YAML frontmatter for a document
func GenerateFrontmatter(doc *domain.Document) (string, error) {
	fm := doc.ToFrontmatter()
	data, err := yaml.Marshal(fm)
	if err != nil {
		return "", err
	}

	return fmt.Sprintf("---\n%s---\n\n", string(data)), nil
}

// AddFrontmatter adds YAML frontmatter to markdown content
func AddFrontmatter(markdown string, doc *domain.Document) (string, error) {
	frontmatter, err := GenerateFrontmatter(doc)
	if err != nil {
		return "", err
	}

	return frontmatter + markdown, nil
}

// CountWords counts words in text
func CountWords(text string) int {
	words := strings.Fields(text)
	return len(words)
}

// CountChars counts characters in text
func CountChars(text string) int {
	return len(text)
}

// StripMarkdown removes markdown formatting to get plain text
func StripMarkdown(markdown string) string {
	// Remove code blocks
	markdown = removeCodeBlocks(markdown)

	// Remove links but keep text: [text](url) -> text
	markdown = linkRegex.ReplaceAllString(markdown, "$1")

	// Remove images: ![alt](url) -> alt
	markdown = imageRegex.ReplaceAllString(markdown, "$1")

	// Remove emphasis: **bold** -> bold, *italic* -> italic
	markdown = boldAsterisksRegex.ReplaceAllString(markdown, "$1")
	markdown = italicAsterisksRegex.ReplaceAllString(markdown, "$1")
	markdown = boldUnderscoresRegex.ReplaceAllString(markdown, "$1")
	markdown = italicUnderscoresRegex.ReplaceAllString(markdown, "$1")

	// Remove headers: # Header -> Header
	markdown = headersRegex.ReplaceAllString(markdown, "")

	// Remove horizontal rules
	markdown = stripHorizontalRuleRegex.ReplaceAllString(markdown, "")

	// Remove blockquotes
	markdown = blockquoteRegex.ReplaceAllString(markdown, "")

	// Remove list markers
	markdown = unorderedListRegex.ReplaceAllString(markdown, "")
	markdown = orderedListRegex.ReplaceAllString(markdown, "")

	return strings.TrimSpace(markdown)
}

// removeCodeBlocks removes fenced code blocks
func removeCodeBlocks(markdown string) string {
	// Remove fenced code blocks
	markdown = fencedCodeBlockRegex.ReplaceAllString(markdown, "")

	// Remove indented code blocks (lines starting with 4 spaces or tab)
	markdown = indentedCodeBlockRegex.ReplaceAllString(markdown, "")

	return markdown
}
</file>
<file path="internal/converter/markdown_reader.go">
package converter

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"gopkg.in/yaml.v3"
)

// MarkdownReader reads and extracts metadata from markdown content
// without using HTML parsing (avoids the 512 node limit issue).
type MarkdownReader struct{}

// NewMarkdownReader creates a new markdown reader.
func NewMarkdownReader() *MarkdownReader {
	return &MarkdownReader{}
}

// Frontmatter represents YAML frontmatter commonly found in markdown files.
type Frontmatter struct {
	Title       string   `yaml:"title"`
	Description string   `yaml:"description"`
	Summary     string   `yaml:"summary"`
	Author      string   `yaml:"author"`
	Date        string   `yaml:"date"`
	Tags        []string `yaml:"tags"`
	Category    string   `yaml:"category"`
}

// Read processes markdown content and returns a Document.
func (r *MarkdownReader) Read(content, sourceURL string) (*domain.Document, error) {
	frontmatter, body := r.parseFrontmatter(content)
	title := r.extractTitle(frontmatter, body)
	description := r.extractDescription(frontmatter, body)
	headers := r.extractHeaders(body)
	links := r.extractLinks(body, sourceURL)

	plainText := StripMarkdown(body)
	wordCount := CountWords(plainText)
	charCount := CountChars(plainText)
	contentHash := r.calculateHash(body)

	return &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        body,
		HTMLContent:    "",
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        headers,
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}, nil
}

func (r *MarkdownReader) parseFrontmatter(content string) (*Frontmatter, string) {
	content = strings.TrimSpace(content)

	if !strings.HasPrefix(content, "---") {
		return nil, content
	}

	rest := content[3:]

	if strings.HasPrefix(rest, "\n") {
		rest = rest[1:]
	} else if strings.HasPrefix(rest, "\r\n") {
		rest = rest[2:]
	}

	lines := strings.Split(rest, "\n")
	yamlLines := []string{}
	closingIdx := -1

	for i, line := range lines {
		trimmed := strings.TrimRight(line, "\r")
		if trimmed == "---" {
			closingIdx = i
			break
		}
		yamlLines = append(yamlLines, line)
	}

	if closingIdx == -1 {
		return nil, content
	}

	yamlContent := strings.Join(yamlLines, "\n")
	var fm Frontmatter
	if err := yaml.Unmarshal([]byte(yamlContent), &fm); err != nil {
		return nil, content
	}

	bodyLines := lines[closingIdx+1:]
	body := strings.TrimSpace(strings.Join(bodyLines, "\n"))

	return &fm, body
}

func (r *MarkdownReader) extractTitle(fm *Frontmatter, body string) string {
	if fm != nil && fm.Title != "" {
		return fm.Title
	}

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		if strings.HasPrefix(trimmed, "# ") {
			title := strings.TrimSpace(strings.TrimPrefix(trimmed, "# "))
			title = strings.TrimRight(title, "#")
			return strings.TrimSpace(title)
		}
	}

	return ""
}

var numberedListRegex = regexp.MustCompile(`^\d+\.\s`)
var horizontalRuleRegex = regexp.MustCompile(`^[-*_]{3,}$`)

func (r *MarkdownReader) extractDescription(fm *Frontmatter, body string) string {
	if fm != nil {
		if fm.Description != "" {
			return fm.Description
		}
		if fm.Summary != "" {
			return fm.Summary
		}
	}

	inCodeBlock := false
	lines := strings.Split(body, "\n")
	var paragraphLines []string

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		if strings.HasPrefix(trimmed, "#") {
			continue
		}

		if strings.HasPrefix(trimmed, "- ") ||
			strings.HasPrefix(trimmed, "* ") ||
			strings.HasPrefix(trimmed, "+ ") ||
			numberedListRegex.MatchString(trimmed) {
			continue
		}

		if trimmed == "" {
			if len(paragraphLines) > 0 {
				break
			}
			continue
		}

		if horizontalRuleRegex.MatchString(trimmed) {
			continue
		}

		paragraphLines = append(paragraphLines, trimmed)
	}

	if len(paragraphLines) > 0 {
		desc := strings.Join(paragraphLines, " ")
		if len(desc) > 300 {
			desc = desc[:297] + "..."
		}
		return desc
	}

	return ""
}

var headingRegex = regexp.MustCompile(`^(#{1,6})\s+(.+)$`)

func (r *MarkdownReader) extractHeaders(body string) map[string][]string {
	headers := make(map[string][]string)

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		matches := headingRegex.FindStringSubmatch(trimmed)
		if len(matches) == 3 {
			level := len(matches[1])
			text := strings.TrimSpace(matches[2])

			text = strings.TrimRight(text, "#")
			text = strings.TrimSpace(text)

			if text != "" {
				key := "h" + string(rune('0'+level))
				headers[key] = append(headers[key], text)
			}
		}
	}

	return headers
}

var markdownLinkRegex = regexp.MustCompile(`\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)`)

func (r *MarkdownReader) extractLinks(body, baseURL string) []string {
	var links []string
	seen := make(map[string]bool)

	base, _ := url.Parse(baseURL)

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		matches := markdownLinkRegex.FindAllStringSubmatch(line, -1)
		for _, match := range matches {
			if len(match) >= 3 {
				href := strings.TrimSpace(match[2])

				if href == "" ||
					strings.HasPrefix(href, "#") ||
					strings.HasPrefix(href, "javascript:") ||
					strings.HasPrefix(href, "mailto:") ||
					strings.HasPrefix(href, "tel:") {
					continue
				}

				if base != nil && !strings.HasPrefix(href, "http://") && !strings.HasPrefix(href, "https://") {
					if refURL, err := url.Parse(href); err == nil {
						href = base.ResolveReference(refURL).String()
					}
				}

				if !seen[href] {
					seen[href] = true
					links = append(links, href)
				}
			}
		}
	}

	return links
}

func (r *MarkdownReader) calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}
</file>
<file path="internal/converter/pipeline.go">
package converter

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"errors"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Pipeline orchestrates the HTML to Markdown conversion process
type Pipeline struct {
	sanitizer       *Sanitizer
	extractor       *ExtractContent
	mdConverter     *MarkdownConverter
	excludeSelector string
}

// PipelineOptions contains options for the conversion pipeline
type PipelineOptions struct {
	BaseURL         string
	ContentSelector string
	ExcludeSelector string
}

// NewPipeline creates a new conversion pipeline
func NewPipeline(opts PipelineOptions) *Pipeline {
	sanitizer := NewSanitizer(SanitizerOptions{
		BaseURL:          opts.BaseURL,
		RemoveNavigation: true,
		RemoveComments:   true,
	})

	extractor := NewExtractContent(opts.ContentSelector)

	mdConverter := NewMarkdownConverter(MarkdownOptions{
		Domain:          opts.BaseURL,
		CodeBlockStyle:  "fenced",
		HeadingStyle:    "atx",
		BulletListStyle: "-",
	})

	return &Pipeline{
		sanitizer:       sanitizer,
		extractor:       extractor,
		mdConverter:     mdConverter,
		excludeSelector: opts.ExcludeSelector,
	}
}

// Convert processes HTML content and returns a Document
func (p *Pipeline) Convert(ctx context.Context, html string, sourceURL string) (*domain.Document, error) {
	// Step 1: Convert encoding to UTF-8
	htmlBytes, err := ConvertToUTF8([]byte(html))
	if err != nil {
		return nil, err
	}
	html = string(htmlBytes)

	// Step 2: Parse original HTML once
	origDoc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return nil, err
	}

	// Step 3: Extract main content
	var contentHTML string
	var title string
	var contentSel *goquery.Selection
	usedSelector := false

	if p.extractor.selector != "" {
		contentHTML, title, err = p.extractor.ExtractFromDocument(origDoc, sourceURL)
		if err != nil {
			if errors.Is(err, ErrSelectorNotFound) {
				contentHTML, title, err = p.extractor.extractWithReadability(html, sourceURL)
			} else {
				return nil, err
			}
		} else {
			usedSelector = true
			contentSel = origDoc.Find(p.extractor.selector)
		}
	} else {
		contentHTML, title, err = p.extractor.extractWithReadability(html, sourceURL)
	}
	if err != nil {
		return nil, err
	}

	// Step 3.5: Apply exclusion selector (remove unwanted elements)
	if p.excludeSelector != "" {
		if usedSelector {
			contentSel = p.removeExcludedFromSelection(contentSel)
		} else {
			contentHTML = p.removeExcluded(contentHTML)
		}
	}

	if usedSelector && contentSel == nil {
		return nil, ErrSelectorNotFound
	}

	// Step 4: Sanitize HTML
	var sanitizedHTML string
	var headers map[string][]string
	var links []string

	description := ExtractDescription(origDoc)

	if usedSelector {
		sanitizedSel, selErr := p.sanitizer.SanitizeSelection(contentSel)
		if selErr != nil {
			return nil, selErr
		}

		headers = extractHeadersFromSelection(sanitizedSel)
		links = extractLinksFromSelection(sanitizedSel, sourceURL)

		sanitizedHTML, err = selectionHTML(sanitizedSel)
		if err != nil {
			return nil, err
		}
	} else {
		contentDoc, docErr := goquery.NewDocumentFromReader(strings.NewReader(contentHTML))
		if docErr != nil {
			return nil, docErr
		}

		sanitizedDoc, docErr := p.sanitizer.SanitizeDocument(contentDoc)
		if docErr != nil {
			return nil, docErr
		}

		headers = ExtractHeadersFromDoc(sanitizedDoc)
		links = ExtractLinksFromDoc(sanitizedDoc, sourceURL)

		sanitizedHTML, err = sanitizedDoc.Html()
		if err != nil {
			return nil, err
		}
	}

	// Step 5: Convert to Markdown
	markdown, err := p.mdConverter.Convert(sanitizedHTML)
	if err != nil {
		return nil, err
	}

	// Step 6: Calculate statistics
	plainText := StripMarkdown(markdown)
	wordCount := CountWords(plainText)
	charCount := CountChars(plainText)
	contentHash := calculateHash(markdown)

	// Step 7: Build document
	document := &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        markdown,
		HTMLContent:    html,
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        headers,
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}

	return document, nil
}

// calculateHash calculates SHA256 hash of content
func calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}

// ConvertHTML is a convenience function for simple HTML to Markdown conversion
func ConvertHTML(html, sourceURL string) (*domain.Document, error) {
	pipeline := NewPipeline(PipelineOptions{
		BaseURL: sourceURL,
	})
	return pipeline.Convert(context.Background(), html, sourceURL)
}

// ConvertHTMLWithSelector converts HTML with a specific content selector
func ConvertHTMLWithSelector(html, sourceURL, selector string) (*domain.Document, error) {
	pipeline := NewPipeline(PipelineOptions{
		BaseURL:         sourceURL,
		ContentSelector: selector,
	})
	return pipeline.Convert(context.Background(), html, sourceURL)
}

// removeExcluded removes elements matching the exclude selector from HTML content
func (p *Pipeline) removeExcluded(html string) string {
	if p.excludeSelector == "" {
		return html
	}

	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return html
	}

	_ = p.removeExcludedFromSelection(doc.Selection)

	result, err := doc.Find("body").Html()
	if err != nil {
		// If body extraction fails, try getting the whole document
		result, err = doc.Html()
		if err != nil {
			return html
		}
	}

	return result
}

func (p *Pipeline) removeExcludedFromSelection(sel *goquery.Selection) *goquery.Selection {
	if p.excludeSelector == "" || sel == nil {
		return sel
	}

	findWithRoot(sel, p.excludeSelector).Remove()
	return sel
}

func selectionHTML(sel *goquery.Selection) (string, error) {
	if sel == nil {
		return "", nil
	}

	var combined strings.Builder
	found := false
	sel.Each(func(_ int, node *goquery.Selection) {
		if html, err := node.Html(); err == nil {
			combined.WriteString(html)
			found = true
		}
	})

	if !found {
		return sel.Html()
	}

	return combined.String(), nil
}
</file>
<file path="internal/converter/plaintext_reader.go">
package converter

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"path"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type PlainTextReader struct{}

func NewPlainTextReader() *PlainTextReader {
	return &PlainTextReader{}
}

func (r *PlainTextReader) Read(content, sourceURL string) (*domain.Document, error) {
	content = strings.TrimSpace(content)

	title := r.extractTitle(content, sourceURL)
	description := r.extractDescription(content)
	links := r.extractLinks(content, sourceURL)

	wordCount := CountWords(content)
	charCount := CountChars(content)
	contentHash := r.calculateHash(content)

	return &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        content,
		HTMLContent:    "",
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        make(map[string][]string),
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}, nil
}

func (r *PlainTextReader) extractTitle(content, sourceURL string) string {
	lines := strings.Split(content, "\n")
	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if trimmed == "" {
			continue
		}

		if strings.HasPrefix(trimmed, "# ") {
			title := strings.TrimPrefix(trimmed, "# ")
			return strings.TrimSpace(title)
		}

		if len(trimmed) > 100 {
			return trimmed[:97] + "..."
		}
		return trimmed
	}

	parsed, err := url.Parse(sourceURL)
	if err == nil {
		filename := path.Base(parsed.Path)
		if filename != "" && filename != "." && filename != "/" {
			return strings.TrimSuffix(filename, ".txt")
		}
	}

	return ""
}

func (r *PlainTextReader) extractDescription(content string) string {
	lines := strings.Split(content, "\n")
	var descLines []string
	started := false

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if !started {
			if trimmed == "" || strings.HasPrefix(trimmed, "#") {
				continue
			}
			started = true
		}

		if trimmed == "" && started {
			break
		}

		descLines = append(descLines, trimmed)
	}

	if len(descLines) > 0 {
		desc := strings.Join(descLines, " ")
		if len(desc) > 300 {
			return desc[:297] + "..."
		}
		return desc
	}

	return ""
}

var plainTextLinkRegex = regexp.MustCompile(`\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)`)

func (r *PlainTextReader) extractLinks(content, baseURL string) []string {
	var links []string
	seen := make(map[string]bool)
	base, _ := url.Parse(baseURL)

	matches := plainTextLinkRegex.FindAllStringSubmatch(content, -1)
	for _, match := range matches {
		if len(match) >= 3 {
			href := strings.TrimSpace(match[2])

			if href == "" ||
				strings.HasPrefix(href, "#") ||
				strings.HasPrefix(href, "javascript:") ||
				strings.HasPrefix(href, "mailto:") ||
				strings.HasPrefix(href, "tel:") {
				continue
			}

			if base != nil && !strings.HasPrefix(href, "http://") && !strings.HasPrefix(href, "https://") {
				if refURL, err := url.Parse(href); err == nil {
					href = base.ResolveReference(refURL).String()
				}
			}

			if !seen[href] {
				seen[href] = true
				links = append(links, href)
			}
		}
	}

	return links
}

func (r *PlainTextReader) calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}
</file>
<file path="internal/converter/readability.go">
package converter

import (
	"errors"
	"fmt"
	"net/url"
	"strings"

	"github.com/PuerkitoBio/goquery"
	"github.com/go-shiori/go-readability"
	"github.com/rs/zerolog/log"
)

// ErrSelectorNotFound indicates no elements matched the selector.
var ErrSelectorNotFound = errors.New("selector not found")

// ExtractContent extracts the main content from HTML
type ExtractContent struct {
	selector string
}

// ExtractOptions contains options for content extraction
type ExtractOptions struct {
	Selector string // CSS selector for main content
	URL      string // Source URL for resolving relative links
}

// NewExtractContent creates a new content extractor
func NewExtractContent(selector string) *ExtractContent {
	return &ExtractContent{selector: selector}
}

// Extract extracts main content from HTML
func (e *ExtractContent) Extract(html, sourceURL string) (string, string, error) {
	// If a selector is provided, use it directly
	if e.selector != "" {
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
		if err != nil {
			return "", "", err
		}

		content, title, err := e.ExtractFromDocument(doc, sourceURL)
		if err == nil {
			return content, title, nil
		}
		if errors.Is(err, ErrSelectorNotFound) {
			return e.extractWithReadability(html, sourceURL)
		}

		return "", "", err
	}

	// Otherwise, use readability algorithm
	return e.extractWithReadability(html, sourceURL)
}

// ExtractFromDocument extracts content using a pre-parsed document.
func (e *ExtractContent) ExtractFromDocument(doc *goquery.Document, sourceURL string) (string, string, error) {
	if e.selector == "" {
		return "", "", fmt.Errorf("extract from document requires selector")
	}

	if doc == nil {
		return "", "", fmt.Errorf("extract from document requires document")
	}

	content := doc.Find(e.selector)
	matchCount := content.Length()

	log.Debug().
		Str("selector", e.selector).
		Int("matches", matchCount).
		Str("url", sourceURL).
		Msg("Content selector applied")

	if matchCount == 0 {
		log.Debug().
			Str("selector", e.selector).
			Str("url", sourceURL).
			Msg("Selector not found, falling back to Readability algorithm")
		return "", "", ErrSelectorNotFound
	}

	title := extractTitle(doc)

	var combined strings.Builder
	content.Each(func(_ int, sel *goquery.Selection) {
		if h, err := sel.Html(); err == nil {
			combined.WriteString(h)
		}
	})

	return combined.String(), title, nil
}

// extractWithSelector extracts content using a CSS selector
func (e *ExtractContent) extractWithSelector(html, sourceURL string) (string, string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return "", "", err
	}

	return e.ExtractFromDocument(doc, sourceURL)
}

// extractWithReadability extracts content using the readability algorithm
func (e *ExtractContent) extractWithReadability(html, sourceURL string) (string, string, error) {
	parsedURL, err := url.Parse(sourceURL)
	if err != nil {
		parsedURL = &url.URL{Scheme: "https", Host: "example.com"}
	}

	article, err := readability.FromReader(strings.NewReader(html), parsedURL)
	if err != nil {
		// If readability fails, try to extract the body
		return e.extractBody(html)
	}

	return article.Content, article.Title, nil
}

// extractBody extracts the body content as a fallback
func (e *ExtractContent) extractBody(html string) (string, string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return html, "", nil
	}

	title := extractTitle(doc)

	// Get body content
	body := doc.Find("body")
	if body.Length() == 0 {
		return html, title, nil
	}

	bodyHTML, err := body.Html()
	if err != nil {
		return html, title, nil
	}

	return bodyHTML, title, nil
}

// extractTitle extracts the page title
func extractTitle(doc *goquery.Document) string {
	// Try <title> tag
	title := strings.TrimSpace(doc.Find("title").First().Text())
	if title != "" {
		return title
	}

	// Try <h1> tag
	h1 := strings.TrimSpace(doc.Find("h1").First().Text())
	if h1 != "" {
		return h1
	}

	// Try og:title meta tag
	ogTitle, exists := doc.Find("meta[property='og:title']").Attr("content")
	if exists && ogTitle != "" {
		return ogTitle
	}

	return ""
}

// ExtractDescription extracts the page description
func ExtractDescription(doc *goquery.Document) string {
	// Try meta description
	desc, exists := doc.Find("meta[name='description']").Attr("content")
	if exists && desc != "" {
		return strings.TrimSpace(desc)
	}

	// Try og:description
	ogDesc, exists := doc.Find("meta[property='og:description']").Attr("content")
	if exists && ogDesc != "" {
		return strings.TrimSpace(ogDesc)
	}

	return ""
}

// ExtractHeaders extracts all headers from HTML
func ExtractHeaders(html string) map[string][]string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return map[string][]string{}
	}

	return ExtractHeadersFromDoc(doc)
}

// ExtractHeadersFromDoc extracts headers from a parsed document.
func ExtractHeadersFromDoc(doc *goquery.Document) map[string][]string {
	return extractHeadersFromSelection(doc.Selection)
}

func extractHeadersFromSelection(sel *goquery.Selection) map[string][]string {
	headers := make(map[string][]string)

	for i := 1; i <= 6; i++ {
		tag := string('h') + string('0'+byte(i)) // h1, h2, ..., h6
		findWithRoot(sel, tag).Each(func(_ int, node *goquery.Selection) {
			text := strings.TrimSpace(node.Text())
			if text != "" {
				headers[tag] = append(headers[tag], text)
			}
		})
	}

	return headers
}

// ExtractLinks extracts all links from HTML
func ExtractLinks(html, baseURL string) []string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return []string{}
	}

	return ExtractLinksFromDoc(doc, baseURL)
}

// ExtractLinksFromDoc extracts links from a parsed document.
func ExtractLinksFromDoc(doc *goquery.Document, baseURL string) []string {
	return extractLinksFromSelection(doc.Selection, baseURL)
}

func extractLinksFromSelection(sel *goquery.Selection, baseURL string) []string {
	var links []string

	base, _ := url.Parse(baseURL)

	findWithRoot(sel, "a[href]").Each(func(_ int, node *goquery.Selection) {
		href, exists := node.Attr("href")
		if !exists || href == "" {
			return
		}

		// Skip anchors, javascript, mailto
		if strings.HasPrefix(href, "#") ||
			strings.HasPrefix(href, "javascript:") ||
			strings.HasPrefix(href, "mailto:") ||
			strings.HasPrefix(href, "tel:") {
			return
		}

		// Resolve relative URLs
		if base != nil && !strings.HasPrefix(href, "http") {
			refURL, err := url.Parse(href)
			if err == nil {
				href = base.ResolveReference(refURL).String()
			}
		}

		links = append(links, href)
	})

	return links
}

func findWithRoot(sel *goquery.Selection, query string) *goquery.Selection {
	return sel.Filter(query).AddSelection(sel.Find(query))
}
</file>
<file path="internal/converter/sanitizer.go">
package converter

import (
	"net/url"
	"regexp"
	"strings"

	"github.com/PuerkitoBio/goquery"
)

// TagsToRemove are HTML tags that should be completely removed
var TagsToRemove = []string{
	"script",
	"style",
	"noscript",
	"iframe",
	"object",
	"embed",
	"applet",
	"form",
	"input",
	"button",
	"select",
	"textarea",
	"footer",
	"header",
	"aside",
	"advertisement",
	"banner",
}

// ClassesToRemove are CSS classes that indicate non-content elements
var ClassesToRemove = []string{
	"sidebar",
	"navigation",
	"nav",
	"menu",
	"footer",
	"header",
	"banner",
	"advertisement",
	"ad",
	"social",
	"share",
	"comment",
	"comments",
	"related",
	"recommended",
}

// IDsToRemove are element IDs that indicate non-content elements
var IDsToRemove = []string{
	"sidebar",
	"navigation",
	"nav",
	"menu",
	"footer",
	"header",
	"banner",
	"advertisement",
	"comments",
}

// Sanitizer cleans HTML content for conversion
type Sanitizer struct {
	baseURL          string
	removeNavigation bool
	removeComments   bool
}

// SanitizerOptions contains options for the sanitizer
type SanitizerOptions struct {
	BaseURL          string
	RemoveNavigation bool
	RemoveComments   bool
}

// NewSanitizer creates a new sanitizer
func NewSanitizer(opts SanitizerOptions) *Sanitizer {
	return &Sanitizer{
		baseURL:          opts.BaseURL,
		removeNavigation: opts.RemoveNavigation,
		removeComments:   opts.RemoveComments,
	}
}

// Sanitize cleans HTML content
func (s *Sanitizer) Sanitize(html string) (string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return "", err
	}

	cleanDoc, err := s.SanitizeDocument(doc)
	if err != nil {
		return "", err
	}

	result, err := cleanDoc.Html()
	if err != nil {
		return "", err
	}

	return result, nil
}

// SanitizeDocument cleans a pre-parsed document in place.
func (s *Sanitizer) SanitizeDocument(doc *goquery.Document) (*goquery.Document, error) {
	if doc == nil {
		return nil, nil
	}

	s.sanitizeSelection(doc.Selection)
	return doc, nil
}

// SanitizeSelection cleans a selection in place.
func (s *Sanitizer) SanitizeSelection(sel *goquery.Selection) (*goquery.Selection, error) {
	if sel == nil {
		return nil, nil
	}

	s.sanitizeSelection(sel)
	return sel, nil
}

func (s *Sanitizer) sanitizeSelection(sel *goquery.Selection) {
	// Remove unwanted tags
	for _, tag := range TagsToRemove {
		findWithRoot(sel, tag).Remove()
	}

	// Remove elements by class
	if s.removeNavigation {
		for _, class := range ClassesToRemove {
			findWithRoot(sel, "."+class).Remove()
			findWithRoot(sel, "[class*='"+class+"']").Remove()
		}

		// Remove elements by ID
		for _, id := range IDsToRemove {
			findWithRoot(sel, "#"+id).Remove()
		}

		findWithRoot(sel, "nav").Remove()
	}

	// Remove hidden elements
	findWithRoot(sel, "[style*='display:none']").Remove()
	findWithRoot(sel, "[style*='display: none']").Remove()
	findWithRoot(sel, "[hidden]").Remove()

	// Normalize URLs if base URL is provided
	if s.baseURL != "" {
		s.normalizeURLsFromSelection(sel)
	}

	// Remove empty paragraphs and divs
	s.removeEmptyElementsFromSelection(sel)
}

// normalizeURLs converts relative URLs to absolute URLs
func (s *Sanitizer) normalizeURLs(doc *goquery.Document) {
	s.normalizeURLsFromSelection(doc.Selection)
}

func (s *Sanitizer) normalizeURLsFromSelection(sel *goquery.Selection) {
	base, err := url.Parse(s.baseURL)
	if err != nil {
		return
	}

	// Normalize href attributes
	findWithRoot(sel, "a[href]").Each(func(_ int, node *goquery.Selection) {
		if href, exists := node.Attr("href"); exists {
			if absoluteURL := resolveURL(base, href); absoluteURL != "" {
				node.SetAttr("href", absoluteURL)
			}
		}
	})

	// Normalize src attributes
	findWithRoot(sel, "[src]").Each(func(_ int, node *goquery.Selection) {
		if src, exists := node.Attr("src"); exists {
			if absoluteURL := resolveURL(base, src); absoluteURL != "" {
				node.SetAttr("src", absoluteURL)
			}
		}
	})

	// Normalize srcset attributes
	findWithRoot(sel, "[srcset]").Each(func(_ int, node *goquery.Selection) {
		if srcset, exists := node.Attr("srcset"); exists {
			node.SetAttr("srcset", normalizeSrcset(base, srcset))
		}
	})
}

// resolveURL resolves a relative URL against a base URL
func resolveURL(base *url.URL, ref string) string {
	// Skip empty, fragment, javascript, mailto, and data URLs
	if ref == "" || strings.HasPrefix(ref, "#") ||
		strings.HasPrefix(ref, "javascript:") ||
		strings.HasPrefix(ref, "mailto:") ||
		strings.HasPrefix(ref, "data:") {
		return ref
	}

	refURL, err := url.Parse(ref)
	if err != nil {
		return ref
	}

	return base.ResolveReference(refURL).String()
}

// normalizeSrcset normalizes URLs in srcset attribute
func normalizeSrcset(base *url.URL, srcset string) string {
	parts := strings.Split(srcset, ",")
	for i, part := range parts {
		part = strings.TrimSpace(part)
		tokens := strings.Fields(part)
		if len(tokens) > 0 {
			tokens[0] = resolveURL(base, tokens[0])
			parts[i] = strings.Join(tokens, " ")
		}
	}
	return strings.Join(parts, ", ")
}

// removeEmptyElements removes empty block elements
func (s *Sanitizer) removeEmptyElements(doc *goquery.Document) {
	s.removeEmptyElementsFromSelection(doc.Selection)
}

func (s *Sanitizer) removeEmptyElementsFromSelection(sel *goquery.Selection) {
	emptyTags := []string{"p", "div", "span", "section", "article"}
	whitespaceRegex := regexp.MustCompile(`^\s*$`)

	for _, tag := range emptyTags {
		findWithRoot(sel, tag).Each(func(_ int, node *goquery.Selection) {
			text := strings.TrimSpace(node.Text())
			if whitespaceRegex.MatchString(text) && node.Children().Length() == 0 {
				node.Remove()
			}
		})
	}
}
</file>
<file path="internal/domain/errors.go">
package domain

import (
	"errors"
	"fmt"
)

// Sentinel errors
var (
	// ErrNotFound indicates a resource was not found
	ErrNotFound = errors.New("not found")

	// ErrCacheMiss indicates a cache miss
	ErrCacheMiss = errors.New("cache miss")

	// ErrCacheExpired indicates the cached entry has expired
	ErrCacheExpired = errors.New("cache entry expired")

	// ErrRateLimited indicates rate limiting was encountered
	ErrRateLimited = errors.New("rate limited")

	// ErrBlocked indicates the request was blocked (e.g., by Cloudflare)
	ErrBlocked = errors.New("request blocked")

	// ErrTimeout indicates a timeout occurred
	ErrTimeout = errors.New("timeout")

	// ErrInvalidURL indicates an invalid URL was provided
	ErrInvalidURL = errors.New("invalid URL")

	// ErrNoStrategy indicates no strategy can handle the URL
	ErrNoStrategy = errors.New("no strategy found for URL")

	// ErrRenderFailed indicates JavaScript rendering failed
	ErrRenderFailed = errors.New("render failed")

	// ErrConversionFailed indicates HTML to Markdown conversion failed
	ErrConversionFailed = errors.New("conversion failed")

	// ErrWriteFailed indicates writing output failed
	ErrWriteFailed = errors.New("write failed")

	// ErrBrowserNotFound indicates Chrome/Chromium was not found
	ErrBrowserNotFound = errors.New("browser not found")
)

// FetchError represents an error during fetching
type FetchError struct {
	URL        string
	StatusCode int
	Err        error
}

func (e *FetchError) Error() string {
	if e.StatusCode > 0 {
		return fmt.Sprintf("fetch error for %s: status %d: %v", e.URL, e.StatusCode, e.Err)
	}
	return fmt.Sprintf("fetch error for %s: %v", e.URL, e.Err)
}

func (e *FetchError) Unwrap() error {
	return e.Err
}

// NewFetchError creates a new FetchError
func NewFetchError(url string, statusCode int, err error) *FetchError {
	return &FetchError{
		URL:        url,
		StatusCode: statusCode,
		Err:        err,
	}
}

// RetryableError indicates an error that can be retried
type RetryableError struct {
	Err        error
	RetryAfter int // Seconds to wait before retry, 0 if unknown
}

func (e *RetryableError) Error() string {
	if e.RetryAfter > 0 {
		return fmt.Sprintf("retryable error (retry after %ds): %v", e.RetryAfter, e.Err)
	}
	return fmt.Sprintf("retryable error: %v", e.Err)
}

func (e *RetryableError) Unwrap() error {
	return e.Err
}

// IsRetryable checks if an error should be retried
func IsRetryable(err error) bool {
	var retryable *RetryableError
	if errors.As(err, &retryable) {
		return true
	}

	var fetchErr *FetchError
	if errors.As(err, &fetchErr) {
		// Retry on specific status codes
		switch fetchErr.StatusCode {
		case 429, 503, 502, 504:
			return true
		}
		// Retry on Cloudflare errors
		if fetchErr.StatusCode >= 520 && fetchErr.StatusCode <= 530 {
			return true
		}
	}

	return errors.Is(err, ErrRateLimited) || errors.Is(err, ErrTimeout)
}

// ValidationError represents a validation error
type ValidationError struct {
	Field   string
	Message string
}

func (e *ValidationError) Error() string {
	return fmt.Sprintf("validation error for %s: %s", e.Field, e.Message)
}

// NewValidationError creates a new ValidationError
func NewValidationError(field, message string) *ValidationError {
	return &ValidationError{
		Field:   field,
		Message: message,
	}
}

// StrategyError represents an error in strategy execution
type StrategyError struct {
	Strategy string
	URL      string
	Err      error
}

func (e *StrategyError) Error() string {
	return fmt.Sprintf("strategy %s failed for %s: %v", e.Strategy, e.URL, e.Err)
}

func (e *StrategyError) Unwrap() error {
	return e.Err
}

// NewStrategyError creates a new StrategyError
func NewStrategyError(strategy, url string, err error) *StrategyError {
	return &StrategyError{
		Strategy: strategy,
		URL:      url,
		Err:      err,
	}
}

// =============================================================================
// LLM Errors
// =============================================================================

// LLM sentinel errors
var (
	// ErrLLMNotConfigured indicates LLM provider is not configured
	ErrLLMNotConfigured = errors.New("LLM provider not configured")

	// ErrLLMMissingAPIKey indicates API key is required but not provided
	ErrLLMMissingAPIKey = errors.New("LLM API key is required")

	// ErrLLMMissingBaseURL indicates base URL is required but not provided
	ErrLLMMissingBaseURL = errors.New("LLM base URL is required")

	// ErrLLMMissingModel indicates model is required but not provided
	ErrLLMMissingModel = errors.New("LLM model is required")

	// ErrLLMInvalidProvider indicates an invalid provider type
	ErrLLMInvalidProvider = errors.New("invalid LLM provider")

	// ErrLLMRequestFailed indicates the LLM request failed
	ErrLLMRequestFailed = errors.New("LLM request failed")

	// ErrLLMRateLimited indicates rate limit was exceeded
	ErrLLMRateLimited = errors.New("LLM rate limit exceeded")

	// ErrLLMAuthFailed indicates authentication failed
	ErrLLMAuthFailed = errors.New("LLM authentication failed")

	// ErrLLMContextTooLong indicates context length was exceeded
	ErrLLMContextTooLong = errors.New("LLM context length exceeded")

	// ErrLLMCircuitOpen indicates the circuit breaker is open
	ErrLLMCircuitOpen = errors.New("LLM circuit breaker is open")

	// ErrLLMMaxRetriesExceeded indicates all retry attempts failed
	ErrLLMMaxRetriesExceeded = errors.New("LLM max retries exceeded")
)

// LLMError represents an LLM-specific error
type LLMError struct {
	Provider   string
	StatusCode int
	Message    string
	Err        error
}

func (e *LLMError) Error() string {
	if e.StatusCode > 0 {
		return fmt.Sprintf("%s error (HTTP %d): %s", e.Provider, e.StatusCode, e.Message)
	}
	return fmt.Sprintf("%s error: %s", e.Provider, e.Message)
}

func (e *LLMError) Unwrap() error {
	return e.Err
}

// NewLLMError creates a new LLMError
func NewLLMError(provider string, statusCode int, message string, err error) *LLMError {
	return &LLMError{
		Provider:   provider,
		StatusCode: statusCode,
		Message:    message,
		Err:        err,
	}
}
</file>
<file path="internal/domain/interfaces.go">
package domain

import (
	"context"
	"net/http"
	"time"
)

// Strategy defines the interface for documentation extraction strategies
type Strategy interface {
	// Name returns the strategy name
	Name() string
	// CanHandle returns true if this strategy can handle the given URL
	CanHandle(url string) bool
	// Execute runs the extraction strategy
	Execute(ctx context.Context, url string, opts StrategyOptions) error
}

// StrategyOptions contains options for strategy execution
type StrategyOptions struct {
	CommonOptions
	Output          string
	Concurrency     int
	MaxDepth        int
	Exclude         []string
	NoFolders       bool
	Split           bool
	IncludeAssets   bool
	ContentSelector string
}

// Fetcher defines the interface for HTTP fetching with stealth capabilities
type Fetcher interface {
	// Get fetches content from a URL
	Get(ctx context.Context, url string) (*Response, error)
	// GetWithHeaders fetches content with custom headers
	GetWithHeaders(ctx context.Context, url string, headers map[string]string) (*Response, error)
	// GetCookies returns cookies for a URL (for sharing with renderer)
	GetCookies(url string) []*http.Cookie
	// Transport returns an http.RoundTripper for integration with other HTTP clients (e.g., colly)
	Transport() http.RoundTripper
	// Close releases resources
	Close() error
}

// Response represents an HTTP response
type Response struct {
	StatusCode  int
	Body        []byte
	Headers     http.Header
	ContentType string
	URL         string
	FromCache   bool
}

// Renderer defines the interface for JavaScript rendering
type Renderer interface {
	// Render fetches and renders a page with JavaScript
	Render(ctx context.Context, url string, opts RenderOptions) (string, error)
	// Close releases browser resources
	Close() error
}

// RenderOptions contains options for page rendering
type RenderOptions struct {
	Timeout     time.Duration
	WaitFor     string        // CSS selector to wait for
	WaitStable  time.Duration // Wait for network idle
	ScrollToEnd bool          // Scroll to load lazy content
	Cookies     []*http.Cookie
}

// Cache defines the interface for content caching
type Cache interface {
	// Get retrieves a value from cache
	Get(ctx context.Context, key string) ([]byte, error)
	// Set stores a value in cache with TTL
	Set(ctx context.Context, key string, value []byte, ttl time.Duration) error
	// Has checks if a key exists in cache
	Has(ctx context.Context, key string) bool
	// Delete removes a key from cache
	Delete(ctx context.Context, key string) error
	// Close releases cache resources
	Close() error
}

// Converter defines the interface for HTML to Markdown conversion
type Converter interface {
	// Convert transforms HTML content to a Document
	Convert(ctx context.Context, html string, sourceURL string) (*Document, error)
}

// Writer defines the interface for output writing
type Writer interface {
	// Write saves a document to the output directory
	Write(ctx context.Context, doc *Document) error
}

// LLMProvider defines the interface for LLM interactions
type LLMProvider interface {
	// Name returns the provider name (openai, anthropic, google)
	Name() string
	// Complete sends a request and returns the response
	Complete(ctx context.Context, req *LLMRequest) (*LLMResponse, error)
	// Close releases resources
	Close() error
}
</file>
<file path="internal/domain/models.go">
package domain

import "time"

// Document represents a processed documentation page
type Document struct {
	URL            string              `json:"url"`
	Title          string              `json:"title"`
	Description    string              `json:"description,omitempty"`
	Content        string              `json:"-"` // Markdown content (not in JSON)
	HTMLContent    string              `json:"-"` // Original HTML (not in JSON)
	FetchedAt      time.Time           `json:"fetched_at"`
	ContentHash    string              `json:"content_hash"`
	WordCount      int                 `json:"word_count"`
	CharCount      int                 `json:"char_count"`
	Links          []string            `json:"links,omitempty"`
	Headers        map[string][]string `json:"headers,omitempty"` // h1, h2, h3...
	RenderedWithJS bool                `json:"rendered_with_js"`
	SourceStrategy string              `json:"source_strategy"`
	CacheHit       bool                `json:"cache_hit"`
	RelativePath   string              `json:"-"` // Relative path for Git-sourced files (used for output structure)

	// LLM-enhanced metadata fields
	Summary  string   `json:"summary,omitempty"`  // AI-generated summary
	Tags     []string `json:"tags,omitempty"`     // AI-generated tags
	Category string   `json:"category,omitempty"` // AI-generated category
}

// Page represents a raw fetched page before conversion
type Page struct {
	URL         string
	Content     []byte
	ContentType string
	StatusCode  int
	FetchedAt   time.Time
	FromCache   bool
	RenderedJS  bool
}

// CacheEntry represents a cached page entry
type CacheEntry struct {
	URL         string    `json:"url"`
	Content     []byte    `json:"content"`
	ContentType string    `json:"content_type"`
	FetchedAt   time.Time `json:"fetched_at"`
	ExpiresAt   time.Time `json:"expires_at"`
}

// SitemapURL represents a URL entry in a sitemap
type SitemapURL struct {
	Loc        string    `xml:"loc"`
	LastMod    time.Time `xml:"-"`
	LastModStr string    `xml:"lastmod"`
	ChangeFreq string    `xml:"changefreq"`
	Priority   float64   `xml:"priority"`
}

// Sitemap represents a parsed sitemap
type Sitemap struct {
	URLs      []SitemapURL
	Sitemaps  []string // For sitemap index files
	IsIndex   bool
	SourceURL string
}

// LLMSLink represents a link parsed from llms.txt
type LLMSLink struct {
	Title string
	URL   string
}

// Deprecated: Metadata is replaced by SimpleMetadata for JSON output.
// Use SimpleMetadata for cleaner, LLM-evaluation-friendly metadata.
type Metadata struct {
	URL            string              `json:"url"`
	Title          string              `json:"title"`
	Description    string              `json:"description,omitempty"`
	FetchedAt      time.Time           `json:"fetched_at"`
	ContentHash    string              `json:"content_hash"`
	WordCount      int                 `json:"word_count"`
	CharCount      int                 `json:"char_count"`
	Links          []string            `json:"links,omitempty"`
	Headers        map[string][]string `json:"headers,omitempty"`
	RenderedWithJS bool                `json:"rendered_with_js"`
	SourceStrategy string              `json:"source_strategy"`
	CacheHit       bool                `json:"cache_hit"`
	Summary        string              `json:"summary,omitempty"`
	Tags           []string            `json:"tags,omitempty"`
	Category       string              `json:"category,omitempty"`
}

// ToMetadata converts a Document to Metadata
func (d *Document) ToMetadata() *Metadata {
	return &Metadata{
		URL:            d.URL,
		Title:          d.Title,
		Description:    d.Description,
		FetchedAt:      d.FetchedAt,
		ContentHash:    d.ContentHash,
		WordCount:      d.WordCount,
		CharCount:      d.CharCount,
		Links:          d.Links,
		Headers:        d.Headers,
		RenderedWithJS: d.RenderedWithJS,
		SourceStrategy: d.SourceStrategy,
		CacheHit:       d.CacheHit,
		Summary:        d.Summary,
		Tags:           d.Tags,
		Category:       d.Category,
	}
}

// Frontmatter represents YAML frontmatter for markdown files
type Frontmatter struct {
	Title      string    `yaml:"title"`
	URL        string    `yaml:"url"`
	Source     string    `yaml:"source"`
	FetchedAt  time.Time `yaml:"fetched_at"`
	RenderedJS bool      `yaml:"rendered_js"`
	WordCount  int       `yaml:"word_count"`
	Summary    string    `yaml:"summary,omitempty"`
	Tags       []string  `yaml:"tags,omitempty"`
	Category   string    `yaml:"category,omitempty"`
}

// ToFrontmatter converts a Document to Frontmatter
func (d *Document) ToFrontmatter() *Frontmatter {
	return &Frontmatter{
		Title:      d.Title,
		URL:        d.URL,
		Source:     d.SourceStrategy,
		FetchedAt:  d.FetchedAt,
		RenderedJS: d.RenderedWithJS,
		WordCount:  d.WordCount,
		Summary:    d.Summary,
		Tags:       d.Tags,
		Category:   d.Category,
	}
}

// Deprecated: MetadataIndex is replaced by SimpleMetadataIndex for JSON output.
type MetadataIndex struct {
	GeneratedAt    time.Time          `json:"generated_at"`
	SourceURL      string             `json:"source_url"`
	Strategy       string             `json:"strategy"`
	TotalDocuments int                `json:"total_documents"`
	TotalWordCount int                `json:"total_word_count"`
	TotalCharCount int                `json:"total_char_count"`
	Documents      []DocumentMetadata `json:"documents"`
}

// Deprecated: DocumentMetadata is replaced by SimpleDocumentMetadata for JSON output.
type DocumentMetadata struct {
	FilePath string `json:"file_path"`
	*Metadata
}

// ToDocumentMetadata creates a DocumentMetadata from a Document
func (d *Document) ToDocumentMetadata(filePath string) *DocumentMetadata {
	return &DocumentMetadata{
		FilePath: filePath,
		Metadata: d.ToMetadata(),
	}
}

// =============================================================================
// Simple Metadata Types (Simplified JSON output for LLM evaluation)
// =============================================================================

// SimpleMetadata represents simplified document metadata for JSON output
// This is a cleaner structure optimized for LLM evaluation, containing only
// essential fields without technical metadata like content_hash, word_count, etc.
type SimpleMetadata struct {
	Title       string    `json:"title"`
	URL         string    `json:"url"`
	Source      string    `json:"source"`
	FetchedAt   time.Time `json:"fetched_at"`
	Description string    `json:"description,omitempty"`
	Summary     string    `json:"summary,omitempty"`
	Tags        []string  `json:"tags,omitempty"`
	Category    string    `json:"category,omitempty"`
}

// SimpleDocumentMetadata adds file_path to SimpleMetadata for document indexing
type SimpleDocumentMetadata struct {
	FilePath string `json:"file_path"`
	*SimpleMetadata
}

// SimpleMetadataIndex represents the consolidated JSON output with simplified metadata
type SimpleMetadataIndex struct {
	GeneratedAt    time.Time                `json:"generated_at"`
	SourceURL      string                   `json:"source_url"`
	Strategy       string                   `json:"strategy"`
	TotalDocuments int                      `json:"total_documents"`
	Documents      []SimpleDocumentMetadata `json:"documents"`
}

// ToSimpleMetadata converts a Document to SimpleMetadata
func (d *Document) ToSimpleMetadata() *SimpleMetadata {
	return &SimpleMetadata{
		Title:       d.Title,
		URL:         d.URL,
		Source:      d.SourceStrategy,
		FetchedAt:   d.FetchedAt,
		Description: d.Description,
		Summary:     d.Summary,
		Tags:        d.Tags,
		Category:    d.Category,
	}
}

// ToSimpleDocumentMetadata creates a SimpleDocumentMetadata from a Document
func (d *Document) ToSimpleDocumentMetadata(filePath string) *SimpleDocumentMetadata {
	return &SimpleDocumentMetadata{
		FilePath:       filePath,
		SimpleMetadata: d.ToSimpleMetadata(),
	}
}

// =============================================================================
// LLM Types
// =============================================================================

// MessageRole represents the role in a conversation
type MessageRole string

const (
	// RoleSystem represents a system message
	RoleSystem MessageRole = "system"
	// RoleUser represents a user message
	RoleUser MessageRole = "user"
	// RoleAssistant represents an assistant message
	RoleAssistant MessageRole = "assistant"
)

// LLMMessage represents a message in the conversation
type LLMMessage struct {
	Role    MessageRole
	Content string
}

// LLMRequest represents a completion request
type LLMRequest struct {
	Messages    []LLMMessage
	MaxTokens   int      // 0 = use provider default
	Temperature *float64 // nil = use provider default
}

// LLMResponse represents the LLM response
type LLMResponse struct {
	Content      string
	Model        string
	FinishReason string
	Usage        LLMUsage
}

// LLMUsage contains token usage statistics
type LLMUsage struct {
	PromptTokens     int
	CompletionTokens int
	TotalTokens      int
}
</file>
<file path="internal/domain/options.go">
package domain

// CommonOptions contains shared configuration options for strategies and orchestration.
type CommonOptions struct {
	Verbose  bool
	DryRun   bool
	Force    bool
	RenderJS bool
	Limit    int
	Sync     bool
	FullSync bool
	Prune    bool
}

// DefaultCommonOptions returns CommonOptions with default values.
func DefaultCommonOptions() CommonOptions {
	return CommonOptions{}
}
</file>
<file path="internal/fetcher/client.go">
package fetcher

import (
	"context"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"time"

	fhttp "github.com/bogdanfinn/fhttp"
	tls_client "github.com/bogdanfinn/tls-client"
	"github.com/bogdanfinn/tls-client/profiles"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Client is a stealth HTTP client using tls-client
type Client struct {
	tlsClient    tls_client.HttpClient
	userAgent    string
	retrier      *Retrier
	cache        domain.Cache
	cacheEnabled bool
	cacheTTL     time.Duration
}

// ClientOptions contains options for creating a Client
type ClientOptions struct {
	Timeout     time.Duration
	MaxRetries  int
	EnableCache bool
	CacheTTL    time.Duration
	Cache       domain.Cache
	UserAgent   string
	ProxyURL    string
}

// DefaultClientOptions returns default client options
func DefaultClientOptions() ClientOptions {
	return ClientOptions{
		Timeout:     30 * time.Second,
		MaxRetries:  3,
		EnableCache: true,
		CacheTTL:    24 * time.Hour,
		UserAgent:   "",
		ProxyURL:    "",
	}
}

// NewClient creates a new stealth HTTP client
func NewClient(opts ClientOptions) (*Client, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = 30 * time.Second
	}

	// TLS client options
	tlsOpts := []tls_client.HttpClientOption{
		tls_client.WithTimeoutSeconds(int(opts.Timeout.Seconds())),
		tls_client.WithClientProfile(profiles.Chrome_131),
		tls_client.WithRandomTLSExtensionOrder(),
		tls_client.WithNotFollowRedirects(),
	}

	if opts.ProxyURL != "" {
		tlsOpts = append(tlsOpts, tls_client.WithProxyUrl(opts.ProxyURL))
	}

	tlsClient, err := tls_client.NewHttpClient(tls_client.NewNoopLogger(), tlsOpts...)
	if err != nil {
		return nil, fmt.Errorf("failed to create tls client: %w", err)
	}

	// Create retrier
	retrier := NewRetrier(RetrierOptions{
		MaxRetries:      opts.MaxRetries,
		InitialInterval: 1 * time.Second,
		MaxInterval:     30 * time.Second,
		Multiplier:      2.0,
	})

	return &Client{
		tlsClient:    tlsClient,
		userAgent:    opts.UserAgent,
		retrier:      retrier,
		cache:        opts.Cache,
		cacheEnabled: opts.EnableCache,
		cacheTTL:     opts.CacheTTL,
	}, nil
}

// Get fetches content from a URL
func (c *Client) Get(ctx context.Context, url string) (*domain.Response, error) {
	return c.GetWithHeaders(ctx, url, nil)
}

// GetWithHeaders fetches content with custom headers
func (c *Client) GetWithHeaders(ctx context.Context, url string, extraHeaders map[string]string) (*domain.Response, error) {
	// Check cache first
	if c.cacheEnabled && c.cache != nil {
		cached, err := c.getFromCache(ctx, url)
		if err == nil && cached != nil {
			return cached, nil
		}
	}

	// Perform request with retry
	var resp *domain.Response
	err := c.retrier.Retry(ctx, func() error {
		var err error
		resp, err = c.doRequest(ctx, url, extraHeaders)
		return err
	})

	if err != nil {
		return nil, err
	}

	// Cache the response
	if c.cacheEnabled && c.cache != nil && resp != nil {
		_ = c.saveToCache(ctx, url, resp)
	}

	return resp, nil
}

// doRequest performs the actual HTTP request
func (c *Client) doRequest(ctx context.Context, targetURL string, extraHeaders map[string]string) (*domain.Response, error) {
	// Create request using fhttp (tls-client's http package)
	req, err := fhttp.NewRequest(fhttp.MethodGet, targetURL, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	// Apply stealth headers
	headers := StealthHeaders(c.userAgent)
	for k, v := range headers {
		req.Header.Set(k, v)
	}

	// Apply extra headers
	for k, v := range extraHeaders {
		req.Header.Set(k, v)
	}

	// Perform request
	resp, err := c.tlsClient.Do(req)
	if err != nil {
		return nil, &domain.FetchError{
			URL: targetURL,
			Err: fmt.Errorf("request failed: %w", err),
		}
	}
	defer resp.Body.Close()

	// Check for error status codes
	if resp.StatusCode >= 400 {
		if ShouldRetryStatus(resp.StatusCode) {
			return nil, &domain.RetryableError{
				Err:        &domain.FetchError{URL: targetURL, StatusCode: resp.StatusCode, Err: fmt.Errorf("HTTP %d", resp.StatusCode)},
				RetryAfter: int(ParseRetryAfter(resp.Header.Get("Retry-After")).Seconds()),
			}
		}
		return nil, &domain.FetchError{
			URL:        targetURL,
			StatusCode: resp.StatusCode,
			Err:        fmt.Errorf("HTTP %d", resp.StatusCode),
		}
	}

	// Read body
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	// Convert fhttp.Header to http.Header
	httpHeaders := make(http.Header)
	for k, v := range resp.Header {
		httpHeaders[k] = v
	}

	return &domain.Response{
		StatusCode:  resp.StatusCode,
		Body:        body,
		Headers:     httpHeaders,
		ContentType: resp.Header.Get("Content-Type"),
		URL:         targetURL,
		FromCache:   false,
	}, nil
}

// GetCookies returns cookies for a URL (for sharing with renderer)
func (c *Client) GetCookies(rawURL string) []*http.Cookie {
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		return nil
	}
	cookies := c.tlsClient.GetCookies(parsedURL)
	result := make([]*http.Cookie, len(cookies))
	for i, cookie := range cookies {
		result[i] = &http.Cookie{
			Name:     cookie.Name,
			Value:    cookie.Value,
			Path:     cookie.Path,
			Domain:   cookie.Domain,
			Expires:  cookie.Expires,
			Secure:   cookie.Secure,
			HttpOnly: cookie.HttpOnly,
		}
	}
	return result
}

// Close releases client resources
func (c *Client) Close() error {
	// TLS client doesn't have a Close method, but we keep this for interface compliance
	return nil
}

// getFromCache retrieves a response from cache
func (c *Client) getFromCache(ctx context.Context, url string) (*domain.Response, error) {
	if c.cache == nil {
		return nil, domain.ErrCacheMiss
	}

	data, err := c.cache.Get(ctx, url)
	if err != nil {
		return nil, err
	}

	return &domain.Response{
		StatusCode:  200,
		Body:        data,
		ContentType: "text/html",
		URL:         url,
		FromCache:   true,
	}, nil
}

// saveToCache saves a response to cache
func (c *Client) saveToCache(ctx context.Context, url string, resp *domain.Response) error {
	if c.cache == nil {
		return nil
	}
	return c.cache.Set(ctx, url, resp.Body, c.cacheTTL)
}

// SetCache sets the cache implementation
func (c *Client) SetCache(cache domain.Cache) {
	c.cache = cache
}

// SetCacheEnabled enables or disables caching
func (c *Client) SetCacheEnabled(enabled bool) {
	c.cacheEnabled = enabled
}
</file>
<file path="internal/fetcher/retry.go">
package fetcher

import (
	"context"
	"time"

	"github.com/cenkalti/backoff/v4"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Retrier handles retry logic with exponential backoff
type Retrier struct {
	maxRetries      int
	initialInterval time.Duration
	maxInterval     time.Duration
	multiplier      float64
}

// RetrierOptions contains options for creating a Retrier
type RetrierOptions struct {
	MaxRetries      int
	InitialInterval time.Duration
	MaxInterval     time.Duration
	Multiplier      float64
}

// DefaultRetrierOptions returns default retrier options
func DefaultRetrierOptions() RetrierOptions {
	return RetrierOptions{
		MaxRetries:      3,
		InitialInterval: 1 * time.Second,
		MaxInterval:     30 * time.Second,
		Multiplier:      2.0,
	}
}

// NewRetrier creates a new Retrier with the given options
func NewRetrier(opts RetrierOptions) *Retrier {
	if opts.MaxRetries <= 0 {
		opts.MaxRetries = 3
	}
	if opts.InitialInterval <= 0 {
		opts.InitialInterval = 1 * time.Second
	}
	if opts.MaxInterval <= 0 {
		opts.MaxInterval = 30 * time.Second
	}
	if opts.Multiplier <= 0 {
		opts.Multiplier = 2.0
	}

	return &Retrier{
		maxRetries:      opts.MaxRetries,
		initialInterval: opts.InitialInterval,
		maxInterval:     opts.MaxInterval,
		multiplier:      opts.Multiplier,
	}
}

// newBackoff creates a new exponential backoff
func (r *Retrier) newBackoff() backoff.BackOff {
	b := backoff.NewExponentialBackOff()
	b.InitialInterval = r.initialInterval
	b.MaxInterval = r.maxInterval
	b.Multiplier = r.multiplier
	b.RandomizationFactor = 0.5
	b.Reset()

	return backoff.WithMaxRetries(b, uint64(r.maxRetries))
}

// Retry executes an operation with exponential backoff
func (r *Retrier) Retry(ctx context.Context, operation func() error) error {
	b := r.newBackoff()
	b = backoff.WithContext(b, ctx)

	return backoff.Retry(func() error {
		err := operation()
		if err == nil {
			return nil
		}

		// Check if error is retryable
		if !domain.IsRetryable(err) {
			return backoff.Permanent(err)
		}

		return err
	}, b)
}

// RetryWithValue executes an operation with exponential backoff and returns a value
func RetryWithValue[T any](ctx context.Context, r *Retrier, operation func() (T, error)) (T, error) {
	var result T
	var lastErr error

	b := r.newBackoff()
	b = backoff.WithContext(b, ctx)

	err := backoff.Retry(func() error {
		var err error
		result, err = operation()
		if err == nil {
			return nil
		}

		lastErr = err

		// Check if error is retryable
		if !domain.IsRetryable(err) {
			return backoff.Permanent(err)
		}

		return err
	}, b)

	if err != nil {
		return result, lastErr
	}

	return result, nil
}

// ShouldRetryStatus returns true if the HTTP status code should be retried
func ShouldRetryStatus(statusCode int) bool {
	switch statusCode {
	case 429: // Too Many Requests
		return true
	case 502: // Bad Gateway
		return true
	case 503: // Service Unavailable
		return true
	case 504: // Gateway Timeout
		return true
	}

	// Cloudflare errors (520-530)
	if statusCode >= 520 && statusCode <= 530 {
		return true
	}

	return false
}

// ParseRetryAfter parses the Retry-After header value
func ParseRetryAfter(retryAfter string) time.Duration {
	if retryAfter == "" {
		return 0
	}

	// Try to parse as seconds
	var seconds int
	if _, err := parseRetryAfterInt(retryAfter, &seconds); err == nil && seconds > 0 {
		return time.Duration(seconds) * time.Second
	}

	// Try to parse as HTTP date (simplified)
	// Full parsing would use time.Parse with HTTP date format
	return 0
}

// parseRetryAfterInt is a helper to parse retry-after as int
func parseRetryAfterInt(s string, result *int) (int, error) {
	n := 0
	for _, c := range s {
		if c < '0' || c > '9' {
			break
		}
		n = n*10 + int(c-'0')
	}
	*result = n
	return n, nil
}
</file>
<file path="internal/fetcher/stealth.go">
package fetcher

import (
	"math/rand"
	"time"
)

// UserAgents is a pool of real Chrome/Firefox/Safari user agents
var UserAgents = []string{
	// Chrome on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
	// Chrome on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	// Chrome on Linux
	"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	// Firefox on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Firefox on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Firefox on Linux
	"Mozilla/5.0 (X11; Linux x86_64; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (X11; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Safari on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15",
	// Edge on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0",
}

// AcceptLanguages are common Accept-Language header values
var AcceptLanguages = []string{
	"en-US,en;q=0.9",
	"en-GB,en;q=0.9,en-US;q=0.8",
	"en-US,en;q=0.9,es;q=0.8",
	"en-US,en;q=0.9,de;q=0.8",
	"en-US,en;q=0.9,fr;q=0.8",
	"en,en-US;q=0.9",
}

// SecChUaPlatforms are Sec-CH-UA-Platform header values
var SecChUaPlatforms = []string{
	`"Windows"`,
	`"macOS"`,
	`"Linux"`,
}

// init seeds the random number generator
func init() {
	rand.Seed(time.Now().UnixNano())
}

// RandomUserAgent returns a random user agent from the pool
func RandomUserAgent() string {
	return UserAgents[rand.Intn(len(UserAgents))]
}

// RandomAcceptLanguage returns a random Accept-Language header value
func RandomAcceptLanguage() string {
	return AcceptLanguages[rand.Intn(len(AcceptLanguages))]
}

// RandomSecChUaPlatform returns a random Sec-CH-UA-Platform header value
func RandomSecChUaPlatform() string {
	return SecChUaPlatforms[rand.Intn(len(SecChUaPlatforms))]
}

// StealthHeaders returns a map of stealth headers for HTTP requests
func StealthHeaders(userAgent string) map[string]string {
	if userAgent == "" {
		userAgent = RandomUserAgent()
	}

	headers := map[string]string{
		"User-Agent":                userAgent,
		"Accept":                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
		"Accept-Language":           RandomAcceptLanguage(),
		"Accept-Encoding":           "gzip, deflate, br",
		"Cache-Control":             "no-cache",
		"Pragma":                    "no-cache",
		"Sec-Fetch-Dest":            "document",
		"Sec-Fetch-Mode":            "navigate",
		"Sec-Fetch-Site":            "none",
		"Sec-Fetch-User":            "?1",
		"Upgrade-Insecure-Requests": "1",
	}

	// Add Chrome-specific headers if using Chrome UA
	if isChrome(userAgent) {
		headers["Sec-CH-UA"] = `"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"`
		headers["Sec-CH-UA-Mobile"] = "?0"
		headers["Sec-CH-UA-Platform"] = RandomSecChUaPlatform()
	}

	return headers
}

// isChrome checks if the user agent is Chrome
func isChrome(userAgent string) bool {
	return len(userAgent) > 0 && (contains(userAgent, "Chrome") || contains(userAgent, "Chromium"))
}

// contains is a simple string contains check
func contains(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}

// RandomDelay returns a random delay between min and max duration
func RandomDelay(min, max time.Duration) time.Duration {
	if min >= max {
		return min
	}
	delta := max - min
	return min + time.Duration(rand.Int63n(int64(delta)))
}
</file>
<file path="internal/fetcher/transport.go">
package fetcher

import (
	"bytes"
	"io"
	"net/http"
)

// StealthTransport is an http.RoundTripper that uses the stealth client
// This allows integration with Colly and other HTTP client libraries
type StealthTransport struct {
	client *Client
}

// NewStealthTransport creates a new StealthTransport
func NewStealthTransport(client *Client) *StealthTransport {
	return &StealthTransport{client: client}
}

// RoundTrip implements http.RoundTripper
func (t *StealthTransport) RoundTrip(req *http.Request) (*http.Response, error) {
	// Extract headers from request
	extraHeaders := make(map[string]string)
	for k, v := range req.Header {
		if len(v) > 0 {
			extraHeaders[k] = v[0]
		}
	}

	// Use the stealth client to make the request
	resp, err := t.client.GetWithHeaders(req.Context(), req.URL.String(), extraHeaders)
	if err != nil {
		return nil, err
	}

	// Convert domain.Response to http.Response
	// IMPORTANT: We must strip Content-Encoding header because we are returning
	// the already decompressed body. If we leave it, the caller (e.g. Colly)
	// will try to decompress it again and fail with "gzip: invalid header".
	resp.Headers.Del("Content-Encoding")

	return &http.Response{
		Status: http.StatusText(resp.StatusCode),

		StatusCode:    resp.StatusCode,
		Proto:         "HTTP/1.1",
		ProtoMajor:    1,
		ProtoMinor:    1,
		Header:        resp.Headers,
		Body:          io.NopCloser(bytes.NewReader(resp.Body)),
		ContentLength: int64(len(resp.Body)),
		Request:       req,
	}, nil
}

// Transport returns the StealthTransport as http.RoundTripper
func (c *Client) Transport() http.RoundTripper {
	return NewStealthTransport(c)
}
</file>
<file path="internal/git/client.go">
package git

import (
	"context"

	"github.com/go-git/go-git/v5"
)

// RealClient implements Client using go-git
type RealClient struct{}

// NewClient creates a new RealClient
func NewClient() *RealClient {
	return &RealClient{}
}

// PlainCloneContext calls git.PlainCloneContext
func (c *RealClient) PlainCloneContext(ctx context.Context, path string, isBare bool, o *git.CloneOptions) (*git.Repository, error) {
	return git.PlainCloneContext(ctx, path, isBare, o)
}
</file>
<file path="internal/git/interface.go">
package git

import (
	"context"

	"github.com/go-git/go-git/v5"
)

// Client defines the interface for Git operations
type Client interface {
	PlainCloneContext(ctx context.Context, path string, isBare bool, o *git.CloneOptions) (*git.Repository, error)
}
</file>
<file path="internal/llm/anthropic.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

const anthropicVersion = "2023-06-01"

type anthropicRequest struct {
	Model     string             `json:"model"`
	MaxTokens int                `json:"max_tokens"`
	Messages  []anthropicMessage `json:"messages"`
	System    string             `json:"system,omitempty"`
}

type anthropicMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type anthropicResponse struct {
	ID      string `json:"id"`
	Type    string `json:"type"`
	Role    string `json:"role"`
	Model   string `json:"model"`
	Content []struct {
		Type string `json:"type"`
		Text string `json:"text"`
	} `json:"content"`
	StopReason string `json:"stop_reason"`
	Usage      struct {
		InputTokens  int `json:"input_tokens"`
		OutputTokens int `json:"output_tokens"`
	} `json:"usage"`
	Error *struct {
		Type    string `json:"type"`
		Message string `json:"message"`
	} `json:"error,omitempty"`
}

type AnthropicProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewAnthropicProvider(cfg ProviderConfig, httpClient *http.Client) (*AnthropicProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	maxTokens := cfg.MaxTokens
	if maxTokens == 0 {
		maxTokens = 4096
	}

	return &AnthropicProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   maxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *AnthropicProvider) Name() string {
	return "anthropic"
}

func (p *AnthropicProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	var systemPrompt string
	messages := make([]anthropicMessage, 0, len(req.Messages))

	for _, msg := range req.Messages {
		if msg.Role == domain.RoleSystem {
			systemPrompt = msg.Content
		} else {
			messages = append(messages, anthropicMessage{
				Role:    string(msg.Role),
				Content: msg.Content,
			})
		}
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	anthropicReq := anthropicRequest{
		Model:     p.model,
		MaxTokens: maxTokens,
		Messages:  messages,
		System:    systemPrompt,
	}

	body, err := json.Marshal(anthropicReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := p.baseURL + "/v1/messages"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-api-key", p.apiKey)
	httpReq.Header.Set("anthropic-version", anthropicVersion)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "anthropic",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for non-OK status codes first
	if resp.StatusCode != http.StatusOK {
		// Try to parse error response
		var anthropicResp anthropicResponse
		if parseErr := json.Unmarshal(respBody, &anthropicResp); parseErr == nil && anthropicResp.Error != nil {
			// Successfully parsed error response
			// Check if this is a rate limit error based on status code or error type
			if resp.StatusCode == http.StatusTooManyRequests || anthropicResp.Error.Type == "rate_limit_error" {
				return nil, &domain.LLMError{
					Provider:   "anthropic",
					StatusCode: resp.StatusCode,
					Message:    anthropicResp.Error.Message,
					Err:        domain.ErrLLMRateLimited,
				}
			}
			// Check if this is an authentication error
			if resp.StatusCode == http.StatusUnauthorized || anthropicResp.Error.Type == "authentication_error" {
				return nil, &domain.LLMError{
					Provider:   "anthropic",
					StatusCode: resp.StatusCode,
					Message:    anthropicResp.Error.Message,
					Err:        domain.ErrLLMAuthFailed,
				}
			}
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMRequestFailed,
			}
		}
		// Failed to parse error response or no error field, use handleHTTPError
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	var anthropicResp anthropicResponse
	if err := json.Unmarshal(respBody, &anthropicResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check if response contains an error field even with status 200
	if anthropicResp.Error != nil {
		// Check if this is a rate limit error based on error type
		if anthropicResp.Error.Type == "rate_limit_error" {
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMRateLimited,
			}
		}
		// Check if this is an authentication error
		if anthropicResp.Error.Type == "authentication_error" {
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMAuthFailed,
			}
		}
		return nil, &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: resp.StatusCode,
			Message:    anthropicResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	var sb strings.Builder
	for _, block := range anthropicResp.Content {
		if block.Type == "text" {
			sb.WriteString(block.Text)
		}
	}
	content := sb.String()

	return &domain.LLMResponse{
		Content:      content,
		Model:        anthropicResp.Model,
		FinishReason: anthropicResp.StopReason,
		Usage: domain.LLMUsage{
			PromptTokens:     anthropicResp.Usage.InputTokens,
			CompletionTokens: anthropicResp.Usage.OutputTokens,
			TotalTokens:      anthropicResp.Usage.InputTokens + anthropicResp.Usage.OutputTokens,
		},
	}, nil
}

func (p *AnthropicProvider) Close() error {
	return nil
}

func (p *AnthropicProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/circuit_breaker.go">
package llm

import (
	"sync"
	"time"
)

// CircuitState represents the state of a circuit breaker
type CircuitState int

const (
	StateClosed CircuitState = iota
	StateOpen
	StateHalfOpen
)

func (s CircuitState) String() string {
	switch s {
	case StateClosed:
		return "closed"
	case StateOpen:
		return "open"
	case StateHalfOpen:
		return "half-open"
	default:
		return "unknown"
	}
}

// CircuitBreaker protects against cascading failures
type CircuitBreaker interface {
	Allow() bool
	RecordSuccess()
	RecordFailure()
	State() CircuitState
}

// CircuitBreakerConfig holds circuit breaker configuration
type CircuitBreakerConfig struct {
	FailureThreshold         int
	SuccessThresholdHalfOpen int
	ResetTimeout             time.Duration
}

// DefaultCircuitBreakerConfig returns sensible defaults
func DefaultCircuitBreakerConfig() CircuitBreakerConfig {
	return CircuitBreakerConfig{
		FailureThreshold:         5,
		SuccessThresholdHalfOpen: 1,
		ResetTimeout:             30 * time.Second,
	}
}

type circuitBreaker struct {
	config          CircuitBreakerConfig
	state           CircuitState
	failures        int
	successes       int
	lastStateChange time.Time
	mu              sync.RWMutex
}

// NewCircuitBreaker creates a new circuit breaker
func NewCircuitBreaker(config CircuitBreakerConfig) CircuitBreaker {
	if config.FailureThreshold <= 0 {
		config.FailureThreshold = 5
	}
	if config.SuccessThresholdHalfOpen <= 0 {
		config.SuccessThresholdHalfOpen = 1
	}
	if config.ResetTimeout <= 0 {
		config.ResetTimeout = 30 * time.Second
	}

	return &circuitBreaker{
		config:          config,
		state:           StateClosed,
		lastStateChange: time.Now(),
	}
}

// Allow checks if a request is allowed to proceed
func (cb *circuitBreaker) Allow() bool {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	switch cb.state {
	case StateClosed:
		return true
	case StateOpen:
		if time.Since(cb.lastStateChange) >= cb.config.ResetTimeout {
			cb.transitionTo(StateHalfOpen)
			return true
		}
		return false
	case StateHalfOpen:
		return true
	default:
		return false
	}
}

// RecordSuccess records a successful operation
func (cb *circuitBreaker) RecordSuccess() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	cb.failures = 0

	switch cb.state {
	case StateHalfOpen:
		cb.successes++
		if cb.successes >= cb.config.SuccessThresholdHalfOpen {
			cb.transitionTo(StateClosed)
		}
	case StateClosed:
		cb.successes++
	}
}

// RecordFailure records a failed operation
func (cb *circuitBreaker) RecordFailure() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	cb.successes = 0

	switch cb.state {
	case StateClosed:
		cb.failures++
		if cb.failures >= cb.config.FailureThreshold {
			cb.transitionTo(StateOpen)
		}
	case StateHalfOpen:
		cb.transitionTo(StateOpen)
	}
}

// State returns the current state
func (cb *circuitBreaker) State() CircuitState {
	cb.mu.RLock()
	defer cb.mu.RUnlock()
	return cb.state
}

func (cb *circuitBreaker) transitionTo(newState CircuitState) {
	cb.state = newState
	cb.lastStateChange = time.Now()
	cb.failures = 0
	cb.successes = 0
}

// NoOpCircuitBreaker always allows requests
type NoOpCircuitBreaker struct{}

// Allow always returns true
func (n *NoOpCircuitBreaker) Allow() bool {
	return true
}

// RecordSuccess does nothing
func (n *NoOpCircuitBreaker) RecordSuccess() {}

// RecordFailure does nothing
func (n *NoOpCircuitBreaker) RecordFailure() {}

// State always returns StateClosed
func (n *NoOpCircuitBreaker) State() CircuitState {
	return StateClosed
}
</file>
<file path="internal/llm/google.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type googleRequest struct {
	Contents          []googleContent  `json:"contents"`
	SystemInstruction *googleContent   `json:"systemInstruction,omitempty"`
	GenerationConfig  *googleGenConfig `json:"generationConfig,omitempty"`
}

type googleContent struct {
	Role  string       `json:"role,omitempty"`
	Parts []googlePart `json:"parts"`
}

type googlePart struct {
	Text string `json:"text"`
}

type googleGenConfig struct {
	MaxOutputTokens int     `json:"maxOutputTokens,omitempty"`
	Temperature     float64 `json:"temperature,omitempty"`
}

type googleResponse struct {
	Candidates []struct {
		Content struct {
			Role  string `json:"role"`
			Parts []struct {
				Text string `json:"text"`
			} `json:"parts"`
		} `json:"content"`
		FinishReason string `json:"finishReason"`
	} `json:"candidates"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount"`
		CandidatesTokenCount int `json:"candidatesTokenCount"`
		TotalTokenCount      int `json:"totalTokenCount"`
	} `json:"usageMetadata"`
	Error *struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Status  string `json:"status"`
	} `json:"error,omitempty"`
}

type GoogleProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewGoogleProvider(cfg ProviderConfig, httpClient *http.Client) (*GoogleProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	return &GoogleProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   cfg.MaxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *GoogleProvider) Name() string {
	return "google"
}

func (p *GoogleProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	var systemInstruction *googleContent
	contents := make([]googleContent, 0, len(req.Messages))

	for _, msg := range req.Messages {
		switch msg.Role {
		case domain.RoleSystem:
			systemInstruction = &googleContent{
				Parts: []googlePart{{Text: msg.Content}},
			}
		case domain.RoleUser:
			contents = append(contents, googleContent{
				Role:  "user",
				Parts: []googlePart{{Text: msg.Content}},
			})
		case domain.RoleAssistant:
			contents = append(contents, googleContent{
				Role:  "model",
				Parts: []googlePart{{Text: msg.Content}},
			})
		}
	}

	googleReq := googleRequest{
		Contents:          contents,
		SystemInstruction: systemInstruction,
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	temp := p.temperature
	if req.Temperature != nil {
		temp = *req.Temperature
	}

	if maxTokens > 0 || temp > 0 {
		googleReq.GenerationConfig = &googleGenConfig{
			MaxOutputTokens: maxTokens,
			Temperature:     temp,
		}
	}

	body, err := json.Marshal(googleReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/v1beta/models/%s:generateContent", p.baseURL, p.model)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-goog-api-key", p.apiKey)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "google",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		var googleResp googleResponse
		if json.Unmarshal(respBody, &googleResp) == nil && googleResp.Error != nil {
			if resp.StatusCode == http.StatusTooManyRequests || googleResp.Error.Status == "RESOURCE_EXHAUSTED" {
				return nil, &domain.LLMError{
					Provider:   "google",
					StatusCode: googleResp.Error.Code,
					Message:    googleResp.Error.Message,
					Err:        domain.ErrLLMRateLimited,
				}
			}
			return nil, &domain.LLMError{
				Provider:   "google",
				StatusCode: googleResp.Error.Code,
				Message:    googleResp.Error.Message,
				Err:        domain.ErrLLMRequestFailed,
			}
		}
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	var googleResp googleResponse
	if err := json.Unmarshal(respBody, &googleResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if googleResp.Error != nil {
		return nil, &domain.LLMError{
			Provider:   "google",
			StatusCode: googleResp.Error.Code,
			Message:    googleResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	if len(googleResp.Candidates) == 0 {
		return nil, &domain.LLMError{
			Provider: "google",
			Message:  "no candidates in response",
		}
	}

	candidate := googleResp.Candidates[0]

	var sb strings.Builder
	for _, part := range candidate.Content.Parts {
		sb.WriteString(part.Text)
	}
	content := sb.String()

	return &domain.LLMResponse{
		Content:      content,
		Model:        p.model,
		FinishReason: candidate.FinishReason,
		Usage: domain.LLMUsage{
			PromptTokens:     googleResp.UsageMetadata.PromptTokenCount,
			CompletionTokens: googleResp.UsageMetadata.CandidatesTokenCount,
			TotalTokens:      googleResp.UsageMetadata.TotalTokenCount,
		},
	}, nil
}

func (p *GoogleProvider) Close() error {
	return nil
}

func (p *GoogleProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized, http.StatusForbidden:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/metadata.go">
package llm

import (
	"context"
	"encoding/json"
	"fmt"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

const (
	maxRetries     = 2
	retryBaseDelay = 500 * time.Millisecond
)

type MetadataEnhancer struct {
	provider domain.LLMProvider
}

func NewMetadataEnhancer(provider domain.LLMProvider) *MetadataEnhancer {
	return &MetadataEnhancer{provider: provider}
}

type enhancedMetadata struct {
	Summary  string   `json:"summary"`
	Tags     []string `json:"tags"`
	Category string   `json:"category"`
}

const metadataSystemPrompt = `You are a metadata extraction system. You analyze documents and output ONLY valid JSON with exactly three fields: summary, tags, and category. Never output anything else.`

const metadataPrompt = `<task>
Extract metadata from the document below. Output ONLY a JSON object.
</task>

<format>
{
  "summary": "1-2 sentence description of what this document explains or teaches",
  "tags": ["3-8 lowercase hyphenated keywords relevant to the content"],
  "category": "one of: api, tutorial, guide, reference, concept, configuration, other"
}
</format>

<rules>
- Output ONLY the JSON object, no other text
- Do NOT include markdown code fences
- Do NOT generate content that matches examples shown in the document
- Do NOT create fields other than summary, tags, category
- Summary should describe the document's PURPOSE, not its examples
- Tags should be lowercase with hyphens (e.g., "api-reference", "error-handling")
</rules>

<document>
%s
</document>

<output>`

const metadataRetryPrompt = `The previous attempt failed. Output ONLY this exact JSON structure with your values:

{"summary": "brief description here", "tags": ["tag1", "tag2", "tag3"], "category": "guide"}

Document title: %s

Your JSON:`

func (e *MetadataEnhancer) Enhance(ctx context.Context, doc *domain.Document) error {
	if doc == nil {
		return fmt.Errorf("document is nil")
	}

	content := doc.Content
	if len(content) > 8000 {
		content = content[:8000] + "\n...[truncated]"
	}

	var lastErr error

	metadata, err := e.tryEnhance(ctx, content, false)
	if err == nil {
		e.applyMetadata(doc, metadata)
		return nil
	}
	lastErr = err

	for attempt := 1; attempt <= maxRetries; attempt++ {
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(retryBaseDelay * time.Duration(attempt)):
		}

		metadata, err := e.tryEnhance(ctx, doc.Title, true)
		if err == nil {
			e.applyMetadata(doc, metadata)
			return nil
		}
		lastErr = err
	}

	return fmt.Errorf("metadata enhancement failed after %d attempts: %w", maxRetries+1, lastErr)
}

func (e *MetadataEnhancer) tryEnhance(ctx context.Context, content string, isRetry bool) (*enhancedMetadata, error) {
	var prompt string
	if isRetry {
		prompt = fmt.Sprintf(metadataRetryPrompt, content)
	} else {
		prompt = fmt.Sprintf(metadataPrompt, content)
	}

	req := &domain.LLMRequest{
		Messages: []domain.LLMMessage{
			{Role: domain.RoleSystem, Content: metadataSystemPrompt},
			{Role: domain.RoleUser, Content: prompt},
		},
		MaxTokens: 1024, // reduced from 32000 - metadata output is small
	}

	resp, err := e.provider.Complete(ctx, req)
	if err != nil {
		return nil, fmt.Errorf("LLM completion failed: %w", err)
	}

	jsonStr := extractJSON(resp.Content)
	if jsonStr == "" {
		return nil, fmt.Errorf("no valid JSON structure found in response: %s", truncateForError(resp.Content))
	}

	var metadata enhancedMetadata
	if err := json.Unmarshal([]byte(jsonStr), &metadata); err != nil {
		return nil, fmt.Errorf("JSON unmarshal failed: %w (extracted: %s)", err, truncateForError(jsonStr))
	}

	return &metadata, nil
}

func (e *MetadataEnhancer) applyMetadata(doc *domain.Document, metadata *enhancedMetadata) {
	doc.Summary = metadata.Summary
	doc.Tags = metadata.Tags
	doc.Category = metadata.Category
}

func extractJSON(text string) string {
	text = strings.TrimSpace(text)

	if candidate := tryExtractAndValidate(text); candidate != "" {
		return candidate
	}

	stripped := stripMarkdownCodeBlocks(text)
	if candidate := tryExtractAndValidate(stripped); candidate != "" {
		return candidate
	}

	if jsonObj := findJSONObjectByBraceMatching(stripped); jsonObj != "" {
		if candidate := tryExtractAndValidate(jsonObj); candidate != "" {
			return candidate
		}
	}

	if jsonObj := findJSONObjectByBraceMatching(text); jsonObj != "" {
		if candidate := tryExtractAndValidate(jsonObj); candidate != "" {
			return candidate
		}
	}

	return ""
}

func tryExtractAndValidate(text string) string {
	if !strings.HasPrefix(text, "{") {
		return ""
	}

	if !json.Valid([]byte(text)) {
		return ""
	}

	var check map[string]interface{}
	if err := json.Unmarshal([]byte(text), &check); err != nil {
		return ""
	}

	if _, ok := check["summary"]; !ok {
		return ""
	}
	if _, ok := check["tags"]; !ok {
		return ""
	}
	if _, ok := check["category"]; !ok {
		return ""
	}

	if _, ok := check["summary"].(string); !ok {
		return ""
	}
	if _, ok := check["tags"].([]interface{}); !ok {
		return ""
	}
	if _, ok := check["category"].(string); !ok {
		return ""
	}

	return text
}

// codeBlockRegex: (?s)^\x60{3}\s*(?:json|JSON)?\s*\n?(.*?)\n?\x60{3}$
// Matches markdown fences with optional json/JSON (and optional space), captures inner content
var codeBlockRegex = regexp.MustCompile(`(?s)^\x60\x60\x60\s*(?:json|JSON)?\s*\n?(.*?)\n?\x60\x60\x60$`)

func stripMarkdownCodeBlocks(text string) string {
	text = strings.TrimSpace(text)

	if matches := codeBlockRegex.FindStringSubmatch(text); len(matches) == 2 {
		return strings.TrimSpace(matches[1])
	}

	for _, prefix := range []string{"```json", "```JSON", "``` json", "```"} {
		text = strings.TrimPrefix(text, prefix)
	}
	text = strings.TrimSuffix(text, "```")
	text = strings.TrimSpace(text)

	if strings.HasPrefix(text, "\n") {
		text = strings.TrimPrefix(text, "\n")
	}

	return strings.TrimSpace(text)
}

func findJSONObjectByBraceMatching(text string) string {
	start := strings.Index(text, "{")
	if start == -1 {
		return ""
	}

	depth := 0
	inString := false
	escaped := false

	for i := start; i < len(text); i++ {
		c := text[i]

		if escaped {
			escaped = false
			continue
		}

		if c == '\\' && inString {
			escaped = true
			continue
		}

		if c == '"' {
			inString = !inString
			continue
		}

		if inString {
			continue
		}

		switch c {
		case '{':
			depth++
		case '}':
			depth--
			if depth == 0 {
				candidate := text[start : i+1]
				if json.Valid([]byte(candidate)) {
					return candidate
				}
			}
		}
	}

	return ""
}

func truncateForError(s string) string {
	const maxLen = 200
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}

func (e *MetadataEnhancer) EnhanceAll(ctx context.Context, docs []*domain.Document) error {
	for _, doc := range docs {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			if err := e.Enhance(ctx, doc); err != nil {
				return err
			}
		}
	}
	return nil
}
</file>
<file path="internal/llm/openai.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type openAIRequest struct {
	Model       string          `json:"model"`
	Messages    []openAIMessage `json:"messages"`
	MaxTokens   int             `json:"max_tokens,omitempty"`
	Temperature float64         `json:"temperature,omitempty"`
}

type openAIMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type openAIResponse struct {
	ID      string `json:"id"`
	Model   string `json:"model"`
	Choices []struct {
		Index   int `json:"index"`
		Message struct {
			Role    string `json:"role"`
			Content string `json:"content"`
		} `json:"message"`
		FinishReason string `json:"finish_reason"`
	} `json:"choices"`
	Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	} `json:"usage"`
	Error *struct {
		Message string `json:"message"`
		Type    string `json:"type"`
		Code    string `json:"code"`
	} `json:"error,omitempty"`
}

type OpenAIProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewOpenAIProvider(cfg ProviderConfig, httpClient *http.Client) (*OpenAIProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	return &OpenAIProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   cfg.MaxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *OpenAIProvider) Name() string {
	return "openai"
}

func (p *OpenAIProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	messages := make([]openAIMessage, len(req.Messages))
	for i, msg := range req.Messages {
		messages[i] = openAIMessage{
			Role:    string(msg.Role),
			Content: msg.Content,
		}
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	temp := p.temperature
	if req.Temperature != nil {
		temp = *req.Temperature
	}

	openAIReq := openAIRequest{
		Model:       p.model,
		Messages:    messages,
		MaxTokens:   maxTokens,
		Temperature: temp,
	}

	body, err := json.Marshal(openAIReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := p.baseURL + "/chat/completions"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", "Bearer "+p.apiKey)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "openai",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	var openAIResp openAIResponse
	if err := json.Unmarshal(respBody, &openAIResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if openAIResp.Error != nil {
		// Check if this is a rate limit error based on status code
		if resp.StatusCode == http.StatusTooManyRequests {
			return nil, &domain.LLMError{
				Provider:   "openai",
				StatusCode: resp.StatusCode,
				Message:    openAIResp.Error.Message,
				Err:        domain.ErrLLMRateLimited,
			}
		}
		// Check if this is an authentication error
		if resp.StatusCode == http.StatusUnauthorized {
			return nil, &domain.LLMError{
				Provider:   "openai",
				StatusCode: resp.StatusCode,
				Message:    openAIResp.Error.Message,
				Err:        domain.ErrLLMAuthFailed,
			}
		}
		return nil, &domain.LLMError{
			Provider:   "openai",
			StatusCode: resp.StatusCode,
			Message:    openAIResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	if resp.StatusCode != http.StatusOK {
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	if len(openAIResp.Choices) == 0 {
		return nil, &domain.LLMError{
			Provider: "openai",
			Message:  "no choices in response",
		}
	}

	choice := openAIResp.Choices[0]

	return &domain.LLMResponse{
		Content:      choice.Message.Content,
		Model:        openAIResp.Model,
		FinishReason: choice.FinishReason,
		Usage: domain.LLMUsage{
			PromptTokens:     openAIResp.Usage.PromptTokens,
			CompletionTokens: openAIResp.Usage.CompletionTokens,
			TotalTokens:      openAIResp.Usage.TotalTokens,
		},
	}, nil
}

func (p *OpenAIProvider) Close() error {
	return nil
}

func (p *OpenAIProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/provider.go">
package llm

import (
	"fmt"
	"net/http"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Default base URLs for each provider
const (
	DefaultOpenAIBaseURL    = "https://api.openai.com/v1"
	DefaultAnthropicBaseURL = "https://api.anthropic.com/v1"
	DefaultGoogleBaseURL    = "https://generativelanguage.googleapis.com"
)

type ProviderConfig struct {
	Provider    string
	APIKey      string
	BaseURL     string
	Model       string
	MaxTokens   int
	Temperature float64
	Timeout     time.Duration
	MaxRetries  int
	HTTPClient  *http.Client
}

// DefaultBaseURL returns the default base URL for a given provider.
// Returns empty string if provider is unknown.
func DefaultBaseURL(provider string) string {
	switch provider {
	case "openai":
		return DefaultOpenAIBaseURL
	case "anthropic":
		return DefaultAnthropicBaseURL
	case "google":
		return DefaultGoogleBaseURL
	default:
		return ""
	}
}

func NewProviderFromConfig(cfg *config.LLMConfig) (domain.LLMProvider, error) {
	if cfg.Provider == "" {
		return nil, domain.ErrLLMNotConfigured
	}
	if cfg.APIKey == "" {
		return nil, domain.ErrLLMMissingAPIKey
	}
	if cfg.Model == "" {
		return nil, domain.ErrLLMMissingModel
	}

	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = DefaultBaseURL(cfg.Provider)
		if baseURL == "" {
			return nil, domain.ErrLLMMissingBaseURL
		}
	}

	pcfg := ProviderConfig{
		Provider:    cfg.Provider,
		APIKey:      cfg.APIKey,
		BaseURL:     baseURL,
		Model:       cfg.Model,
		MaxTokens:   cfg.MaxTokens,
		Temperature: cfg.Temperature,
		Timeout:     cfg.Timeout,
		MaxRetries:  cfg.MaxRetries,
	}

	return NewProvider(pcfg)
}

func NewProvider(cfg ProviderConfig) (domain.LLMProvider, error) {
	timeout := cfg.Timeout
	if timeout == 0 {
		timeout = 60 * time.Second
	}

	httpClient := cfg.HTTPClient
	if httpClient == nil {
		httpClient = &http.Client{Timeout: timeout}
	}

	switch cfg.Provider {
	case "openai":
		return NewOpenAIProvider(cfg, httpClient)
	case "anthropic":
		return NewAnthropicProvider(cfg, httpClient)
	case "google":
		return NewGoogleProvider(cfg, httpClient)
	default:
		return nil, fmt.Errorf("%w: %s", domain.ErrLLMInvalidProvider, cfg.Provider)
	}
}
</file>
<file path="internal/llm/provider_wrapper.go">
package llm

import (
	"context"
	"fmt"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// RateLimitedProviderConfig holds configuration for the wrapper
type RateLimitedProviderConfig struct {
	RequestsPerMinute        int
	BurstSize                int
	MaxRetries               int
	InitialDelay             time.Duration
	MaxDelay                 time.Duration
	Multiplier               float64
	JitterFactor             float64
	CircuitBreakerEnabled    bool
	FailureThreshold         int
	SuccessThresholdHalfOpen int
	ResetTimeout             time.Duration
}

// DefaultRateLimitedProviderConfig returns sensible defaults
func DefaultRateLimitedProviderConfig() RateLimitedProviderConfig {
	return RateLimitedProviderConfig{
		RequestsPerMinute:        60,
		BurstSize:                10,
		MaxRetries:               3,
		InitialDelay:             time.Second,
		MaxDelay:                 60 * time.Second,
		Multiplier:               2.0,
		JitterFactor:             0.1,
		CircuitBreakerEnabled:    true,
		FailureThreshold:         5,
		SuccessThresholdHalfOpen: 1,
		ResetTimeout:             30 * time.Second,
	}
}

// RateLimitedProvider wraps an LLMProvider with rate limiting, retry, and circuit breaker
type RateLimitedProvider struct {
	provider       domain.LLMProvider
	rateLimiter    RateLimiter
	retrier        *Retrier
	circuitBreaker CircuitBreaker
	logger         *utils.Logger
}

// NewRateLimitedProvider creates a new rate-limited provider wrapper
func NewRateLimitedProvider(
	provider domain.LLMProvider,
	config RateLimitedProviderConfig,
	logger *utils.Logger,
) *RateLimitedProvider {
	var rateLimiter RateLimiter
	if config.RequestsPerMinute > 0 {
		rateLimiter = NewTokenBucket(config.RequestsPerMinute, config.BurstSize)
	} else {
		rateLimiter = &NoOpRateLimiter{}
	}

	retrier := NewRetrier(RetryConfig{
		MaxRetries:      config.MaxRetries,
		InitialInterval: config.InitialDelay,
		MaxInterval:     config.MaxDelay,
		Multiplier:      config.Multiplier,
		JitterFactor:    config.JitterFactor,
	}, logger)

	var circuitBreaker CircuitBreaker
	if config.CircuitBreakerEnabled {
		circuitBreaker = NewCircuitBreaker(CircuitBreakerConfig{
			FailureThreshold:         config.FailureThreshold,
			SuccessThresholdHalfOpen: config.SuccessThresholdHalfOpen,
			ResetTimeout:             config.ResetTimeout,
		})
	} else {
		circuitBreaker = &NoOpCircuitBreaker{}
	}

	return &RateLimitedProvider{
		provider:       provider,
		rateLimiter:    rateLimiter,
		retrier:        retrier,
		circuitBreaker: circuitBreaker,
		logger:         logger,
	}
}

// Name returns the wrapped provider's name
func (p *RateLimitedProvider) Name() string {
	return p.provider.Name()
}

// Complete executes the request with rate limiting, retry, and circuit breaker
func (p *RateLimitedProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	if p.logger != nil {
		p.logger.Debug().
			Float64("tokens_available", p.rateLimiter.Available()).
			Msg("Waiting for rate limit token")
	}

	if err := p.rateLimiter.Wait(ctx); err != nil {
		return nil, fmt.Errorf("rate limit wait cancelled: %w", err)
	}

	if !p.circuitBreaker.Allow() {
		if p.logger != nil {
			p.logger.Warn().
				Str("state", p.circuitBreaker.State().String()).
				Msg("Circuit breaker is open, rejecting request")
		}
		return nil, domain.ErrLLMCircuitOpen
	}

	var response *domain.LLMResponse
	err := p.retrier.Execute(ctx, func() error {
		var err error
		response, err = p.provider.Complete(ctx, req)
		return err
	})

	if err != nil {
		p.circuitBreaker.RecordFailure()
		if p.logger != nil {
			p.logger.Error().
				Err(err).
				Str("circuit_state", p.circuitBreaker.State().String()).
				Msg("LLM request failed")
		}
		return nil, err
	}

	p.circuitBreaker.RecordSuccess()
	return response, nil
}

// Close closes the wrapped provider
func (p *RateLimitedProvider) Close() error {
	return p.provider.Close()
}
</file>
<file path="internal/llm/ratelimit.go">
package llm

import (
	"context"
	"sync"
	"time"
)

// RateLimiter controls the rate of operations
type RateLimiter interface {
	Wait(ctx context.Context) error
	TryAcquire() bool
	Available() float64
}

// TokenBucket implements a token bucket rate limiter
type TokenBucket struct {
	tokens     float64
	capacity   float64
	refillRate float64
	lastRefill time.Time
	mu         sync.Mutex
}

// NewTokenBucket creates a new token bucket rate limiter
func NewTokenBucket(requestsPerMinute int, burstSize int) *TokenBucket {
	if requestsPerMinute <= 0 {
		requestsPerMinute = 60
	}
	if burstSize <= 0 {
		burstSize = 1
	}

	return &TokenBucket{
		tokens:     float64(burstSize),
		capacity:   float64(burstSize),
		refillRate: float64(requestsPerMinute) / 60.0,
		lastRefill: time.Now(),
	}
}

func (tb *TokenBucket) refill() {
	now := time.Now()
	elapsed := now.Sub(tb.lastRefill).Seconds()
	tb.tokens += elapsed * tb.refillRate
	if tb.tokens > tb.capacity {
		tb.tokens = tb.capacity
	}
	tb.lastRefill = now
}

// Wait blocks until a token is available or context is cancelled
func (tb *TokenBucket) Wait(ctx context.Context) error {
	for {
		tb.mu.Lock()
		tb.refill()
		if tb.tokens >= 1.0 {
			tb.tokens--
			tb.mu.Unlock()
			return nil
		}

		tokensNeeded := 1.0 - tb.tokens
		waitDuration := time.Duration(tokensNeeded / tb.refillRate * float64(time.Second))
		tb.mu.Unlock()

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(waitDuration):
			continue
		}
	}
}

// TryAcquire attempts to acquire a token without blocking
func (tb *TokenBucket) TryAcquire() bool {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	if tb.tokens >= 1.0 {
		tb.tokens--
		return true
	}
	return false
}

// Available returns the current number of available tokens
func (tb *TokenBucket) Available() float64 {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	return tb.tokens
}

// NoOpRateLimiter is a rate limiter that doesn't limit
type NoOpRateLimiter struct{}

// Wait always returns immediately
func (n *NoOpRateLimiter) Wait(_ context.Context) error {
	return nil
}

// TryAcquire always returns true
func (n *NoOpRateLimiter) TryAcquire() bool {
	return true
}

// Available always returns 1
func (n *NoOpRateLimiter) Available() float64 {
	return 1.0
}
</file>
<file path="internal/llm/retry.go">
package llm

import (
	"context"
	"errors"
	"fmt"
	"math"
	"math/rand"
	"net/http"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// RetryConfig holds retry configuration
type RetryConfig struct {
	MaxRetries      int
	InitialInterval time.Duration
	MaxInterval     time.Duration
	Multiplier      float64
	JitterFactor    float64
}

// DefaultRetryConfig returns sensible retry defaults
func DefaultRetryConfig() RetryConfig {
	return RetryConfig{
		MaxRetries:      3,
		InitialInterval: 1 * time.Second,
		MaxInterval:     60 * time.Second,
		Multiplier:      2.0,
		JitterFactor:    0.1,
	}
}

// Retrier executes operations with retry logic
type Retrier struct {
	config RetryConfig
	logger *utils.Logger
}

// NewRetrier creates a new Retrier
func NewRetrier(config RetryConfig, logger *utils.Logger) *Retrier {
	if config.MaxRetries < 0 {
		config.MaxRetries = 0
	}
	if config.InitialInterval <= 0 {
		config.InitialInterval = time.Second
	}
	if config.MaxInterval <= 0 {
		config.MaxInterval = 60 * time.Second
	}
	if config.Multiplier <= 0 {
		config.Multiplier = 2.0
	}
	if config.JitterFactor < 0 {
		config.JitterFactor = 0
	}

	return &Retrier{
		config: config,
		logger: logger,
	}
}

// Execute runs the operation with retry logic
func (r *Retrier) Execute(ctx context.Context, operation func() error) error {
	var lastErr error

	for attempt := 0; attempt <= r.config.MaxRetries; attempt++ {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		lastErr = operation()
		if lastErr == nil {
			if attempt > 0 && r.logger != nil {
				r.logger.Info().Int("attempts", attempt+1).Msg("LLM request succeeded after retries")
			}
			return nil
		}

		if !IsRetryableError(lastErr) {
			return lastErr
		}

		if attempt >= r.config.MaxRetries {
			break
		}

		backoff := r.calculateBackoff(attempt)
		if r.logger != nil {
			r.logger.Warn().
				Int("attempt", attempt+1).
				Int("max_retries", r.config.MaxRetries).
				Dur("backoff", backoff).
				Err(lastErr).
				Msg("Retrying LLM request after error")
		}

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(backoff):
			continue
		}
	}

	return fmt.Errorf("%w: %v", domain.ErrLLMMaxRetriesExceeded, lastErr)
}

func (r *Retrier) calculateBackoff(attempt int) time.Duration {
	backoff := float64(r.config.InitialInterval) * math.Pow(r.config.Multiplier, float64(attempt))

	if r.config.JitterFactor > 0 {
		jitter := backoff * r.config.JitterFactor * (rand.Float64()*2 - 1)
		backoff += jitter
	}

	if backoff > float64(r.config.MaxInterval) {
		backoff = float64(r.config.MaxInterval)
	}

	if backoff < 0 {
		backoff = 0
	}

	return time.Duration(backoff)
}

// IsRetryableError checks if an error should be retried
func IsRetryableError(err error) bool {
	if err == nil {
		return false
	}

	if errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {
		return false
	}

	if errors.Is(err, domain.ErrLLMRateLimited) {
		return true
	}

	var llmErr *domain.LLMError
	if errors.As(err, &llmErr) {
		return ShouldRetryStatusCode(llmErr.StatusCode)
	}

	var fetchErr *domain.FetchError
	if errors.As(err, &fetchErr) {
		return ShouldRetryStatusCode(fetchErr.StatusCode)
	}

	return false
}

// ShouldRetryStatusCode checks if an HTTP status code is retryable
func ShouldRetryStatusCode(statusCode int) bool {
	switch statusCode {
	case http.StatusTooManyRequests,
		http.StatusInternalServerError,
		http.StatusBadGateway,
		http.StatusServiceUnavailable,
		http.StatusGatewayTimeout:
		return true
	default:
		return false
	}
}

// ShouldRetry is kept for backward compatibility
func ShouldRetry(statusCode int) bool {
	return ShouldRetryStatusCode(statusCode)
}

// CalculateBackoff is kept for backward compatibility
func CalculateBackoff(attempt int, cfg RetryConfig) time.Duration {
	backoff := float64(cfg.InitialInterval) * math.Pow(cfg.Multiplier, float64(attempt))

	jitter := backoff * 0.1 * (rand.Float64()*2 - 1)
	backoff += jitter

	if backoff > float64(cfg.MaxInterval) {
		backoff = float64(cfg.MaxInterval)
	}

	return time.Duration(backoff)
}
</file>
<file path="internal/manifest/doc.go">
// Package manifest provides types and utilities for loading and validating
// RepoDocs manifest files. A manifest defines multiple documentation sources
// with per-source configurations, enabling batch processing.
//
// # Manifest Format
//
// Manifests can be written in YAML or JSON format:
//
//	sources:
//	  - url: https://docs.example.com
//	    strategy: crawler
//	    content_selector: "article.main"
//	  - url: https://github.com/org/repo
//	    strategy: git
//	    include: ["docs/**/*.md"]
//	options:
//	  continue_on_error: true
//	  output: ./knowledge-base
//
// # Usage
//
// Load a manifest file:
//
//	loader := manifest.NewLoader()
//	cfg, err := loader.Load("sources.yaml")
//	if err != nil {
//	    log.Fatal(err)
//	}
//
//	for _, source := range cfg.Sources {
//	    // Process each source
//	}
//
// # Error Handling
//
// The package defines sentinel errors for common failure cases:
//   - ErrNoSources: manifest has no sources defined
//   - ErrEmptyURL: source is missing required URL field
//   - ErrInvalidFormat: file is not valid YAML/JSON
//   - ErrFileNotFound: manifest file does not exist
//   - ErrUnsupportedExt: unsupported file extension
package manifest
</file>
<file path="internal/manifest/errors.go">
package manifest

import "errors"

// Sentinel errors for the manifest package
var (
	// ErrNoSources indicates the manifest has no sources defined
	ErrNoSources = errors.New("manifest must contain at least one source")

	// ErrEmptyURL indicates a source is missing the required URL field
	ErrEmptyURL = errors.New("source URL cannot be empty")

	// ErrInvalidFormat indicates the manifest file is not valid YAML or JSON
	ErrInvalidFormat = errors.New("manifest must be valid YAML or JSON")

	// ErrFileNotFound indicates the manifest file does not exist
	ErrFileNotFound = errors.New("manifest file not found")

	// ErrUnsupportedExt indicates an unsupported file extension
	ErrUnsupportedExt = errors.New("unsupported file extension (use .yaml, .yml, or .json)")
)
</file>
<file path="internal/manifest/loader.go">
package manifest

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"gopkg.in/yaml.v3"
)

// Loader loads and validates manifest files
type Loader struct{}

// NewLoader creates a new manifest loader
func NewLoader() *Loader {
	return &Loader{}
}

// Load reads and parses a manifest file from the given path
func (l *Loader) Load(path string) (*Config, error) {
	if _, err := os.Stat(path); os.IsNotExist(err) {
		return nil, fmt.Errorf("%w: %s", ErrFileNotFound, path)
	}

	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("failed to read manifest file: %w", err)
	}

	return l.LoadFromBytes(data, filepath.Ext(path))
}

// LoadFromBytes parses manifest configuration from raw bytes
func (l *Loader) LoadFromBytes(data []byte, ext string) (*Config, error) {
	ext = strings.ToLower(ext)

	var cfg Config
	switch ext {
	case ".yaml", ".yml":
		if err := yaml.Unmarshal(data, &cfg); err != nil {
			return nil, fmt.Errorf("%w: %v", ErrInvalidFormat, err)
		}
	case ".json":
		if err := json.Unmarshal(data, &cfg); err != nil {
			return nil, fmt.Errorf("%w: %v", ErrInvalidFormat, err)
		}
	default:
		return nil, fmt.Errorf("%w: %s", ErrUnsupportedExt, ext)
	}

	l.applyDefaults(&cfg)

	if err := cfg.Validate(); err != nil {
		return nil, err
	}

	return &cfg, nil
}

func (l *Loader) applyDefaults(cfg *Config) {
	defaults := DefaultOptions()

	if cfg.Options.Output == "" {
		cfg.Options.Output = defaults.Output
	}
	if cfg.Options.Concurrency == 0 {
		cfg.Options.Concurrency = defaults.Concurrency
	}
	if cfg.Options.CacheTTL == 0 {
		cfg.Options.CacheTTL = defaults.CacheTTL
	}
}
</file>
<file path="internal/manifest/types.go">
package manifest

import (
	"fmt"
	"time"
)

// Config represents the complete manifest configuration
type Config struct {
	Sources []Source `yaml:"sources" json:"sources"`
	Options Options  `yaml:"options" json:"options"`
}

// Source represents an individual documentation source
type Source struct {
	URL             string   `yaml:"url" json:"url"`
	Strategy        string   `yaml:"strategy,omitempty" json:"strategy,omitempty"`
	ContentSelector string   `yaml:"content_selector,omitempty" json:"content_selector,omitempty"`
	ExcludeSelector string   `yaml:"exclude_selector,omitempty" json:"exclude_selector,omitempty"`
	Exclude         []string `yaml:"exclude,omitempty" json:"exclude,omitempty"`
	Include         []string `yaml:"include,omitempty" json:"include,omitempty"`
	MaxDepth        int      `yaml:"max_depth,omitempty" json:"max_depth,omitempty"`
	RenderJS        *bool    `yaml:"render_js,omitempty" json:"render_js,omitempty"`
	Limit           int      `yaml:"limit,omitempty" json:"limit,omitempty"`
}

// Options represents global manifest options
type Options struct {
	ContinueOnError bool          `yaml:"continue_on_error" json:"continue_on_error"`
	Output          string        `yaml:"output,omitempty" json:"output,omitempty"`
	Concurrency     int           `yaml:"concurrency,omitempty" json:"concurrency,omitempty"`
	CacheTTL        time.Duration `yaml:"cache_ttl,omitempty" json:"cache_ttl,omitempty"`
}

// Validate validates the manifest configuration
func (c *Config) Validate() error {
	if len(c.Sources) == 0 {
		return ErrNoSources
	}
	for i, src := range c.Sources {
		if src.URL == "" {
			return fmt.Errorf("source %d: %w", i, ErrEmptyURL)
		}
	}
	return nil
}

// DefaultOptions returns options with sensible defaults
func DefaultOptions() Options {
	return Options{
		ContinueOnError: false,
		Output:          "./docs",
		Concurrency:     5,
		CacheTTL:        24 * time.Hour,
	}
}
</file>
<file path="internal/output/collector.go">
package output

import (
	"encoding/json"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type MetadataCollector struct {
	mu        sync.RWMutex
	documents []*domain.SimpleDocumentMetadata
	sourceURL string
	strategy  string
	baseDir   string
	filename  string
	enabled   bool
}

type CollectorOptions struct {
	BaseDir   string
	Filename  string
	SourceURL string
	Strategy  string
	Enabled   bool
}

func NewMetadataCollector(opts CollectorOptions) *MetadataCollector {
	filename := opts.Filename
	if filename == "" {
		filename = "metadata.json"
	}
	return &MetadataCollector{
		documents: make([]*domain.SimpleDocumentMetadata, 0),
		sourceURL: opts.SourceURL,
		strategy:  opts.Strategy,
		baseDir:   opts.BaseDir,
		filename:  filename,
		enabled:   opts.Enabled,
	}
}

func (c *MetadataCollector) Add(doc *domain.Document, filePath string) {
	if !c.enabled || doc == nil {
		return
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	relPath, err := filepath.Rel(c.baseDir, filePath)
	if err != nil {
		relPath = filePath
	}
	relPath = filepath.ToSlash(relPath)

	metadata := doc.ToSimpleDocumentMetadata(relPath)
	// Use the collector's strategy as the source, overriding the document's SourceStrategy
	metadata.Source = c.strategy
	c.documents = append(c.documents, metadata)
}

func (c *MetadataCollector) Flush() error {
	if !c.enabled || len(c.documents) == 0 {
		return nil
	}

	c.mu.RLock()
	defer c.mu.RUnlock()

	index := c.buildIndex()

	data, err := json.MarshalIndent(index, "", "  ")
	if err != nil {
		return err
	}

	outputPath := filepath.Join(c.baseDir, c.filename)
	return os.WriteFile(outputPath, data, 0644)
}

func (c *MetadataCollector) buildIndex() *domain.SimpleMetadataIndex {
	docs := make([]domain.SimpleDocumentMetadata, len(c.documents))

	for i, doc := range c.documents {
		docs[i] = *doc
	}

	return &domain.SimpleMetadataIndex{
		GeneratedAt:    time.Now(),
		SourceURL:      c.sourceURL,
		Strategy:       c.strategy,
		TotalDocuments: len(c.documents),
		Documents:      docs,
	}
}

func (c *MetadataCollector) Count() int {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return len(c.documents)
}

func (c *MetadataCollector) GetIndex() *domain.SimpleMetadataIndex {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.buildIndex()
}

func (c *MetadataCollector) IsEnabled() bool {
	return c.enabled
}

func (c *MetadataCollector) SetStrategy(name string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.strategy = name
}

func (c *MetadataCollector) SetSourceURL(url string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.sourceURL = url
}
</file>
<file path="internal/output/writer.go">
package output

import (
	"context"
	"os"
	"path/filepath"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type Writer struct {
	baseDir      string
	flat         bool
	jsonMetadata bool
	force        bool
	dryRun       bool
	collector    *MetadataCollector
}

type WriterOptions struct {
	BaseDir      string
	Flat         bool
	JSONMetadata bool
	Force        bool
	DryRun       bool
	Collector    *MetadataCollector
}

func NewWriter(opts WriterOptions) *Writer {
	if opts.BaseDir == "" {
		opts.BaseDir = "./docs"
	}

	return &Writer{
		baseDir:      opts.BaseDir,
		flat:         opts.Flat,
		jsonMetadata: opts.JSONMetadata,
		force:        opts.Force,
		dryRun:       opts.DryRun,
		collector:    opts.Collector,
	}
}

// Write saves a document to the output directory
func (w *Writer) Write(ctx context.Context, doc *domain.Document) error {
	// Generate path
	var path string
	if doc.RelativePath != "" {
		// For Git-sourced files, use the relative path directly
		path = utils.GeneratePathFromRelative(w.baseDir, doc.RelativePath, w.flat)
	} else {
		// For other sources, generate path from URL
		path = utils.GeneratePath(w.baseDir, doc.URL, w.flat)
	}

	// Check if file exists
	if !w.force {
		if _, err := os.Stat(path); err == nil {
			// File exists, skip
			return nil
		}
	}

	// Dry run - just return
	if w.dryRun {
		return nil
	}

	// Ensure directory exists
	if err := utils.EnsureDir(path); err != nil {
		return err
	}

	// Add frontmatter
	content, err := converter.AddFrontmatter(doc.Content, doc)
	if err != nil {
		return err
	}

	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		return err
	}

	if w.jsonMetadata && w.collector != nil {
		w.collector.Add(doc, path)
	}

	return nil
}

func (w *Writer) FlushMetadata() error {
	if w.collector != nil {
		return w.collector.Flush()
	}
	return nil
}

func (w *Writer) WriteMultiple(ctx context.Context, docs []*domain.Document) error {
	for _, doc := range docs {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			if err := w.Write(ctx, doc); err != nil {
				return err
			}
		}
	}
	return nil
}

// GetPath returns the output path for a URL
func (w *Writer) GetPath(url string) string {
	return utils.GeneratePath(w.baseDir, url, w.flat)
}

// Exists checks if a document already exists
func (w *Writer) Exists(url string) bool {
	path := w.GetPath(url)
	_, err := os.Stat(path)
	return err == nil
}

// EnsureBaseDir creates the base directory if it doesn't exist
func (w *Writer) EnsureBaseDir() error {
	return os.MkdirAll(w.baseDir, 0755)
}

// Clean removes the output directory
func (w *Writer) Clean() error {
	return os.RemoveAll(w.baseDir)
}

// Stats returns statistics about the output directory
func (w *Writer) Stats() (int, int64, error) {
	var count int
	var size int64

	err := filepath.Walk(w.baseDir, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}
		if !info.IsDir() && filepath.Ext(path) == ".md" {
			count++
			size += info.Size()
		}
		return nil
	})

	return count, size, err
}
</file>
<file path="internal/renderer/detector.go">
package renderer

import (
	"regexp"
	"strings"
)

// SPA detection patterns
var (
	// React patterns
	reactPatterns = []string{
		`<div id="root"></div>`,
		`<div id="root"/>`,
		`<div id="app"></div>`,
		`<div id="app"/>`,
		`data-reactroot`,
		`__REACT_DEVTOOLS_GLOBAL_HOOK__`,
	}

	// Vue patterns
	vuePatterns = []string{
		`<div id="app"></div>`,
		`<div id="app"/>`,
		`__VUE__`,
		`v-cloak`,
		`Vue.createApp`,
	}

	// Next.js patterns
	nextPatterns = []string{
		`<div id="__next"></div>`,
		`<div id="__next"/>`,
		`__NEXT_DATA__`,
		`_next/static`,
	}

	// Nuxt patterns
	nuxtPatterns = []string{
		`__NUXT__`,
		`window.__NUXT__`,
		`<div id="__nuxt">`,
	}

	// Angular patterns
	angularPatterns = []string{
		`ng-version`,
		`ng-app`,
		`ng-controller`,
		`<app-root>`,
	}

	// Svelte patterns
	sveltePatterns = []string{
		`__svelte`,
		`svelte-`,
	}

	// Generic SPA indicators
	spaIndicators = []string{
		`window.__INITIAL_STATE__`,
		`window.__STATE__`,
		`window.__PRELOADED_STATE__`,
	}
)

// contentMinLength is the minimum content length to consider a page as rendered
const contentMinLength = 500

// scriptTagRegex matches script tags
var scriptTagRegex = regexp.MustCompile(`<script[^>]*>[\s\S]*?</script>`)

// htmlTagRegex matches HTML tags
var htmlTagRegex = regexp.MustCompile(`<[^>]+>`)

// NeedsJSRendering detects if a page needs JavaScript rendering
func NeedsJSRendering(html string) bool {
	// Check for SPA framework patterns
	if hasSPAPattern(html) {
		return true
	}

	// Check content length without scripts
	contentWithoutScripts := scriptTagRegex.ReplaceAllString(html, "")
	textContent := htmlTagRegex.ReplaceAllString(contentWithoutScripts, "")
	textContent = strings.TrimSpace(textContent)

	// If there's very little content but many scripts, likely a SPA
	if len(textContent) < contentMinLength {
		scriptCount := strings.Count(strings.ToLower(html), "<script")
		if scriptCount > 3 {
			return true
		}
	}

	return false
}

// hasSPAPattern checks if the HTML contains any SPA framework patterns
func hasSPAPattern(html string) bool {
	htmlLower := strings.ToLower(html)

	allPatterns := append([]string{}, reactPatterns...)
	allPatterns = append(allPatterns, vuePatterns...)
	allPatterns = append(allPatterns, nextPatterns...)
	allPatterns = append(allPatterns, nuxtPatterns...)
	allPatterns = append(allPatterns, angularPatterns...)
	allPatterns = append(allPatterns, sveltePatterns...)
	allPatterns = append(allPatterns, spaIndicators...)

	for _, pattern := range allPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return true
		}
	}

	return false
}

// DetectFramework attempts to detect which SPA framework is being used
func DetectFramework(html string) string {
	htmlLower := strings.ToLower(html)

	for _, pattern := range nextPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Next.js"
		}
	}

	for _, pattern := range nuxtPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Nuxt"
		}
	}

	for _, pattern := range reactPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "React"
		}
	}

	for _, pattern := range vuePatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Vue"
		}
	}

	for _, pattern := range angularPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Angular"
		}
	}

	for _, pattern := range sveltePatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Svelte"
		}
	}

	return "Unknown"
}

// HasDynamicContent checks for indicators of dynamic content loading
func HasDynamicContent(html string) bool {
	indicators := []string{
		"loading...",
		"loading…",
		"please wait",
		"spinner",
		"skeleton",
		"lazy-load",
		"lazyload",
		"infinite-scroll",
	}

	htmlLower := strings.ToLower(html)
	for _, indicator := range indicators {
		if strings.Contains(htmlLower, indicator) {
			return true
		}
	}

	return false
}
</file>
<file path="internal/renderer/pool.go">
package renderer

import (
	"context"
	"sync"

	"github.com/go-rod/rod"
)

// TabPool manages a pool of browser tabs for concurrent rendering
type TabPool struct {
	browser    *rod.Browser
	maxTabs    int
	activeTabs chan *rod.Page
	mu         sync.Mutex
	closed     bool
}

// NewTabPool creates a new tab pool
func NewTabPool(browser *rod.Browser, maxTabs int) (*TabPool, error) {
	if maxTabs <= 0 {
		maxTabs = 5
	}

	pool := &TabPool{
		browser:    browser,
		maxTabs:    maxTabs,
		activeTabs: make(chan *rod.Page, maxTabs),
	}

	// Pre-create tabs
	for i := 0; i < maxTabs; i++ {
		page, err := StealthPage(browser)
		if err != nil {
			pool.Close()
			return nil, err
		}
		pool.activeTabs <- page
	}

	return pool, nil
}

// Acquire gets a page from the pool, blocking if none available
func (p *TabPool) Acquire(ctx context.Context) (*rod.Page, error) {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		return nil, ErrPoolClosed
	}
	p.mu.Unlock()

	select {
	case page := <-p.activeTabs:
		return page, nil
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}

// Release returns a page to the pool after cleaning up
func (p *TabPool) Release(page *rod.Page) {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		page.Close()
		return
	}
	p.mu.Unlock()

	// Clean up the page before returning to pool
	_ = page.Navigate("about:blank")

	select {
	case p.activeTabs <- page:
		// Successfully returned to pool
	default:
		// Pool is full (shouldn't happen normally)
		page.Close()
	}
}

// Close closes all tabs and the pool
func (p *TabPool) Close() error {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		return nil
	}
	p.closed = true
	p.mu.Unlock()

	close(p.activeTabs)

	// Close remaining pages
	for page := range p.activeTabs {
		page.Close()
	}

	return nil
}

// Size returns the current number of available tabs
func (p *TabPool) Size() int {
	return len(p.activeTabs)
}

// MaxSize returns the maximum pool size
func (p *TabPool) MaxSize() int {
	return p.maxTabs
}

// ErrPoolClosed is returned when trying to acquire from a closed pool
var ErrPoolClosed = &poolError{message: "pool is closed"}

type poolError struct {
	message string
}

func (e *poolError) Error() string {
	return e.message
}
</file>
<file path="internal/renderer/rod.go">
package renderer

import (
	"context"
	"fmt"
	"net/http"
	"net/url"
	"os"
	"time"

	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/launcher"
	"github.com/go-rod/rod/lib/proto"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Renderer provides JavaScript rendering using headless Chrome
type Renderer struct {
	browser  *rod.Browser
	pool     *TabPool
	timeout  time.Duration
	stealth  bool
	headless bool
}

// RendererOptions contains options for creating a Renderer
type RendererOptions struct {
	Timeout     time.Duration
	MaxTabs     int
	Stealth     bool
	Headless    bool
	BrowserPath string
	NoSandbox   bool // Required for running in CI/Docker environments
}

// DefaultRendererOptions returns default renderer options
func DefaultRendererOptions() RendererOptions {
	return RendererOptions{
		Timeout:     60 * time.Second,
		MaxTabs:     5,
		Stealth:     true,
		Headless:    true,
		BrowserPath: "",
		NoSandbox:   isCI(), // Auto-detect CI environment
	}
}

// isCI returns true if running in a CI environment
func isCI() bool {
	return os.Getenv("CI") != "" || os.Getenv("GITHUB_ACTIONS") != ""
}

// NewRenderer creates a new headless browser renderer
func NewRenderer(opts RendererOptions) (*Renderer, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = 60 * time.Second
	}
	if opts.MaxTabs <= 0 {
		opts.MaxTabs = 5
	}

	// Create launcher
	l := launcher.New()

	if opts.BrowserPath != "" {
		l = l.Bin(opts.BrowserPath)
	}

	if opts.Headless {
		l = l.Headless(true)
	}

	// Additional flags for stealth
	if opts.Stealth {
		l = l.Set("disable-blink-features", "AutomationControlled")
	}

	// NoSandbox is required for running in CI/Docker environments
	if opts.NoSandbox {
		l = l.NoSandbox(true)
	}

	// Launch browser
	controlURL, err := l.Launch()
	if err != nil {
		return nil, fmt.Errorf("failed to launch browser: %w", err)
	}

	browser := rod.New().ControlURL(controlURL)
	if err := browser.Connect(); err != nil {
		return nil, fmt.Errorf("failed to connect to browser: %w", err)
	}

	// Create tab pool
	pool, err := NewTabPool(browser, opts.MaxTabs)
	if err != nil {
		browser.Close()
		return nil, fmt.Errorf("failed to create tab pool: %w", err)
	}

	return &Renderer{
		browser:  browser,
		pool:     pool,
		timeout:  opts.Timeout,
		stealth:  opts.Stealth,
		headless: opts.Headless,
	}, nil
}

// Render fetches and renders a page with JavaScript
func (r *Renderer) Render(ctx context.Context, url string, opts domain.RenderOptions) (string, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = r.timeout
	}

	// Create context with timeout
	ctx, cancel := context.WithTimeout(ctx, opts.Timeout)
	defer cancel()

	// Acquire a page from the pool
	page, err := r.pool.Acquire(ctx)
	if err != nil {
		return "", fmt.Errorf("failed to acquire page: %w", err)
	}
	defer r.pool.Release(page)

	// Apply context to page so all operations respect the timeout
	page = page.Context(ctx)

	// Apply stealth mode
	if r.stealth {
		if err := ApplyStealthMode(page); err != nil {
			return "", fmt.Errorf("failed to apply stealth mode: %w", err)
		}
	}

	// Set cookies if provided
	if len(opts.Cookies) > 0 {
		if err := r.setCookies(page, url, opts.Cookies); err != nil {
			return "", fmt.Errorf("failed to set cookies: %w", err)
		}
	}

	// Navigate to URL
	if err := page.Navigate(url); err != nil {
		return "", domain.NewFetchError(url, 0, fmt.Errorf("navigation failed: %w", err))
	}

	// Wait for page to load
	if err := page.WaitLoad(); err != nil {
		return "", fmt.Errorf("failed waiting for load: %w", err)
	}

	// Wait for specific selector if provided
	if opts.WaitFor != "" {
		if err := page.Timeout(opts.Timeout).MustElement(opts.WaitFor).WaitVisible(); err != nil {
			// Don't fail, just continue
		}
	}

	// Wait for network to be idle
	if opts.WaitStable > 0 {
		if err := page.WaitRequestIdle(opts.WaitStable, nil, nil, nil); err != nil {
			// Don't fail, just continue
		}
	}

	// Scroll to bottom to load lazy content
	if opts.ScrollToEnd {
		if err := r.scrollToEnd(page); err != nil {
			// Don't fail, just continue
		}
	}

	// Get rendered HTML
	html, err := page.HTML()
	if err != nil {
		return "", fmt.Errorf("failed to get HTML: %w", err)
	}

	return html, nil
}

// setCookies sets cookies on a page
func (r *Renderer) setCookies(page *rod.Page, pageURL string, cookies []*http.Cookie) error {
	// Parse URL to extract domain if cookie domain is empty
	parsedURL, err := url.Parse(pageURL)
	if err != nil {
		return fmt.Errorf("failed to parse URL for cookies: %w", err)
	}

	for _, cookie := range cookies {
		// Use cookie domain if set, otherwise extract from URL
		domain := cookie.Domain
		if domain == "" {
			domain = parsedURL.Hostname()
		}

		// Use cookie path if set, otherwise default to "/"
		path := cookie.Path
		if path == "" {
			path = "/"
		}

		err := page.SetCookies([]*proto.NetworkCookieParam{
			{
				Name:     cookie.Name,
				Value:    cookie.Value,
				Domain:   domain,
				Path:     path,
				Secure:   cookie.Secure,
				HTTPOnly: cookie.HttpOnly,
			},
		})
		if err != nil {
			return err
		}
	}
	return nil
}

// scrollToEnd scrolls to the bottom of the page to trigger lazy loading
func (r *Renderer) scrollToEnd(page *rod.Page) error {
	// Get initial scroll height
	result, err := page.Eval(`() => document.body.scrollHeight`)
	if err != nil {
		return err
	}
	lastHeight := result.Value.Int()

	for i := 0; i < 10; i++ { // Max 10 scroll iterations
		// Scroll to bottom
		_, err := page.Eval(`() => window.scrollTo(0, document.body.scrollHeight)`)
		if err != nil {
			return err
		}

		// Wait for content to load
		time.Sleep(500 * time.Millisecond)

		// Check new scroll height
		result, err := page.Eval(`() => document.body.scrollHeight`)
		if err != nil {
			return err
		}
		newHeight := result.Value.Int()

		// If height hasn't changed, we've reached the bottom
		if newHeight == lastHeight {
			break
		}
		lastHeight = newHeight
	}

	// Scroll back to top
	_, _ = page.Eval(`() => window.scrollTo(0, 0)`)

	return nil
}

// DefaultRenderOptions returns default render options
func DefaultRenderOptions() domain.RenderOptions {
	return domain.RenderOptions{
		Timeout:     60 * time.Second,
		WaitStable:  2 * time.Second,
		ScrollToEnd: true,
	}
}

// Close releases browser resources
func (r *Renderer) Close() error {
	if r.pool != nil {
		r.pool.Close()
		r.pool = nil
	}
	if r.browser != nil {
		browser := r.browser
		r.browser = nil
		return browser.Close()
	}
	return nil
}

// IsAvailable checks if the browser is available
func IsAvailable() bool {
	path, exists := launcher.LookPath()
	return exists && path != ""
}

// GetBrowserPath returns the detected browser path
func GetBrowserPath() (string, bool) {
	return launcher.LookPath()
}

// GetTabPool returns the tab pool for testing purposes
func (r *Renderer) GetTabPool() (*TabPool, error) {
	if r.pool == nil {
		return nil, fmt.Errorf("pool not initialized")
	}
	return r.pool, nil
}
</file>
<file path="internal/renderer/stealth.go">
package renderer

import (
	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/proto"
	"github.com/go-rod/stealth"
)

// StealthPage creates a new stealth page that's harder to detect as automated
func StealthPage(browser *rod.Browser) (*rod.Page, error) {
	page, err := stealth.Page(browser)
	if err != nil {
		return nil, err
	}
	return page, nil
}

// ApplyStealthMode applies stealth mode configurations to a page
// This includes removing webdriver flags and emulating real browser behavior
func ApplyStealthMode(page *rod.Page) error {
	// The stealth package already handles most of this, but we can add extra measures

	// Set a realistic viewport using proto.EmulationSetDeviceMetricsOverride
	err := page.SetViewport(&proto.EmulationSetDeviceMetricsOverride{
		Width:  1920,
		Height: 1080,
	})
	if err != nil {
		return err
	}

	// Simple stealth: just hide webdriver flag
	// Rod expects arrow function format: () => expression
	js := `() => { Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); return true; }`
	_, err = page.Eval(js)
	return err
}

// StealthOptions contains options for stealth mode
type StealthOptions struct {
	// HideWebdriver hides the webdriver property
	HideWebdriver bool
	// EmulatePlugins emulates real browser plugins
	EmulatePlugins bool
	// RandomizeViewport randomizes the viewport size
	RandomizeViewport bool
	// DisableAutomationFlags disables Chrome automation flags
	DisableAutomationFlags bool
}

// DefaultStealthOptions returns default stealth options
func DefaultStealthOptions() StealthOptions {
	return StealthOptions{
		HideWebdriver:          true,
		EmulatePlugins:         true,
		RandomizeViewport:      false,
		DisableAutomationFlags: true,
	}
}
</file>
<file path="internal/state/errors.go">
package state

import "errors"

var (
	// ErrStateNotFound indicates the state file does not exist
	ErrStateNotFound = errors.New("state file not found")

	// ErrStateCorrupted indicates the state file contains invalid JSON
	ErrStateCorrupted = errors.New("state file is corrupted")

	// ErrVersionMismatch indicates an incompatible state schema version
	ErrVersionMismatch = errors.New("state version mismatch")
)
</file>
<file path="internal/state/manager.go">
package state

import (
	"context"
	"encoding/json"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

const StateFileName = ".repodocs-state.json"

type Manager struct {
	baseDir  string
	state    *SyncState
	mu       sync.RWMutex
	dirty    bool
	logger   *utils.Logger
	disabled bool
	seenURLs sync.Map
}

type ManagerOptions struct {
	BaseDir   string
	SourceURL string
	Strategy  string
	Logger    *utils.Logger
	Disabled  bool
}

func NewManager(opts ManagerOptions) *Manager {
	return &Manager{
		baseDir:  opts.BaseDir,
		logger:   opts.Logger,
		disabled: opts.Disabled,
		state:    NewSyncState(opts.SourceURL, opts.Strategy),
	}
}

func (m *Manager) Load(ctx context.Context) error {
	if m.disabled {
		return nil
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	path := m.statePath()
	data, err := os.ReadFile(path)
	if os.IsNotExist(err) {
		return ErrStateNotFound
	}
	if err != nil {
		return err
	}

	var state SyncState
	if err := json.Unmarshal(data, &state); err != nil {
		return ErrStateCorrupted
	}

	if state.Version != StateVersion {
		if m.logger != nil {
			m.logger.Warn().
				Int("file_version", state.Version).
				Int("expected_version", StateVersion).
				Msg("State version mismatch, will rebuild state")
		}
		return ErrVersionMismatch
	}

	m.state = &state
	return nil
}

func (m *Manager) Save(ctx context.Context) error {
	if m.disabled {
		return nil
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	if !m.dirty {
		return nil
	}

	m.state.LastSync = time.Now()

	data, err := json.MarshalIndent(m.state, "", "  ")
	if err != nil {
		return err
	}

	path := m.statePath()
	if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
		return err
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return err
	}

	m.dirty = false
	if m.logger != nil {
		m.logger.Debug().
			Int("pages", len(m.state.Pages)).
			Str("path", path).
			Msg("State saved")
	}
	return nil
}

func (m *Manager) ShouldProcess(url, contentHash string) bool {
	if m.disabled {
		return true
	}

	m.mu.RLock()
	defer m.mu.RUnlock()

	page, exists := m.state.Pages[url]
	if !exists {
		return true
	}

	return page.ContentHash != contentHash
}

func (m *Manager) Update(url string, page PageState) {
	if m.disabled {
		return
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	m.state.Pages[url] = page
	m.dirty = true
}

func (m *Manager) MarkSeen(url string) {
	m.seenURLs.Store(url, true)
}

func (m *Manager) GetDeletedPages() []PageState {
	if m.disabled {
		return nil
	}

	m.mu.RLock()
	defer m.mu.RUnlock()

	var deleted []PageState
	for url, page := range m.state.Pages {
		if _, seen := m.seenURLs.Load(url); !seen {
			deleted = append(deleted, page)
		}
	}
	return deleted
}

func (m *Manager) RemoveDeletedFromState() {
	if m.disabled {
		return
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	for url := range m.state.Pages {
		if _, seen := m.seenURLs.Load(url); !seen {
			delete(m.state.Pages, url)
			m.dirty = true
		}
	}
}

func (m *Manager) Stats() (total, cached int) {
	m.mu.RLock()
	defer m.mu.RUnlock()

	total = len(m.state.Pages)

	var seenCount int
	m.seenURLs.Range(func(_, _ any) bool {
		seenCount++
		return true
	})

	return total, total - seenCount
}

func (m *Manager) IsDisabled() bool {
	return m.disabled
}

func (m *Manager) statePath() string {
	return filepath.Join(m.baseDir, StateFileName)
}
</file>
<file path="internal/state/models.go">
package state

import "time"

// StateVersion is the schema version for state file migration
const StateVersion = 1

// SyncState represents the complete synchronization state for a source
type SyncState struct {
	Version   int                  `json:"version"`
	SourceURL string               `json:"source_url"`
	Strategy  string               `json:"strategy,omitempty"`
	LastSync  time.Time            `json:"last_sync"`
	Pages     map[string]PageState `json:"pages"`
}

// PageState represents the state of an individual processed page
type PageState struct {
	ContentHash string    `json:"content_hash"`
	FetchedAt   time.Time `json:"fetched_at"`
	FilePath    string    `json:"file_path"`
}

// NewSyncState creates a new empty sync state
func NewSyncState(sourceURL, strategy string) *SyncState {
	return &SyncState{
		Version:   StateVersion,
		SourceURL: sourceURL,
		Strategy:  strategy,
		LastSync:  time.Now(),
		Pages:     make(map[string]PageState),
	}
}

// PageCount returns the number of pages in the state
func (s *SyncState) PageCount() int {
	return len(s.Pages)
}

// HasPage checks if a page exists in the state
func (s *SyncState) HasPage(url string) bool {
	_, exists := s.Pages[url]
	return exists
}

// GetPage returns a page state by URL
func (s *SyncState) GetPage(url string) (PageState, bool) {
	page, exists := s.Pages[url]
	return page, exists
}

// SetPage updates or adds a page to the state
func (s *SyncState) SetPage(url string, page PageState) {
	s.Pages[url] = page
}

// RemovePage removes a page from the state
func (s *SyncState) RemovePage(url string) {
	delete(s.Pages, url)
}
</file>
<file path="internal/strategies/git/archive.go">
package git

import (
	"archive/tar"
	"compress/gzip"
	"context"
	"fmt"
	"io"
	"net/http"
	"os"
	"path/filepath"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type ArchiveFetcher struct {
	httpClient *http.Client
	logger     *utils.Logger
}

type ArchiveFetcherOptions struct {
	HTTPClient *http.Client
	Logger     *utils.Logger
}

func NewArchiveFetcher(opts ArchiveFetcherOptions) *ArchiveFetcher {
	return &ArchiveFetcher{
		httpClient: opts.HTTPClient,
		logger:     opts.Logger,
	}
}

func (f *ArchiveFetcher) Name() string {
	return "archive"
}

func (f *ArchiveFetcher) Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error) {
	archiveURL := f.BuildArchiveURL(info, branch)
	if f.logger != nil {
		f.logger.Debug().Str("archive_url", archiveURL).Msg("Downloading archive")
	}

	if err := f.DownloadAndExtract(ctx, archiveURL, destDir); err != nil {
		return nil, err
	}

	return &FetchResult{
		LocalPath: destDir,
		Branch:    branch,
		Method:    "archive",
	}, nil
}

func (f *ArchiveFetcher) BuildArchiveURL(info *RepoInfo, branch string) string {
	switch info.Platform {
	case PlatformGitHub:
		return fmt.Sprintf("https://github.com/%s/%s/archive/refs/heads/%s.tar.gz",
			info.Owner, info.Repo, branch)
	case PlatformGitLab:
		return fmt.Sprintf("https://gitlab.com/%s/%s/-/archive/%s/%s-%s.tar.gz",
			info.Owner, info.Repo, branch, info.Repo, branch)
	case PlatformBitbucket:
		return fmt.Sprintf("https://bitbucket.org/%s/%s/get/%s.tar.gz",
			info.Owner, info.Repo, branch)
	default:
		return fmt.Sprintf("https://github.com/%s/%s/archive/refs/heads/%s.tar.gz",
			info.Owner, info.Repo, branch)
	}
}

func (f *ArchiveFetcher) DownloadAndExtract(ctx context.Context, archiveURL, destDir string) error {
	req, err := http.NewRequestWithContext(ctx, "GET", archiveURL, nil)
	if err != nil {
		return err
	}

	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		req.Header.Set("Authorization", "token "+token)
	}

	resp, err := f.httpClient.Do(req)
	if err != nil {
		return fmt.Errorf("download request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode == http.StatusNotFound {
		return fmt.Errorf("archive not found (404)")
	}
	if resp.StatusCode == http.StatusUnauthorized {
		return fmt.Errorf("authentication required (401)")
	}
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("download failed with status: %d", resp.StatusCode)
	}

	return f.ExtractTarGz(resp.Body, destDir)
}

func (f *ArchiveFetcher) ExtractTarGz(r io.Reader, destDir string) error {
	gzr, err := gzip.NewReader(r)
	if err != nil {
		return fmt.Errorf("gzip reader failed: %w", err)
	}
	defer gzr.Close()

	tr := tar.NewReader(gzr)

	for {
		header, err := tr.Next()
		if err == io.EOF {
			break
		}
		if err != nil {
			return fmt.Errorf("tar read failed: %w", err)
		}

		parts := strings.SplitN(header.Name, "/", 2)
		if len(parts) < 2 || parts[1] == "" {
			continue
		}
		relativePath := parts[1]

		targetPath := filepath.Join(destDir, relativePath)

		if !strings.HasPrefix(filepath.Clean(targetPath), filepath.Clean(destDir)) {
			continue
		}

		switch header.Typeflag {
		case tar.TypeDir:
			if err := os.MkdirAll(targetPath, 0755); err != nil {
				return fmt.Errorf("mkdir failed: %w", err)
			}
		case tar.TypeReg:
			if err := os.MkdirAll(filepath.Dir(targetPath), 0755); err != nil {
				return fmt.Errorf("mkdir failed: %w", err)
			}

			file, err := os.OpenFile(targetPath, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.FileMode(header.Mode))
			if err != nil {
				return fmt.Errorf("create file failed: %w", err)
			}

			if _, err := io.Copy(file, tr); err != nil {
				file.Close()
				return fmt.Errorf("copy failed: %w", err)
			}
			file.Close()
		}
	}

	return nil
}
</file>
<file path="internal/strategies/git/clone.go">
package git

import (
	"bufio"
	"context"
	"fmt"
	"os"
	"os/exec"
	"strings"

	"github.com/go-git/go-git/v5"
	githttp "github.com/go-git/go-git/v5/plumbing/transport/http"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type CloneFetcher struct {
	logger *utils.Logger
}

type CloneFetcherOptions struct {
	Logger *utils.Logger
}

func NewCloneFetcher(opts CloneFetcherOptions) *CloneFetcher {
	return &CloneFetcher{logger: opts.Logger}
}

func (f *CloneFetcher) Name() string {
	return "clone"
}

func (f *CloneFetcher) Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error) {
	if f.logger != nil {
		f.logger.Info().Str("url", info.URL).Msg("Cloning repository")
	}

	cloneOpts := &git.CloneOptions{
		URL:      info.URL,
		Depth:    1,
		Progress: os.Stdout,
	}

	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		cloneOpts.Auth = &githttp.BasicAuth{
			Username: "token",
			Password: token,
		}
	}

	repo, err := git.PlainCloneContext(ctx, destDir, false, cloneOpts)
	if err != nil {
		return nil, err
	}

	detectedBranch := branch
	head, err := repo.Head()
	if err == nil {
		refName := head.Name().String()
		if strings.HasPrefix(refName, "refs/heads/") {
			detectedBranch = strings.TrimPrefix(refName, "refs/heads/")
		}
	}

	if detectedBranch == "" {
		detectedBranch = "main"
	}

	return &FetchResult{
		LocalPath: destDir,
		Branch:    detectedBranch,
		Method:    "clone",
	}, nil
}

func DetectDefaultBranch(ctx context.Context, url string) (string, error) {
	cmd := exec.CommandContext(ctx, "git", "ls-remote", "--symref", url, "HEAD")
	output, err := cmd.Output()
	if err != nil {
		return "", fmt.Errorf("git ls-remote failed: %w", err)
	}

	scanner := bufio.NewScanner(strings.NewReader(string(output)))
	for scanner.Scan() {
		line := scanner.Text()
		if strings.HasPrefix(line, "ref: refs/heads/") {
			parts := strings.Split(line, "\t")
			if len(parts) >= 1 {
				branch := strings.TrimPrefix(parts[0], "ref: refs/heads/")
				return branch, nil
			}
		}
	}

	return "", fmt.Errorf("could not determine default branch")
}
</file>
<file path="internal/strategies/git/doc.go">
// Package git implements the git repository extraction strategy.
//
// It supports extracting documentation from GitHub, GitLab, and Bitbucket
// repositories using either archive download (faster) or git clone (fallback).
//
// Architecture:
//   - Strategy: Coordinator implementing strategies.Strategy interface
//   - Parser: URL parsing and platform detection
//   - ArchiveFetcher: HTTP-based tar.gz download and extraction
//   - CloneFetcher: go-git based repository cloning
//   - Processor: File discovery and document conversion
//
// Usage:
//
//	strategy := git.NewStrategy(deps)
//	if strategy.CanHandle(url) {
//	    err := strategy.Execute(ctx, url, opts)
//	}
package git
</file>
<file path="internal/strategies/git/fetcher.go">
package git

import "context"

type RepoFetcher interface {
	Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error)
	Name() string
}
</file>
<file path="internal/strategies/git/parser.go">
package git

import (
	"fmt"
	"net/url"
	"path/filepath"
	"regexp"
	"strings"
)

type platformPattern struct {
	platform    Platform
	repoPattern *regexp.Regexp
	treePattern *regexp.Regexp
}

type Parser struct {
	patterns []platformPattern
}

func NewParser() *Parser {
	return &Parser{
		patterns: []platformPattern{
			{
				platform:    PlatformGitHub,
				repoPattern: regexp.MustCompile(`^(https?://github\.com/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/tree/([^/]+)(?:/(.+))?$`),
			},
			{
				platform:    PlatformGitLab,
				repoPattern: regexp.MustCompile(`^(https?://gitlab\.com/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/-/tree/([^/]+)(?:/(.+))?$`),
			},
			{
				platform:    PlatformBitbucket,
				repoPattern: regexp.MustCompile(`^(https?://bitbucket\.org/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/src/([^/]+)(?:/(.+))?$`),
			},
		},
	}
}

func (p *Parser) ParseURL(rawURL string) (*RepoInfo, error) {
	patterns := []struct {
		platform Platform
		regex    *regexp.Regexp
	}{
		{PlatformGitHub, regexp.MustCompile(`github\.com[:/]([^/]+)/([^/.]+)`)},
		{PlatformGitLab, regexp.MustCompile(`gitlab\.com[:/]([^/]+)/([^/.]+)`)},
		{PlatformBitbucket, regexp.MustCompile(`bitbucket\.org[:/]([^/]+)/([^/.]+)`)},
	}

	for _, pat := range patterns {
		if matches := pat.regex.FindStringSubmatch(rawURL); len(matches) == 3 {
			return &RepoInfo{
				Platform: pat.platform,
				Owner:    matches[1],
				Repo:     strings.TrimSuffix(matches[2], ".git"),
				URL:      rawURL,
			}, nil
		}
	}

	return nil, fmt.Errorf("unsupported git URL format: %s", rawURL)
}

func (p *Parser) ParseURLWithPath(rawURL string) (*GitURLInfo, error) {
	info := &GitURLInfo{}
	lower := strings.ToLower(rawURL)

	for _, pat := range p.patterns {
		if !strings.Contains(lower, string(pat.platform)) {
			continue
		}

		repoMatches := pat.repoPattern.FindStringSubmatch(rawURL)
		if len(repoMatches) < 4 {
			continue
		}

		info.Platform = pat.platform
		info.RepoURL = repoMatches[1]
		info.Owner = repoMatches[2]
		info.Repo = strings.TrimSuffix(repoMatches[3], ".git")

		treeMatches := pat.treePattern.FindStringSubmatch(rawURL)
		if len(treeMatches) >= 2 {
			info.Branch = treeMatches[1]
			if len(treeMatches) >= 3 && treeMatches[2] != "" {
				info.SubPath = NormalizeFilterPath(treeMatches[2])
			}
		}

		return info, nil
	}

	if strings.HasPrefix(rawURL, "http://") || strings.HasPrefix(rawURL, "https://") {
		info.Platform = PlatformGeneric
		info.RepoURL = rawURL
		return info, nil
	}

	return nil, fmt.Errorf("unsupported git URL format: %s", rawURL)
}

func NormalizeFilterPath(path string) string {
	if path == "" {
		return ""
	}

	if strings.HasPrefix(path, "http://") || strings.HasPrefix(path, "https://") {
		path = ExtractPathFromTreeURL(path)
	}

	decoded, err := url.PathUnescape(path)
	if err == nil {
		path = decoded
	}

	path = strings.ReplaceAll(path, "\\", "/")
	path = strings.Trim(path, "/")
	path = filepath.Clean(path)

	return path
}

func ExtractPathFromTreeURL(rawURL string) string {
	patterns := []*regexp.Regexp{
		regexp.MustCompile(`github\.com/[^/]+/[^/]+/(?:tree|blob)/[^/]+/(.+)$`),
		regexp.MustCompile(`gitlab\.com/[^/]+/[^/]+/-/(?:tree|blob)/[^/]+/(.+)$`),
		regexp.MustCompile(`bitbucket\.org/[^/]+/[^/]+/src/[^/]+/(.+)$`),
	}

	for _, pat := range patterns {
		if matches := pat.FindStringSubmatch(rawURL); len(matches) >= 2 {
			return matches[1]
		}
	}

	return rawURL
}
</file>
<file path="internal/strategies/git/processor.go">
package git

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type Processor struct {
	logger *utils.Logger
}

type ProcessorOptions struct {
	Logger *utils.Logger
}

func NewProcessor(opts ProcessorOptions) *Processor {
	return &Processor{logger: opts.Logger}
}

type ProcessOptions struct {
	RepoURL      string
	Branch       string
	FilterPath   string
	Concurrency  int
	Limit        int
	DryRun       bool
	WriteFunc    func(ctx context.Context, doc *domain.Document) error
	StateManager *state.Manager
}

func (p *Processor) FindDocumentationFiles(dir string, filterPath string) ([]string, error) {
	var files []string

	walkDir := dir
	if filterPath != "" {
		walkDir = filepath.Join(dir, filterPath)

		info, err := os.Stat(walkDir)
		if err != nil {
			if os.IsNotExist(err) {
				return nil, fmt.Errorf("filter path does not exist in repository: %s", filterPath)
			}
			return nil, fmt.Errorf("failed to access filter path: %w", err)
		}
		if !info.IsDir() {
			return nil, fmt.Errorf("filter path is not a directory: %s", filterPath)
		}

		if p.logger != nil {
			p.logger.Debug().Str("filter_path", filterPath).Str("walk_dir", walkDir).Msg("Walking filtered directory")
		}
	}

	err := filepath.WalkDir(walkDir, func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			return err
		}

		if d.IsDir() {
			if IgnoreDirs[d.Name()] {
				return fs.SkipDir
			}
			return nil
		}

		ext := strings.ToLower(filepath.Ext(path))
		if DocumentExtensions[ext] {
			files = append(files, path)
		}

		return nil
	})

	return files, err
}

func (p *Processor) ProcessFiles(ctx context.Context, files []string, tmpDir string, opts ProcessOptions) error {
	bar := utils.NewProgressBar(len(files), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, files, opts.Concurrency, func(ctx context.Context, file string) error {
		defer bar.Add(1)

		if err := p.ProcessFile(ctx, file, tmpDir, opts); err != nil {
			if p.logger != nil {
				p.logger.Warn().Err(err).Str("file", file).Msg("Failed to process file")
			}
		}
		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	if p.logger != nil {
		p.logger.Info().Msg("Git extraction completed")
	}
	return nil
}

func (p *Processor) ProcessFile(ctx context.Context, path, tmpDir string, opts ProcessOptions) error {
	content, err := os.ReadFile(path)
	if err != nil {
		return err
	}

	if len(content) > 10*1024*1024 {
		return nil
	}

	relPath, _ := filepath.Rel(tmpDir, path)
	relPathURL := strings.ReplaceAll(relPath, "\\", "/")
	fileURL := opts.RepoURL + "/blob/" + opts.Branch + "/" + relPathURL

	contentHash := computeHash(content)

	doc := &domain.Document{
		URL:            fileURL,
		Title:          ExtractTitleFromPath(relPath),
		Content:        string(content),
		ContentHash:    contentHash,
		FetchedAt:      time.Now(),
		WordCount:      len(strings.Fields(string(content))),
		CharCount:      len(content),
		SourceStrategy: "git",
		RelativePath:   relPath,
	}

	ext := strings.ToLower(filepath.Ext(path))
	if ext != ".md" && ext != ".mdx" {
		doc.Content = "```\n" + string(content) + "\n```"
	}

	if opts.StateManager != nil {
		opts.StateManager.MarkSeen(fileURL)
		if !opts.StateManager.ShouldProcess(fileURL, contentHash) {
			if p.logger != nil {
				p.logger.Debug().Str("file", relPath).Msg("Skipping unchanged file")
			}
			return nil
		}
	}

	if !opts.DryRun && opts.WriteFunc != nil {
		return opts.WriteFunc(ctx, doc)
	}

	return nil
}

func computeHash(content []byte) string {
	hash := sha256.Sum256(content)
	return hex.EncodeToString(hash[:])
}

func ExtractTitleFromPath(path string) string {
	base := filepath.Base(path)
	ext := filepath.Ext(base)
	name := strings.TrimSuffix(base, ext)

	name = strings.ReplaceAll(name, "-", " ")
	name = strings.ReplaceAll(name, "_", " ")

	if len(name) > 0 {
		name = strings.ToUpper(name[:1]) + name[1:]
	}

	return name
}
</file>
<file path="internal/strategies/git/strategy.go">
package git

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type StrategyDependencies struct {
	Writer       *output.Writer
	Logger       *utils.Logger
	HTTPClient   *http.Client
	WriteFunc    func(ctx context.Context, doc *domain.Document) error
	StateManager *state.Manager
}

type Strategy struct {
	deps             *StrategyDependencies
	parser           *Parser
	archiveFetcher   *ArchiveFetcher
	cloneFetcher     *CloneFetcher
	processor        *Processor
	logger           *utils.Logger
	httpClient       *http.Client
	skipBranchDetect bool
}

func NewStrategy(deps *StrategyDependencies) *Strategy {
	var client *http.Client
	var skipBranchDetect bool

	if deps == nil {
		client = createDefaultHTTPClient()
		return &Strategy{
			httpClient: client,
			parser:     NewParser(),
		}
	}

	client = deps.HTTPClient
	if client == nil {
		client = createDefaultHTTPClient()
	} else {
		skipBranchDetect = true
	}

	logger := deps.Logger

	return &Strategy{
		deps:   deps,
		parser: NewParser(),
		archiveFetcher: NewArchiveFetcher(ArchiveFetcherOptions{
			HTTPClient: client,
			Logger:     logger,
		}),
		cloneFetcher: NewCloneFetcher(CloneFetcherOptions{
			Logger: logger,
		}),
		processor: NewProcessor(ProcessorOptions{
			Logger: logger,
		}),
		logger:           logger,
		httpClient:       client,
		skipBranchDetect: skipBranchDetect,
	}
}

func (s *Strategy) Name() string {
	return "git"
}

func (s *Strategy) CanHandle(url string) bool {
	lower := strings.ToLower(url)

	isDocsSubdomain := strings.Contains(lower, "docs.github.com") ||
		strings.Contains(lower, "pages.github.io") ||
		strings.Contains(lower, "github.io")

	if isDocsSubdomain {
		return false
	}

	if isWikiURL(url) {
		return false
	}

	return strings.HasPrefix(url, "git@") ||
		strings.HasSuffix(lower, ".git") ||
		(strings.Contains(lower, "github.com") && !strings.Contains(lower, "/blob/")) ||
		(strings.Contains(lower, "gitlab.com") && !strings.Contains(lower, "/-/blob/")) ||
		strings.Contains(lower, "bitbucket.org")
}

type ExecuteOptions struct {
	Output      string
	Concurrency int
	Limit       int
	DryRun      bool
	FilterURL   string
}

func (s *Strategy) Execute(ctx context.Context, rawURL string, opts ExecuteOptions) error {
	if s.logger != nil {
		s.logger.Info().Str("url", rawURL).Msg("Starting git extraction")
	}

	urlInfo, err := s.parser.ParseURLWithPath(rawURL)
	if err != nil {
		return fmt.Errorf("failed to parse git URL: %w", err)
	}

	filterPath := urlInfo.SubPath
	if filterPath == "" && opts.FilterURL != "" {
		filterPath = NormalizeFilterPath(opts.FilterURL)
	}

	if filterPath != "" && s.logger != nil {
		s.logger.Info().Str("filter_path", filterPath).Msg("Path filter active")
	}

	tmpDir, err := os.MkdirTemp("", "repodocs-git-*")
	if err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tmpDir)

	repoURL := urlInfo.RepoURL
	branch, method, err := s.TryArchiveDownload(ctx, repoURL, tmpDir)
	if err != nil {
		if s.logger != nil {
			s.logger.Info().Err(err).Msg("Archive download failed, using git clone")
		}
		branch, err = s.CloneRepository(ctx, repoURL, tmpDir)
		if err != nil {
			return fmt.Errorf("failed to acquire repository: %w", err)
		}
		method = "clone"
	}

	if urlInfo.Branch != "" {
		branch = urlInfo.Branch
	}

	if s.logger != nil {
		s.logger.Info().
			Str("method", method).
			Str("branch", branch).
			Msg("Repository acquired successfully")
	}

	files, err := s.processor.FindDocumentationFiles(tmpDir, filterPath)
	if err != nil {
		return err
	}

	if len(files) == 0 && filterPath != "" {
		return fmt.Errorf("no documentation files found under path: %s", filterPath)
	}

	if s.logger != nil {
		s.logger.Info().Int("count", len(files)).Msg("Found documentation files")
	}

	if opts.Limit > 0 && len(files) > opts.Limit {
		files = files[:opts.Limit]
	}

	processOpts := ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		FilterPath:   filterPath,
		Concurrency:  opts.Concurrency,
		Limit:        opts.Limit,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteFunc,
		StateManager: s.deps.StateManager,
	}

	return s.processor.ProcessFiles(ctx, files, tmpDir, processOpts)
}

func (s *Strategy) TryArchiveDownload(ctx context.Context, url, destDir string) (branch, method string, err error) {
	if strings.HasPrefix(url, "git@") {
		return "", "", fmt.Errorf("SSH URLs not supported for archive download")
	}

	info, err := s.parser.ParseURL(url)
	if err != nil {
		return "", "", err
	}

	if !s.skipBranchDetect {
		branch, err = DetectDefaultBranch(ctx, url)
		if err != nil {
			if s.logger != nil {
				s.logger.Warn().Err(err).Msg("Failed to detect branch, using 'main'")
			}
			branch = "main"
		}
	} else {
		branch = "main"
	}

	result, err := s.archiveFetcher.Fetch(ctx, info, branch, destDir)
	if err != nil {
		if branch == "main" {
			if s.logger != nil {
				s.logger.Debug().Msg("Trying 'master' branch")
			}
			result, err = s.archiveFetcher.Fetch(ctx, info, "master", destDir)
			if err == nil {
				return "master", "archive", nil
			}
		}
		return "", "", err
	}

	return result.Branch, result.Method, nil
}

func (s *Strategy) CloneRepository(ctx context.Context, url, destDir string) (string, error) {
	info := &RepoInfo{URL: url}
	result, err := s.cloneFetcher.Fetch(ctx, info, "", destDir)
	if err != nil {
		return "", err
	}
	return result.Branch, nil
}

func isWikiURL(url string) bool {
	lower := strings.ToLower(url)
	return strings.Contains(lower, "/wiki") ||
		strings.HasSuffix(lower, ".wiki.git")
}

func createDefaultHTTPClient() *http.Client {
	return &http.Client{
		Timeout: 10 * time.Minute,
		CheckRedirect: func(req *http.Request, via []*http.Request) error {
			if len(via) >= 10 {
				return fmt.Errorf("too many redirects")
			}
			return nil
		},
	}
}
</file>
<file path="internal/strategies/git/types.go">
package git

// Platform represents a git hosting platform
type Platform string

const (
	PlatformGitHub    Platform = "github"
	PlatformGitLab    Platform = "gitlab"
	PlatformBitbucket Platform = "bitbucket"
	PlatformGeneric   Platform = "generic"
)

// RepoInfo contains parsed repository information
type RepoInfo struct {
	Platform Platform
	Owner    string
	Repo     string
	URL      string // Original URL
}

// GitURLInfo contains parsed Git URL information including optional path
type GitURLInfo struct {
	RepoURL  string // Clean repository URL (without /tree/... suffix)
	Platform Platform
	Owner    string
	Repo     string
	Branch   string // Branch from URL (empty if not specified)
	SubPath  string // Subdirectory path (empty if root)
}

// FetchResult contains the result of a repository fetch operation
type FetchResult struct {
	LocalPath string // Path to extracted/cloned repo
	Branch    string // Detected or specified branch
	Method    string // "archive" or "clone"
}

// DocumentExtensions are file extensions to process (markdown only)
var DocumentExtensions = map[string]bool{
	".md":  true,
	".mdx": true,
}

// IgnoreDirs are directories to skip during file discovery
var IgnoreDirs = map[string]bool{
	".git":         true,
	"node_modules": true,
	"vendor":       true,
	"__pycache__":  true,
	".venv":        true,
	"venv":         true,
	"dist":         true,
	"build":        true,
	".next":        true,
	".nuxt":        true,
}
</file>
<file path="internal/strategies/crawler.go">
package strategies

import (
	"context"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/gocolly/colly/v2"
	"github.com/schollz/progressbar/v3"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// CrawlerStrategy crawls websites to extract documentation
type CrawlerStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// crawlContext holds shared state between concurrent crawler callbacks.
type crawlContext struct {
	ctx            context.Context
	baseURL        string
	opts           Options
	visited        *sync.Map
	processedCount *int
	mu             *sync.Mutex
	bar            *progressbar.ProgressBar
	barMu          *sync.Mutex
	excludeRegexps []*regexp.Regexp
}

func newCrawlContext(ctx context.Context, baseURL string, opts Options) *crawlContext {
	var excludeRegexps []*regexp.Regexp
	for _, pattern := range opts.Exclude {
		if re, err := regexp.Compile(pattern); err == nil {
			excludeRegexps = append(excludeRegexps, re)
		}
	}

	var processedCount int
	return &crawlContext{
		ctx:            ctx,
		baseURL:        baseURL,
		opts:           opts,
		visited:        &sync.Map{},
		processedCount: &processedCount,
		mu:             &sync.Mutex{},
		bar:            utils.NewProgressBar(-1, utils.DescExtracting),
		barMu:          &sync.Mutex{},
		excludeRegexps: excludeRegexps,
	}
}

func (s *CrawlerStrategy) shouldProcessURL(link, baseURL string, cctx *crawlContext) bool {
	if link == "" {
		return false
	}

	if !utils.IsSameDomain(link, baseURL) {
		return false
	}

	if cctx.opts.FilterURL != "" && !utils.HasBaseURL(link, cctx.opts.FilterURL) {
		return false
	}

	for _, re := range cctx.excludeRegexps {
		if re.MatchString(link) {
			return false
		}
	}

	cctx.mu.Lock()
	if cctx.opts.Limit > 0 && *cctx.processedCount >= cctx.opts.Limit {
		cctx.mu.Unlock()
		return false
	}
	cctx.mu.Unlock()

	if _, exists := cctx.visited.LoadOrStore(link, true); exists {
		return false
	}

	return true
}

func (s *CrawlerStrategy) processMarkdownResponse(body []byte, url string) (*domain.Document, error) {
	doc, err := s.markdownReader.Read(string(body), url)
	if err != nil {
		s.logger.Warn().Err(err).Str("url", url).Msg("Failed to read markdown")
		return nil, err
	}
	return doc, nil
}

func (s *CrawlerStrategy) processHTMLResponse(ctx context.Context, body []byte, url string, opts Options) (*domain.Document, error) {
	html := string(body)

	if opts.RenderJS || renderer.NeedsJSRendering(html) {
		if r, err := s.deps.GetRenderer(); err == nil {
			s.renderer = r
			rendered, err := s.renderer.Render(ctx, url, domain.RenderOptions{
				Timeout:     60 * time.Second,
				WaitStable:  2 * time.Second,
				ScrollToEnd: true,
			})
			if err == nil {
				html = rendered
			}
		}
	}

	doc, err := s.converter.Convert(ctx, html, url)
	if err != nil {
		s.logger.Warn().Err(err).Str("url", url).Msg("Failed to convert page")
		return nil, err
	}

	return doc, nil
}

func (s *CrawlerStrategy) processResponse(ctx context.Context, r *colly.Response, cctx *crawlContext) {
	select {
	case <-ctx.Done():
		return
	default:
	}

	contentType := r.Headers.Get("Content-Type")
	currentURL := r.Request.URL.String()
	isMarkdown := converter.IsMarkdownContent(contentType, currentURL)
	isHTML := IsHTMLContentType(contentType)

	if !isMarkdown && !isHTML {
		return
	}

	cctx.mu.Lock()
	if cctx.opts.Limit > 0 && *cctx.processedCount >= cctx.opts.Limit {
		cctx.mu.Unlock()
		return
	}
	*cctx.processedCount++
	cctx.mu.Unlock()

	cctx.barMu.Lock()
	cctx.bar.Add(1)
	cctx.barMu.Unlock()

	if !cctx.opts.Force && s.writer.Exists(currentURL) {
		return
	}

	var doc *domain.Document
	var err error

	if isMarkdown {
		doc, err = s.processMarkdownResponse(r.Body, currentURL)
	} else {
		doc, err = s.processHTMLResponse(ctx, r.Body, currentURL, cctx.opts)
	}

	if err != nil || doc == nil {
		return
	}

	doc.SourceStrategy = s.Name()
	doc.FetchedAt = time.Now()

	if s.deps.StateManager != nil {
		s.deps.StateManager.MarkSeen(currentURL)
		if doc.ContentHash != "" && !s.deps.StateManager.ShouldProcess(currentURL, doc.ContentHash) {
			s.logger.Debug().Str("url", currentURL).Msg("Skipping unchanged page")
			return
		}
	}

	if !cctx.opts.DryRun {
		if err := s.deps.WriteDocument(ctx, doc); err != nil {
			s.logger.Warn().Err(err).Str("url", currentURL).Msg("Failed to write document")
		}
	}
}

// NewCrawlerStrategy creates a new crawler strategy
func NewCrawlerStrategy(deps *Dependencies) *CrawlerStrategy {
	if deps == nil {
		return &CrawlerStrategy{}
	}
	return &CrawlerStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *CrawlerStrategy) Name() string {
	return "crawler"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *CrawlerStrategy) CanHandle(url string) bool {
	return utils.IsHTTPURL(url)
}

// SetFetcher allows setting a custom fetcher for testing
func (s *CrawlerStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

func (s *CrawlerStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Starting web crawl")

	if opts.FilterURL != "" {
		s.logger.Info().Str("filter", opts.FilterURL).Msg("URL filter active - only crawling URLs under this path")
	}

	cctx := newCrawlContext(ctx, url, opts)

	c := colly.NewCollector(
		colly.Async(true),
		colly.MaxDepth(opts.MaxDepth),
	)

	c.WithTransport(s.fetcher.Transport())

	_ = c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: opts.Concurrency,
		RandomDelay: 2 * time.Second,
	})

	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Request.AbsoluteURL(e.Attr("href"))
		if s.shouldProcessURL(link, url, cctx) {
			_ = e.Request.Visit(link)
		}
	})

	c.OnResponse(func(r *colly.Response) {
		s.processResponse(ctx, r, cctx)
	})

	c.OnError(func(r *colly.Response, err error) {
		s.logger.Debug().Err(err).Str("url", r.Request.URL.String()).Msg("Request failed")
	})

	if err := c.Visit(url); err != nil {
		return err
	}

	done := make(chan struct{})
	go func() {
		c.Wait()
		close(done)
	}()

	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-done:
	}

	s.logger.Info().Int("pages", *cctx.processedCount).Msg("Crawl completed")
	return nil
}

// IsHTMLContentType checks if content type is HTML
func IsHTMLContentType(contentType string) bool {
	if contentType == "" {
		return true
	}
	lower := strings.ToLower(contentType)
	return strings.Contains(lower, "text/html") ||
		strings.Contains(lower, "application/xhtml")
}
</file>
<file path="internal/strategies/docsrs.go">
package strategies

import (
	"context"
	"fmt"
	"net/url"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type DocsRSURL struct {
	CrateName    string
	Version      string
	ModulePath   string
	IsCratePage  bool
	IsSourceView bool
}

type DocsRSStrategy struct {
	deps     *Dependencies
	fetcher  domain.Fetcher
	writer   *output.Writer
	logger   *utils.Logger
	baseHost string
}

func NewDocsRSStrategy(deps *Dependencies) *DocsRSStrategy {
	if deps == nil {
		return &DocsRSStrategy{baseHost: "docs.rs"}
	}
	return &DocsRSStrategy{
		deps:     deps,
		fetcher:  deps.Fetcher,
		writer:   deps.Writer,
		logger:   deps.Logger,
		baseHost: "docs.rs",
	}
}

func (s *DocsRSStrategy) Name() string {
	return "docsrs"
}

func (s *DocsRSStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

func (s *DocsRSStrategy) SetBaseHost(host string) {
	s.baseHost = host
}

func (s *DocsRSStrategy) parseURL(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPathWithHost(rawURL, s.baseHost)
}

func (s *DocsRSStrategy) CanHandle(rawURL string) bool {
	parsed, err := parseDocsRSPath(rawURL)
	if err != nil {
		return false
	}

	if parsed.IsSourceView {
		return false
	}

	return parsed.CrateName != ""
}

func parseDocsRSPath(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPathWithHost(rawURL, "docs.rs")
}

func parseDocsRSPathWithHost(rawURL, expectedHost string) (*DocsRSURL, error) {
	u, err := url.Parse(rawURL)
	if err != nil {
		return nil, err
	}

	if !strings.Contains(u.Host, expectedHost) {
		return nil, fmt.Errorf("not a docs.rs URL")
	}

	u.Fragment = ""
	u.RawQuery = ""

	segments := strings.Split(strings.Trim(u.Path, "/"), "/")
	if len(segments) == 0 || segments[0] == "" {
		return nil, fmt.Errorf("empty path")
	}

	result := &DocsRSURL{}

	if segments[0] == "crate" {
		result.IsCratePage = true
		if len(segments) >= 2 {
			result.CrateName = segments[1]
		}
		if len(segments) >= 3 {
			result.Version = segments[2]
		} else {
			result.Version = "latest"
		}
		if len(segments) >= 4 && (segments[3] == "source" || segments[3] == "src") {
			result.IsSourceView = true
		}
		return result, nil
	}

	for _, seg := range segments {
		if seg == "src" || seg == "source" {
			result.IsSourceView = true
		}
	}

	result.CrateName = segments[0]

	if len(segments) >= 2 {
		result.Version = segments[1]
	} else {
		result.Version = "latest"
	}

	if len(segments) >= 4 {
		result.ModulePath = strings.Join(segments[3:], "/")
	}

	return result, nil
}

func (s *DocsRSStrategy) Execute(ctx context.Context, rawURL string, opts Options) error {
	s.logger.Info().Str("url", rawURL).Msg("Starting docs.rs JSON extraction")

	if s.fetcher == nil {
		return fmt.Errorf("docsrs strategy fetcher is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("docsrs strategy writer is nil")
	}

	baseInfo, err := s.parseURL(rawURL)
	if err != nil {
		return fmt.Errorf("invalid docs.rs URL: %w", err)
	}

	s.logger.Info().
		Str("crate", baseInfo.CrateName).
		Str("version", baseInfo.Version).
		Msg("Parsed docs.rs URL")

	index, err := s.fetchRustdocJSON(ctx, baseInfo.CrateName, baseInfo.Version)
	if err != nil {
		return fmt.Errorf("failed to fetch rustdoc JSON: %w", err)
	}

	if err := s.checkFormatVersion(index.FormatVersion); err != nil {
		return err
	}

	renderer := NewRustdocRenderer(index, baseInfo.CrateName, baseInfo.Version)

	items := s.collectItems(index, opts)
	s.logger.Info().Int("count", len(items)).Msg("Collected items to process")

	if opts.Limit > 0 && len(items) > opts.Limit {
		items = items[:opts.Limit]
		s.logger.Info().Int("limit", opts.Limit).Msg("Applied item limit")
	}

	bar := utils.NewProgressBar(len(items), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, items, opts.Concurrency, func(ctx context.Context, item *RustdocItem) error {
		defer bar.Add(1)
		return s.processItem(ctx, item, renderer, baseInfo, opts)
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Int("items", len(items)).Msg("docs.rs JSON extraction completed")
	return nil
}

func (s *DocsRSStrategy) processItem(ctx context.Context, item *RustdocItem, renderer *RustdocRenderer, baseInfo *DocsRSURL, opts Options) error {
	itemURL := s.buildItemURL(item, baseInfo)

	if !opts.Force && s.writer.Exists(itemURL) {
		return nil
	}

	markdown := renderer.RenderItem(item)
	if markdown == "" {
		return nil
	}

	doc := &domain.Document{
		URL:            itemURL,
		Title:          s.buildItemTitle(item),
		Content:        markdown,
		Description:    s.buildItemDescription(item, baseInfo),
		SourceStrategy: s.Name(),
		FetchedAt:      time.Now(),
		Tags:           s.buildItemTags(item, baseInfo),
	}

	if !opts.DryRun {
		if err := s.deps.WriteDocument(ctx, doc); err != nil {
			s.logger.Warn().Err(err).Str("url", itemURL).Msg("Failed to write document")
			return nil
		}
	}

	return nil
}

func (s *DocsRSStrategy) buildItemURL(item *RustdocItem, baseInfo *DocsRSURL) string {
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	itemType := ""
	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return fmt.Sprintf("https://docs.rs/%s/%s/%s/",
				baseInfo.CrateName, baseInfo.Version, baseInfo.CrateName)
		}
		itemType = "mod"
	} else if item.GetStruct() != nil {
		itemType = "struct"
	} else if item.GetEnum() != nil {
		itemType = "enum"
	} else if item.GetTrait() != nil {
		itemType = "trait"
	} else if item.GetFunction() != nil {
		itemType = "fn"
	} else if item.GetTypeAlias() != nil {
		itemType = "type"
	} else if item.GetConstant() != nil {
		itemType = "constant"
	} else if item.GetMacro() != nil {
		itemType = "macro"
	} else {
		itemType = "item"
	}

	path := baseInfo.CrateName
	if item.Span != nil && item.Span.Filename != "" {
		spanPath := strings.TrimPrefix(item.Span.Filename, "src/")
		spanPath = strings.TrimSuffix(spanPath, ".rs")
		spanPath = strings.TrimSuffix(spanPath, "/mod")
		if spanPath != "lib" && spanPath != "" {
			path = baseInfo.CrateName + "/" + strings.ReplaceAll(spanPath, "/", "::")
		}
	}

	return fmt.Sprintf("https://docs.rs/%s/%s/%s/%s.%s.html",
		baseInfo.CrateName, baseInfo.Version, path, itemType, name)
}

func (s *DocsRSStrategy) buildItemTitle(item *RustdocItem) string {
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return fmt.Sprintf("Crate %s", name)
		}
		return fmt.Sprintf("Module %s", name)
	}
	if item.GetStruct() != nil {
		return fmt.Sprintf("Struct %s", name)
	}
	if item.GetEnum() != nil {
		return fmt.Sprintf("Enum %s", name)
	}
	if item.GetTrait() != nil {
		return fmt.Sprintf("Trait %s", name)
	}
	if item.GetFunction() != nil {
		return fmt.Sprintf("Function %s", name)
	}
	if item.GetTypeAlias() != nil {
		return fmt.Sprintf("Type %s", name)
	}
	if item.GetMacro() != nil {
		return fmt.Sprintf("Macro %s", name)
	}
	return name
}

func (s *DocsRSStrategy) buildItemDescription(item *RustdocItem, baseInfo *DocsRSURL) string {
	itemType := s.getItemTypeName(item)
	stability := "stable"
	if item.Deprecation != nil {
		stability = "deprecated"
	}

	return fmt.Sprintf("crate:%s version:%s type:%s stability:%s",
		baseInfo.CrateName, baseInfo.Version, itemType, stability)
}

func (s *DocsRSStrategy) buildItemTags(item *RustdocItem, baseInfo *DocsRSURL) []string {
	itemType := s.getItemTypeName(item)

	tags := []string{
		"docs.rs",
		"rust",
		baseInfo.CrateName,
		itemType,
	}

	if item.Deprecation != nil {
		tags = append(tags, "deprecated")
	}

	return tags
}

func (s *DocsRSStrategy) getItemTypeName(item *RustdocItem) string {
	if item.GetModule() != nil {
		return "module"
	}
	if item.GetStruct() != nil {
		return "struct"
	}
	if item.GetEnum() != nil {
		return "enum"
	}
	if item.GetTrait() != nil {
		return "trait"
	}
	if item.GetFunction() != nil {
		return "function"
	}
	if item.GetTypeAlias() != nil {
		return "type"
	}
	if item.GetMacro() != nil {
		return "macro"
	}
	return "item"
}

func ParseDocsRSPathForTest(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPath(rawURL)
}

// BuildItemURLForTest exposes buildItemURL for testing
func (s *DocsRSStrategy) BuildItemURLForTest(item *RustdocItem, baseInfo *DocsRSURL) string {
	return s.buildItemURL(item, baseInfo)
}

// BuildItemTitleForTest exposes buildItemTitle for testing
func (s *DocsRSStrategy) BuildItemTitleForTest(item *RustdocItem) string {
	return s.buildItemTitle(item)
}

// GetItemTypeNameForTest exposes getItemTypeName for testing
func (s *DocsRSStrategy) GetItemTypeNameForTest(item *RustdocItem) string {
	return s.getItemTypeName(item)
}

// BuildItemDescriptionForTest exposes buildItemDescription for testing
func (s *DocsRSStrategy) BuildItemDescriptionForTest(item *RustdocItem, baseInfo *DocsRSURL) string {
	return s.buildItemDescription(item, baseInfo)
}

// BuildItemTagsForTest exposes buildItemTags for testing
func (s *DocsRSStrategy) BuildItemTagsForTest(item *RustdocItem, baseInfo *DocsRSURL) []string {
	return s.buildItemTags(item, baseInfo)
}
</file>
<file path="internal/strategies/docsrs_json.go">
package strategies

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	"github.com/klauspost/compress/zstd"
)

func DocsRSJSONEndpoint(crateName, version string) string {
	return fmt.Sprintf("https://docs.rs/crate/%s/%s/json", crateName, version)
}

func ParseRustdocJSON(data []byte) (*RustdocIndex, error) {
	var index RustdocIndex
	if err := json.Unmarshal(data, &index); err != nil {
		return nil, fmt.Errorf("failed to parse rustdoc JSON: %w", err)
	}
	return &index, nil
}

func (s *DocsRSStrategy) buildJSONEndpoint(crateName, version string) string {
	if s.baseHost != "docs.rs" {
		return fmt.Sprintf("http://%s/crate/%s/%s/json", s.baseHost, crateName, version)
	}
	return DocsRSJSONEndpoint(crateName, version)
}

func (s *DocsRSStrategy) fetchRustdocJSON(ctx context.Context, crateName, version string) (*RustdocIndex, error) {
	endpoint := s.buildJSONEndpoint(crateName, version)

	s.logger.Info().
		Str("crate", crateName).
		Str("version", version).
		Str("endpoint", endpoint).
		Msg("Fetching rustdoc JSON")

	resp, err := s.fetcher.Get(ctx, endpoint)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch rustdoc JSON: %w", err)
	}

	var jsonData []byte
	contentType := resp.ContentType

	// Check for zstd compression via content-type OR magic bytes
	// Magic bytes 0x28 0xB5 0x2F 0xFD indicate zstd compression
	// This handles cached responses where content-type may be lost
	isZstd := strings.Contains(contentType, "zstd") ||
		strings.Contains(contentType, "x-zstd") ||
		strings.HasSuffix(endpoint, ".zst") ||
		(len(resp.Body) >= 4 && resp.Body[0] == 0x28 && resp.Body[1] == 0xB5 && resp.Body[2] == 0x2F && resp.Body[3] == 0xFD)

	if isZstd {
		decoder, err := zstd.NewReader(nil)
		if err != nil {
			return nil, fmt.Errorf("failed to create zstd decoder: %w", err)
		}
		defer decoder.Close()

		jsonData, err = decoder.DecodeAll(resp.Body, nil)
		if err != nil {
			return nil, fmt.Errorf("failed to decompress zstd: %w", err)
		}
	} else {
		jsonData = resp.Body
	}

	s.logger.Debug().
		Int("compressed_size", len(resp.Body)).
		Int("decompressed_size", len(jsonData)).
		Msg("Processed rustdoc JSON")

	index, err := ParseRustdocJSON(jsonData)
	if err != nil {
		return nil, err
	}

	s.logger.Info().
		Int("items", len(index.Index)).
		Int("format_version", index.FormatVersion).
		Str("crate_version", index.CrateVersion).
		Msg("Parsed rustdoc index")

	return index, nil
}

const (
	MinFormatVersion = 30
	MaxFormatVersion = 60
)

func (s *DocsRSStrategy) checkFormatVersion(version int) error {
	if version < MinFormatVersion {
		return fmt.Errorf("rustdoc JSON format version %d is too old (min: %d)", version, MinFormatVersion)
	}
	if version > MaxFormatVersion {
		s.logger.Warn().Int("version", version).Msg("Untested format version, proceeding anyway")
	}
	return nil
}

func (s *DocsRSStrategy) getItemByID(index *RustdocIndex, id interface{}) *RustdocItem {
	switch v := id.(type) {
	case string:
		return index.Index[v]
	case float64:
		return index.Index[fmt.Sprintf("%.0f", v)]
	case int:
		return index.Index[fmt.Sprintf("%d", v)]
	default:
		return nil
	}
}

func (s *DocsRSStrategy) collectItems(index *RustdocIndex, opts Options) []*RustdocItem {
	var items []*RustdocItem

	for _, item := range index.Index {
		if item.CrateID != 0 {
			continue
		}

		if item.Name == nil {
			continue
		}

		if item.Docs == nil && !s.hasDocumentableChildren(item) {
			continue
		}

		if !item.IsPublic() {
			continue
		}

		if item.GetUse() != nil {
			continue
		}

		items = append(items, item)
	}

	return items
}

func (s *DocsRSStrategy) hasDocumentableChildren(item *RustdocItem) bool {
	if mod := item.GetModule(); mod != nil {
		return len(mod.Items) > 0
	}
	if trait := item.GetTrait(); trait != nil {
		return len(trait.Items) > 0
	}
	if st := item.GetStruct(); st != nil {
		return len(st.Impls) > 0
	}
	if en := item.GetEnum(); en != nil {
		return len(en.Variants) > 0 || len(en.Impls) > 0
	}
	return false
}
</file>
<file path="internal/strategies/docsrs_renderer.go">
package strategies

import (
	"fmt"
	"strings"
)

type RustdocRenderer struct {
	index     *RustdocIndex
	crateName string
	version   string
}

func NewRustdocRenderer(index *RustdocIndex, crateName, version string) *RustdocRenderer {
	return &RustdocRenderer{
		index:     index,
		crateName: crateName,
		version:   version,
	}
}

func (r *RustdocRenderer) RenderItem(item *RustdocItem) string {
	var sb strings.Builder

	itemType := r.getItemType(item)
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	if name != "" {
		sb.WriteString(fmt.Sprintf("# %s `%s`\n\n", itemType, name))
	}

	if item.Deprecation != nil {
		sb.WriteString("> **Deprecated**")
		if item.Deprecation.Since != "" {
			sb.WriteString(fmt.Sprintf(" since %s", item.Deprecation.Since))
		}
		if item.Deprecation.Note != "" {
			sb.WriteString(fmt.Sprintf(": %s", item.Deprecation.Note))
		}
		sb.WriteString("\n\n")
	}

	sig := r.renderSignature(item)
	if sig != "" {
		sb.WriteString("```rust\n")
		sb.WriteString(sig)
		sb.WriteString("\n```\n\n")
	}

	if item.Docs != nil && *item.Docs != "" {
		docs := r.resolveCrossRefs(*item.Docs, item.Links)
		sb.WriteString(docs)
		sb.WriteString("\n\n")
	}

	if mod := item.GetModule(); mod != nil {
		sb.WriteString(r.renderModuleContents(item))
	}

	if trait := item.GetTrait(); trait != nil {
		sb.WriteString(r.renderTraitContents(item))
	}

	if item.GetStruct() != nil || item.GetEnum() != nil {
		sb.WriteString(r.renderImplContents(item))
	}

	return sb.String()
}

func (r *RustdocRenderer) getItemType(item *RustdocItem) string {
	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return "Crate"
		}
		return "Module"
	}
	if item.GetStruct() != nil {
		return "Struct"
	}
	if item.GetEnum() != nil {
		return "Enum"
	}
	if item.GetTrait() != nil {
		return "Trait"
	}
	if item.GetFunction() != nil {
		return "Function"
	}
	if item.GetTypeAlias() != nil {
		return "Type Alias"
	}
	if item.GetConstant() != nil {
		return "Constant"
	}
	if item.GetMacro() != nil {
		return "Macro"
	}
	if item.GetUse() != nil {
		return "Re-export"
	}
	if item.GetVariant() != nil {
		return "Variant"
	}
	return "Item"
}

func (r *RustdocRenderer) renderSignature(item *RustdocItem) string {
	if item.GetFunction() != nil {
		return r.renderFunctionSignature(item)
	}
	if item.GetTrait() != nil {
		return r.renderTraitSignature(item)
	}
	if item.GetStruct() != nil {
		return r.renderStructSignature(item)
	}
	if item.GetEnum() != nil {
		return r.renderEnumSignature(item)
	}
	if item.GetTypeAlias() != nil {
		return r.renderTypeAliasSignature(item)
	}
	if item.GetConstant() != nil {
		return r.renderConstantSignature(item)
	}
	return ""
}

func (r *RustdocRenderer) renderFunctionSignature(item *RustdocItem) string {
	fn := item.GetFunction()
	if fn == nil {
		return ""
	}

	var sb strings.Builder

	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	if fn.Header != nil {
		if fn.Header.IsConst {
			sb.WriteString("const ")
		}
		if fn.Header.IsAsync {
			sb.WriteString("async ")
		}
		if fn.Header.IsUnsafe {
			sb.WriteString("unsafe ")
		}
	}

	sb.WriteString("fn ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}

	if fn.Generics != nil {
		sb.WriteString(r.renderGenerics(fn.Generics))
	}

	sb.WriteString("(")
	if fn.Sig != nil {
		for i, input := range fn.Sig.Inputs {
			if i > 0 {
				sb.WriteString(", ")
			}
			if arr, ok := input.([]interface{}); ok && len(arr) >= 2 {
				name := fmt.Sprintf("%v", arr[0])
				typeStr := r.RenderType(arr[1])
				if name == "self" {
					sb.WriteString(typeStr)
				} else {
					sb.WriteString(fmt.Sprintf("%s: %s", name, typeStr))
				}
			}
		}
	}
	sb.WriteString(")")

	if fn.Sig != nil && fn.Sig.Output != nil {
		outputStr := r.RenderType(fn.Sig.Output)
		if outputStr != "" && outputStr != "()" {
			sb.WriteString(" -> ")
			sb.WriteString(outputStr)
		}
	}

	if fn.Generics != nil {
		sb.WriteString(r.renderWhereClauses(fn.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) RenderType(t interface{}) string {
	if t == nil {
		return "()"
	}

	switch v := t.(type) {
	case map[string]interface{}:
		return r.RenderTypeMap(v)
	case string:
		return v
	default:
		return fmt.Sprintf("%v", v)
	}
}

func (r *RustdocRenderer) RenderTypeMap(t map[string]interface{}) string {
	if prim, ok := t["primitive"]; ok {
		return fmt.Sprintf("%v", prim)
	}

	if gen, ok := t["generic"]; ok {
		return fmt.Sprintf("%v", gen)
	}

	if resolved, ok := t["resolved_path"].(map[string]interface{}); ok {
		path := fmt.Sprintf("%v", resolved["path"])
		if args := resolved["args"]; args != nil {
			if argsMap, ok := args.(map[string]interface{}); ok {
				if angleArgs, ok := argsMap["angle_bracketed"].(map[string]interface{}); ok {
					if typeArgs, ok := angleArgs["args"].([]interface{}); ok && len(typeArgs) > 0 {
						var argStrs []string
						for _, arg := range typeArgs {
							if argMap, ok := arg.(map[string]interface{}); ok {
								if typeArg, ok := argMap["type"]; ok {
									argStrs = append(argStrs, r.RenderType(typeArg))
								}
							}
						}
						if len(argStrs) > 0 {
							path += "<" + strings.Join(argStrs, ", ") + ">"
						}
					}
				}
			}
		}
		return path
	}

	if borrowed, ok := t["borrowed_ref"].(map[string]interface{}); ok {
		mut := ""
		if borrowed["is_mutable"] == true {
			mut = "mut "
		}
		lifetime := ""
		if l, ok := borrowed["lifetime"].(string); ok && l != "" {
			lifetime = l + " "
		}
		inner := r.RenderType(borrowed["type"])
		return fmt.Sprintf("&%s%s%s", lifetime, mut, inner)
	}

	if slice, ok := t["slice"]; ok {
		return fmt.Sprintf("[%s]", r.RenderType(slice))
	}

	if arr, ok := t["array"].(map[string]interface{}); ok {
		innerType := r.RenderType(arr["type"])
		length := arr["len"]
		return fmt.Sprintf("[%s; %v]", innerType, length)
	}

	if tuple, ok := t["tuple"].([]interface{}); ok {
		if len(tuple) == 0 {
			return "()"
		}
		parts := make([]string, len(tuple))
		for i, elem := range tuple {
			parts[i] = r.RenderType(elem)
		}
		return fmt.Sprintf("(%s)", strings.Join(parts, ", "))
	}

	if rawPtr, ok := t["raw_pointer"].(map[string]interface{}); ok {
		mut := "*const"
		if rawPtr["is_mutable"] == true {
			mut = "*mut"
		}
		inner := r.RenderType(rawPtr["type"])
		return fmt.Sprintf("%s %s", mut, inner)
	}

	if implTrait, ok := t["impl_trait"].([]interface{}); ok {
		var bounds []string
		for _, bound := range implTrait {
			if boundMap, ok := bound.(map[string]interface{}); ok {
				if traitBound, ok := boundMap["trait_bound"].(map[string]interface{}); ok {
					if trait, ok := traitBound["trait"].(map[string]interface{}); ok {
						if path, ok := trait["path"].(string); ok {
							bounds = append(bounds, path)
						}
					}
				}
			}
		}
		if len(bounds) > 0 {
			return "impl " + strings.Join(bounds, " + ")
		}
		return "impl ..."
	}

	if qualPath, ok := t["qualified_path"].(map[string]interface{}); ok {
		name := ""
		if n, ok := qualPath["name"].(string); ok {
			name = n
		}
		return name
	}

	return "..."
}

func (r *RustdocRenderer) renderGenerics(g *RustdocGenerics) string {
	if g == nil || len(g.Params) == 0 {
		return ""
	}

	var parts []string
	for _, p := range g.Params {
		if p.Name != "" && p.Name != "Self" {
			parts = append(parts, p.Name)
		}
	}

	if len(parts) == 0 {
		return ""
	}

	return fmt.Sprintf("<%s>", strings.Join(parts, ", "))
}

func (r *RustdocRenderer) renderWhereClauses(g *RustdocGenerics) string {
	if g == nil || len(g.WherePredicates) == 0 {
		return ""
	}
	return ""
}

func (r *RustdocRenderer) resolveCrossRefs(docs string, links map[string]interface{}) string {
	if links == nil || len(links) == 0 {
		return docs
	}

	result := docs
	for name, id := range links {
		targetItem := r.getItemByID(id)
		if targetItem == nil {
			continue
		}

		targetName := ""
		if targetItem.Name != nil {
			targetName = *targetItem.Name
		}

		targetURL := fmt.Sprintf("https://docs.rs/%s/%s/%s/%s",
			r.crateName, r.version, r.crateName, targetName)

		cleanName := strings.Trim(name, "`")
		result = strings.ReplaceAll(result,
			fmt.Sprintf("[%s]", name),
			fmt.Sprintf("[%s](%s)", cleanName, targetURL))
	}

	return result
}

func (r *RustdocRenderer) getItemByID(id interface{}) *RustdocItem {
	if r.index == nil {
		return nil
	}
	switch v := id.(type) {
	case string:
		return r.index.Index[v]
	case float64:
		return r.index.Index[fmt.Sprintf("%.0f", v)]
	case int:
		return r.index.Index[fmt.Sprintf("%d", v)]
	default:
		return nil
	}
}

func (r *RustdocRenderer) renderModuleContents(item *RustdocItem) string {
	mod := item.GetModule()
	if mod == nil || len(mod.Items) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Contents\n\n")

	groups := make(map[string][]*RustdocItem)
	for _, childID := range mod.Items {
		child := r.getItemByID(childID)
		if child == nil || child.Name == nil {
			continue
		}
		itemType := r.getItemType(child)
		groups[itemType] = append(groups[itemType], child)
	}

	order := []string{"Module", "Struct", "Enum", "Trait", "Function", "Type Alias", "Constant", "Macro"}
	for _, itemType := range order {
		if items, ok := groups[itemType]; ok && len(items) > 0 {
			sb.WriteString(fmt.Sprintf("### %ss\n\n", itemType))
			for _, child := range items {
				if child.Name != nil {
					sb.WriteString(fmt.Sprintf("- `%s`\n", *child.Name))
				}
			}
			sb.WriteString("\n")
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTraitContents(item *RustdocItem) string {
	trait := item.GetTrait()
	if trait == nil || len(trait.Items) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Required Methods\n\n")

	for _, childID := range trait.Items {
		child := r.getItemByID(childID)
		if child == nil {
			continue
		}

		if fn := child.GetFunction(); fn != nil && child.Name != nil {
			sb.WriteString(fmt.Sprintf("### `%s`\n\n", *child.Name))
			sb.WriteString("```rust\n")
			sb.WriteString(r.renderFunctionSignature(child))
			sb.WriteString("\n```\n\n")
			if child.Docs != nil {
				sb.WriteString(*child.Docs)
				sb.WriteString("\n\n")
			}
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderImplContents(item *RustdocItem) string {
	var impls []interface{}
	if st := item.GetStruct(); st != nil {
		impls = st.Impls
	} else if en := item.GetEnum(); en != nil {
		impls = en.Impls
	}

	if len(impls) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Implementations\n\n")

	for _, implID := range impls {
		implItem := r.getItemByID(implID)
		if implItem == nil {
			continue
		}

		impl := implItem.GetImpl()
		if impl == nil {
			continue
		}

		if impl.Trait != nil {
			if traitPath, ok := impl.Trait.(map[string]interface{}); ok {
				if path, ok := traitPath["path"].(string); ok {
					sb.WriteString(fmt.Sprintf("### impl %s\n\n", path))
				}
			}
		} else {
			sb.WriteString("### impl\n\n")
		}

		for _, methodID := range impl.Items {
			method := r.getItemByID(methodID)
			if method == nil || method.Name == nil {
				continue
			}

			sb.WriteString(fmt.Sprintf("#### `%s`\n\n", *method.Name))
			if fn := method.GetFunction(); fn != nil {
				sb.WriteString("```rust\n")
				sb.WriteString(r.renderFunctionSignature(method))
				sb.WriteString("\n```\n\n")
			}
			if method.Docs != nil {
				sb.WriteString(*method.Docs)
				sb.WriteString("\n\n")
			}
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTraitSignature(item *RustdocItem) string {
	trait := item.GetTrait()
	if trait == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	if trait.IsUnsafe {
		sb.WriteString("unsafe ")
	}
	if trait.IsAuto {
		sb.WriteString("auto ")
	}
	sb.WriteString("trait ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if trait.Generics != nil {
		sb.WriteString(r.renderGenerics(trait.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderStructSignature(item *RustdocItem) string {
	st := item.GetStruct()
	if st == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("struct ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if st.Generics != nil {
		sb.WriteString(r.renderGenerics(st.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderEnumSignature(item *RustdocItem) string {
	en := item.GetEnum()
	if en == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("enum ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if en.Generics != nil {
		sb.WriteString(r.renderGenerics(en.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTypeAliasSignature(item *RustdocItem) string {
	ta := item.GetTypeAlias()
	if ta == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("type ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if ta.Generics != nil {
		sb.WriteString(r.renderGenerics(ta.Generics))
	}
	if ta.Type != nil {
		sb.WriteString(" = ")
		sb.WriteString(r.RenderType(ta.Type))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderConstantSignature(item *RustdocItem) string {
	c := item.GetConstant()
	if c == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("const ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if c.Type != nil {
		sb.WriteString(": ")
		sb.WriteString(r.RenderType(c.Type))
	}

	return sb.String()
}
</file>
<file path="internal/strategies/docsrs_types.go">
package strategies

// RustdocIndex represents the top-level rustdoc JSON structure
type RustdocIndex struct {
	Root            interface{}               `json:"root"` // Can be int or string depending on format version
	CrateVersion    string                    `json:"crate_version"`
	FormatVersion   int                       `json:"format_version"`
	IncludesPrivate bool                      `json:"includes_private"`
	Index           map[string]*RustdocItem   `json:"index"`
	Paths           map[string]*RustdocPath   `json:"paths"`
	ExternalCrates  map[string]*ExternalCrate `json:"external_crates"`
}

// RustdocItem represents a single item in the index
type RustdocItem struct {
	ID          interface{}            `json:"id"` // Can be int or string
	CrateID     int                    `json:"crate_id"`
	Name        *string                `json:"name"` // nullable
	Span        *RustdocSpan           `json:"span"`
	Visibility  interface{}            `json:"visibility"` // Can be string or object
	Docs        *string                `json:"docs"`       // nullable, MARKDOWN!
	Links       map[string]interface{} `json:"links"`      // name -> item ID (int or string)
	Attrs       []interface{}          `json:"attrs"`
	Deprecation *RustdocDeprecation    `json:"deprecation"`
	Inner       map[string]interface{} `json:"inner"` // Dynamic based on item type
}

// RustdocSpan represents source code location
type RustdocSpan struct {
	Filename string `json:"filename"`
	Begin    [2]int `json:"begin"` // [line, column]
	End      [2]int `json:"end"`
}

// RustdocPath represents a path reference
type RustdocPath struct {
	CrateID int         `json:"crate_id"`
	Path    interface{} `json:"path"` // Can be string or []string depending on format version
	Kind    string      `json:"kind"`
}

// RustdocDeprecation represents deprecation info
type RustdocDeprecation struct {
	Since string `json:"since"`
	Note  string `json:"note"`
}

// ExternalCrate represents an external crate reference
type ExternalCrate struct {
	Name        string `json:"name"`
	HTMLRootURL string `json:"html_root_url"`
}

// RustdocModule represents a module item (extracted from inner)
type RustdocModule struct {
	IsCrate    bool          `json:"is_crate"`
	Items      []interface{} `json:"items"` // Can be int or string IDs
	IsStripped bool          `json:"is_stripped"`
}

// RustdocFunction represents a function/method (extracted from inner)
type RustdocFunction struct {
	Sig      *RustdocFunctionSig `json:"sig"`
	Generics *RustdocGenerics    `json:"generics"`
	Header   *RustdocHeader      `json:"header"`
	HasBody  bool                `json:"has_body"`
}

// RustdocFunctionSig represents a function signature
type RustdocFunctionSig struct {
	Inputs     []interface{} `json:"inputs"` // [[name, type], ...]
	Output     interface{}   `json:"output"` // nullable (void), type object
	IsVariadic bool          `json:"is_c_variadic"`
}

// RustdocGenerics represents generic parameters
type RustdocGenerics struct {
	Params          []RustdocGenericParam `json:"params"`
	WherePredicates []interface{}         `json:"where_predicates"`
}

// RustdocGenericParam represents a generic parameter
type RustdocGenericParam struct {
	Name string      `json:"name"`
	Kind interface{} `json:"kind"`
}

// RustdocHeader represents function header attributes
type RustdocHeader struct {
	IsConst  bool   `json:"is_const"`
	IsUnsafe bool   `json:"is_unsafe"`
	IsAsync  bool   `json:"is_async"`
	ABI      string `json:"abi"`
}

// RustdocTrait represents a trait (extracted from inner)
type RustdocTrait struct {
	IsAuto          bool             `json:"is_auto"`
	IsUnsafe        bool             `json:"is_unsafe"`
	IsDynCompatible bool             `json:"is_dyn_compatible"`
	Items           []interface{}    `json:"items"` // Can be int or string IDs
	Generics        *RustdocGenerics `json:"generics"`
	Bounds          []interface{}    `json:"bounds"`
	Implementations []interface{}    `json:"implementations"` // Can be int or string IDs
}

// RustdocStruct represents a struct (extracted from inner)
type RustdocStruct struct {
	Kind     interface{}      `json:"kind"` // "unit", "tuple", or struct fields
	Generics *RustdocGenerics `json:"generics"`
	Impls    []interface{}    `json:"impls"` // Can be int or string IDs
}

// RustdocEnum represents an enum (extracted from inner)
type RustdocEnum struct {
	Variants         []interface{}    `json:"variants"` // Can be int or string IDs
	Generics         *RustdocGenerics `json:"generics"`
	Impls            []interface{}    `json:"impls"` // Can be int or string IDs
	VariantsStripped bool             `json:"variants_stripped"`
}

// RustdocImpl represents an impl block (extracted from inner)
type RustdocImpl struct {
	IsUnsafe        bool             `json:"is_unsafe"`
	Generics        *RustdocGenerics `json:"generics"`
	ProvidedMethods []string         `json:"provided_trait_methods"`
	Trait           interface{}      `json:"trait"` // nullable, RustdocPath-like
	For             interface{}      `json:"for"`   // Type
	Items           []interface{}    `json:"items"` // Can be int or string IDs
	IsNegative      bool             `json:"is_negative"`
	IsSynthetic     bool             `json:"is_synthetic"`
	BlanketImpl     interface{}      `json:"blanket_impl"` // nullable, Type
}

// RustdocUse represents a re-export (use statement)
type RustdocUse struct {
	Source string      `json:"source"`
	Name   string      `json:"name"`
	ID     interface{} `json:"id"` // nullable if external
	IsGlob bool        `json:"is_glob"`
}

// RustdocTypeAlias represents a type alias (extracted from inner)
type RustdocTypeAlias struct {
	Type     interface{}      `json:"type"`
	Generics *RustdocGenerics `json:"generics"`
}

// RustdocConstant represents a constant (extracted from inner)
type RustdocConstant struct {
	Type   interface{} `json:"type"`
	Const_ interface{} `json:"const"` // The constant expression/value
}

// RustdocStatic represents a static variable (extracted from inner)
type RustdocStatic struct {
	Type      interface{} `json:"type"`
	IsMutable bool        `json:"mutable"`
	Expr      string      `json:"expr"`
}

// RustdocMacro represents a macro (extracted from inner)
type RustdocMacro struct {
	Macro string `json:"macro"`
}

// RustdocAssocType represents an associated type in a trait
type RustdocAssocType struct {
	Generics *RustdocGenerics `json:"generics"`
	Bounds   []interface{}    `json:"bounds"`
	Type     interface{}      `json:"type"` // Default type if any
}

// RustdocAssocConst represents an associated constant in a trait
type RustdocAssocConst struct {
	Type  interface{} `json:"type"`
	Value *string     `json:"value"` // Default value if any
}

// RustdocVariant represents an enum variant (extracted from inner)
type RustdocVariant struct {
	Kind         interface{} `json:"kind"` // "plain", "tuple", "struct"
	Discriminant interface{} `json:"discriminant"`
}

// Helper functions to extract typed inner values

// GetModule extracts module data from an item's inner field
func (item *RustdocItem) GetModule() *RustdocModule {
	if item.Inner == nil {
		return nil
	}
	if moduleData, ok := item.Inner["module"]; ok {
		return parseModule(moduleData)
	}
	return nil
}

// GetFunction extracts function data from an item's inner field
func (item *RustdocItem) GetFunction() *RustdocFunction {
	if item.Inner == nil {
		return nil
	}
	if fnData, ok := item.Inner["function"]; ok {
		return parseFunction(fnData)
	}
	return nil
}

// GetTrait extracts trait data from an item's inner field
func (item *RustdocItem) GetTrait() *RustdocTrait {
	if item.Inner == nil {
		return nil
	}
	if traitData, ok := item.Inner["trait"]; ok {
		return parseTrait(traitData)
	}
	return nil
}

// GetStruct extracts struct data from an item's inner field
func (item *RustdocItem) GetStruct() *RustdocStruct {
	if item.Inner == nil {
		return nil
	}
	if structData, ok := item.Inner["struct"]; ok {
		return parseStruct(structData)
	}
	return nil
}

// GetEnum extracts enum data from an item's inner field
func (item *RustdocItem) GetEnum() *RustdocEnum {
	if item.Inner == nil {
		return nil
	}
	if enumData, ok := item.Inner["enum"]; ok {
		return parseEnum(enumData)
	}
	return nil
}

// GetImpl extracts impl data from an item's inner field
func (item *RustdocItem) GetImpl() *RustdocImpl {
	if item.Inner == nil {
		return nil
	}
	if implData, ok := item.Inner["impl"]; ok {
		return parseImpl(implData)
	}
	return nil
}

// GetUse extracts use/re-export data from an item's inner field
func (item *RustdocItem) GetUse() *RustdocUse {
	if item.Inner == nil {
		return nil
	}
	if useData, ok := item.Inner["use"]; ok {
		return parseUse(useData)
	}
	return nil
}

// GetTypeAlias extracts type alias data from an item's inner field
func (item *RustdocItem) GetTypeAlias() *RustdocTypeAlias {
	if item.Inner == nil {
		return nil
	}
	if taData, ok := item.Inner["type_alias"]; ok {
		return parseTypeAlias(taData)
	}
	return nil
}

// GetConstant extracts constant data from an item's inner field
func (item *RustdocItem) GetConstant() *RustdocConstant {
	if item.Inner == nil {
		return nil
	}
	if constData, ok := item.Inner["constant"]; ok {
		return parseConstant(constData)
	}
	return nil
}

// GetMacro extracts macro data from an item's inner field
func (item *RustdocItem) GetMacro() *RustdocMacro {
	if item.Inner == nil {
		return nil
	}
	if macroData, ok := item.Inner["macro"]; ok {
		return parseMacro(macroData)
	}
	return nil
}

// GetVariant extracts variant data from an item's inner field
func (item *RustdocItem) GetVariant() *RustdocVariant {
	if item.Inner == nil {
		return nil
	}
	if variantData, ok := item.Inner["variant"]; ok {
		return parseVariant(variantData)
	}
	return nil
}

// IsPublic returns true if the item has public visibility
func (item *RustdocItem) IsPublic() bool {
	if item.Visibility == nil {
		return false
	}
	switch v := item.Visibility.(type) {
	case string:
		return v == "public"
	case map[string]interface{}:
		// Restricted visibility like pub(crate)
		return false
	default:
		return false
	}
}

// GetItemType returns the type of item based on the inner field
func (item *RustdocItem) GetItemType() string {
	if item.Inner == nil {
		return "unknown"
	}
	for key := range item.Inner {
		return key
	}
	return "unknown"
}

// Parser helper functions

func parseModule(data interface{}) *RustdocModule {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	mod := &RustdocModule{}
	if v, ok := m["is_crate"].(bool); ok {
		mod.IsCrate = v
	}
	if v, ok := m["items"].([]interface{}); ok {
		mod.Items = v
	}
	if v, ok := m["is_stripped"].(bool); ok {
		mod.IsStripped = v
	}
	return mod
}

func parseFunction(data interface{}) *RustdocFunction {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	fn := &RustdocFunction{}
	if sigData, ok := m["sig"].(map[string]interface{}); ok {
		fn.Sig = parseFunctionSig(sigData)
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		fn.Generics = parseGenerics(genData)
	}
	if headerData, ok := m["header"].(map[string]interface{}); ok {
		fn.Header = parseHeader(headerData)
	}
	if v, ok := m["has_body"].(bool); ok {
		fn.HasBody = v
	}
	return fn
}

func parseFunctionSig(data map[string]interface{}) *RustdocFunctionSig {
	sig := &RustdocFunctionSig{}
	if v, ok := data["inputs"].([]interface{}); ok {
		sig.Inputs = v
	}
	sig.Output = data["output"]
	if v, ok := data["is_c_variadic"].(bool); ok {
		sig.IsVariadic = v
	}
	return sig
}

func parseGenerics(data map[string]interface{}) *RustdocGenerics {
	gen := &RustdocGenerics{}
	if params, ok := data["params"].([]interface{}); ok {
		for _, p := range params {
			if pm, ok := p.(map[string]interface{}); ok {
				param := RustdocGenericParam{}
				if name, ok := pm["name"].(string); ok {
					param.Name = name
				}
				param.Kind = pm["kind"]
				gen.Params = append(gen.Params, param)
			}
		}
	}
	if wp, ok := data["where_predicates"].([]interface{}); ok {
		gen.WherePredicates = wp
	}
	return gen
}

func parseHeader(data map[string]interface{}) *RustdocHeader {
	header := &RustdocHeader{}
	if v, ok := data["is_const"].(bool); ok {
		header.IsConst = v
	}
	if v, ok := data["is_unsafe"].(bool); ok {
		header.IsUnsafe = v
	}
	if v, ok := data["is_async"].(bool); ok {
		header.IsAsync = v
	}
	if v, ok := data["abi"].(string); ok {
		header.ABI = v
	}
	return header
}

func parseTrait(data interface{}) *RustdocTrait {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	trait := &RustdocTrait{}
	if v, ok := m["is_auto"].(bool); ok {
		trait.IsAuto = v
	}
	if v, ok := m["is_unsafe"].(bool); ok {
		trait.IsUnsafe = v
	}
	if v, ok := m["is_dyn_compatible"].(bool); ok {
		trait.IsDynCompatible = v
	}
	if v, ok := m["items"].([]interface{}); ok {
		trait.Items = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		trait.Generics = parseGenerics(genData)
	}
	if v, ok := m["bounds"].([]interface{}); ok {
		trait.Bounds = v
	}
	if v, ok := m["implementations"].([]interface{}); ok {
		trait.Implementations = v
	}
	return trait
}

func parseStruct(data interface{}) *RustdocStruct {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	s := &RustdocStruct{}
	s.Kind = m["kind"]
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		s.Generics = parseGenerics(genData)
	}
	if v, ok := m["impls"].([]interface{}); ok {
		s.Impls = v
	}
	return s
}

func parseEnum(data interface{}) *RustdocEnum {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	e := &RustdocEnum{}
	if v, ok := m["variants"].([]interface{}); ok {
		e.Variants = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		e.Generics = parseGenerics(genData)
	}
	if v, ok := m["impls"].([]interface{}); ok {
		e.Impls = v
	}
	if v, ok := m["variants_stripped"].(bool); ok {
		e.VariantsStripped = v
	}
	return e
}

func parseImpl(data interface{}) *RustdocImpl {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	impl := &RustdocImpl{}
	if v, ok := m["is_unsafe"].(bool); ok {
		impl.IsUnsafe = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		impl.Generics = parseGenerics(genData)
	}
	if v, ok := m["provided_trait_methods"].([]interface{}); ok {
		for _, s := range v {
			if str, ok := s.(string); ok {
				impl.ProvidedMethods = append(impl.ProvidedMethods, str)
			}
		}
	}
	impl.Trait = m["trait"]
	impl.For = m["for"]
	if v, ok := m["items"].([]interface{}); ok {
		impl.Items = v
	}
	if v, ok := m["is_negative"].(bool); ok {
		impl.IsNegative = v
	}
	if v, ok := m["is_synthetic"].(bool); ok {
		impl.IsSynthetic = v
	}
	impl.BlanketImpl = m["blanket_impl"]
	return impl
}

func parseUse(data interface{}) *RustdocUse {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	use := &RustdocUse{}
	if v, ok := m["source"].(string); ok {
		use.Source = v
	}
	if v, ok := m["name"].(string); ok {
		use.Name = v
	}
	use.ID = m["id"]
	if v, ok := m["is_glob"].(bool); ok {
		use.IsGlob = v
	}
	return use
}

func parseTypeAlias(data interface{}) *RustdocTypeAlias {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	ta := &RustdocTypeAlias{}
	ta.Type = m["type"]
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		ta.Generics = parseGenerics(genData)
	}
	return ta
}

func parseConstant(data interface{}) *RustdocConstant {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	c := &RustdocConstant{}
	c.Type = m["type"]
	c.Const_ = m["const"]
	return c
}

func parseMacro(data interface{}) *RustdocMacro {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	macro := &RustdocMacro{}
	if v, ok := m["macro"].(string); ok {
		macro.Macro = v
	}
	return macro
}

func parseVariant(data interface{}) *RustdocVariant {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	v := &RustdocVariant{}
	v.Kind = m["kind"]
	v.Discriminant = m["discriminant"]
	return v
}
</file>
<file path="internal/strategies/git_compat.go">
package strategies

import (
	"context"
	"io"
	"net/http"

	"github.com/quantmind-br/repodocs-go/internal/strategies/git"
)

var DocumentExtensions = git.DocumentExtensions
var IgnoreDirs = git.IgnoreDirs

type repoInfo = git.RepoInfo

type gitURLInfo = git.GitURLInfo

type GitStrategy struct {
	strategy       *git.Strategy
	deps           *Dependencies
	parser         *git.Parser
	archiveFetcher *git.ArchiveFetcher
	processor      *git.Processor
	httpClient     *http.Client
}

func NewGitStrategy(deps *Dependencies) *GitStrategy {
	var gitDeps *git.StrategyDependencies
	var httpClient *http.Client

	if deps != nil {
		gitDeps = &git.StrategyDependencies{
			Writer:       deps.Writer,
			Logger:       deps.Logger,
			HTTPClient:   deps.HTTPClient,
			WriteFunc:    deps.WriteDocument,
			StateManager: deps.StateManager,
		}
		httpClient = deps.HTTPClient
	}

	if httpClient == nil {
		httpClient = http.DefaultClient
	}

	var logger = deps.Logger
	if deps == nil {
		logger = nil
	}

	return &GitStrategy{
		strategy: git.NewStrategy(gitDeps),
		deps:     deps,
		parser:   git.NewParser(),
		archiveFetcher: git.NewArchiveFetcher(git.ArchiveFetcherOptions{
			HTTPClient: httpClient,
			Logger:     logger,
		}),
		processor: git.NewProcessor(git.ProcessorOptions{
			Logger: logger,
		}),
		httpClient: httpClient,
	}
}

func (s *GitStrategy) Name() string {
	return s.strategy.Name()
}

func (s *GitStrategy) CanHandle(url string) bool {
	return s.strategy.CanHandle(url)
}

func (s *GitStrategy) Execute(ctx context.Context, rawURL string, opts Options) error {
	gitOpts := git.ExecuteOptions{
		Output:      opts.Output,
		Concurrency: opts.Concurrency,
		Limit:       opts.Limit,
		DryRun:      opts.DryRun,
		FilterURL:   opts.FilterURL,
	}
	return s.strategy.Execute(ctx, rawURL, gitOpts)
}

func (s *GitStrategy) detectDefaultBranch(ctx context.Context, url string) (string, error) {
	return git.DetectDefaultBranch(ctx, url)
}

func (s *GitStrategy) buildArchiveURL(info *repoInfo, branch string) string {
	return s.archiveFetcher.BuildArchiveURL(info, branch)
}

func (s *GitStrategy) downloadAndExtract(ctx context.Context, archiveURL, destDir string) error {
	return s.archiveFetcher.DownloadAndExtract(ctx, archiveURL, destDir)
}

func (s *GitStrategy) extractTarGz(r io.Reader, destDir string) error {
	return s.archiveFetcher.ExtractTarGz(r, destDir)
}

func (s *GitStrategy) findDocumentationFiles(dir string, filterPath string) ([]string, error) {
	return s.processor.FindDocumentationFiles(dir, filterPath)
}

func (s *GitStrategy) processFiles(ctx context.Context, files []string, tmpDir, repoURL, branch string, opts Options) error {
	processOpts := git.ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		Concurrency:  opts.Concurrency,
		Limit:        opts.Limit,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteDocument,
		StateManager: s.deps.StateManager,
	}
	return s.processor.ProcessFiles(ctx, files, tmpDir, processOpts)
}

func (s *GitStrategy) processFile(ctx context.Context, path, tmpDir, repoURL, branch string, opts Options) error {
	processOpts := git.ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteDocument,
		StateManager: s.deps.StateManager,
	}
	return s.processor.ProcessFile(ctx, path, tmpDir, processOpts)
}

func (s *GitStrategy) parseGitURLWithPath(rawURL string) (*gitURLInfo, error) {
	return s.parser.ParseURLWithPath(rawURL)
}

func (s *GitStrategy) tryArchiveDownload(ctx context.Context, url, destDir string) (branch, method string, err error) {
	return s.strategy.TryArchiveDownload(ctx, url, destDir)
}

func (s *GitStrategy) cloneRepository(ctx context.Context, url, destDir string) (string, error) {
	return s.strategy.CloneRepository(ctx, url, destDir)
}

func normalizeFilterPath(path string) string {
	return git.NormalizeFilterPath(path)
}

func extractTitleFromPath(path string) string {
	return git.ExtractTitleFromPath(path)
}
</file>
<file path="internal/strategies/github_pages.go">
package strategies

import (
	"context"
	"fmt"
	"net/url"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// GitHubPagesStrategy extracts documentation from GitHub Pages sites (*.github.io)
type GitHubPagesStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// NewGitHubPagesStrategy creates a new GitHub Pages strategy
func NewGitHubPagesStrategy(deps *Dependencies) *GitHubPagesStrategy {
	if deps == nil {
		return &GitHubPagesStrategy{
			markdownReader: converter.NewMarkdownReader(),
		}
	}
	return &GitHubPagesStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *GitHubPagesStrategy) Name() string {
	return "github_pages"
}

// CanHandle returns true if URL is a GitHub Pages site
func (s *GitHubPagesStrategy) CanHandle(rawURL string) bool {
	return IsGitHubPagesURL(rawURL)
}

// IsGitHubPagesURL checks if a URL is a GitHub Pages site
func IsGitHubPagesURL(rawURL string) bool {
	parsed, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	host := strings.ToLower(parsed.Host)
	return strings.HasSuffix(host, ".github.io")
}

// Execute runs the GitHub Pages extraction strategy
func (s *GitHubPagesStrategy) Execute(ctx context.Context, inputURL string, opts Options) error {
	s.logger.Info().
		Str("url", inputURL).
		Msg("Starting GitHub Pages extraction")

	// Normalize base URL
	baseURL, err := s.normalizeBaseURL(inputURL)
	if err != nil {
		return fmt.Errorf("invalid URL: %w", err)
	}

	// Phase 1: Discover URLs (HTTP-first, browser fallback)
	urls, discoveryMethod, err := s.discoverURLs(ctx, baseURL, opts)
	if err != nil {
		return fmt.Errorf("URL discovery failed: %w", err)
	}

	if len(urls) == 0 {
		s.logger.Warn().Msg("No URLs discovered")
		return nil
	}

	s.logger.Info().
		Int("count", len(urls)).
		Str("method", discoveryMethod).
		Msg("URLs discovered")

	// Filter and deduplicate URLs
	urls = FilterAndDeduplicateURLs(urls, baseURL)

	// Apply filters and limits
	urls = s.filterURLs(urls, baseURL, opts)
	if opts.Limit > 0 && len(urls) > opts.Limit {
		urls = urls[:opts.Limit]
	}

	s.logger.Info().
		Int("count", len(urls)).
		Msg("Processing URLs")

	// Phase 2: Extract content (HTTP-first, browser fallback)
	return s.processURLs(ctx, urls, opts)
}

// discoverURLs finds all URLs using multi-tier discovery
func (s *GitHubPagesStrategy) discoverURLs(ctx context.Context, baseURL string, opts Options) ([]string, string, error) {
	// Tier 1: Try HTTP probes sequentially
	urls, method, err := s.discoverViaHTTPProbes(ctx, baseURL)
	if err == nil && len(urls) > 0 {
		return urls, method, nil
	}

	s.logger.Debug().Err(err).Msg("HTTP discovery failed, falling back to browser crawl")

	renderer, rendererErr := s.deps.GetRenderer()
	if rendererErr != nil {
		return nil, "", fmt.Errorf("no URLs found via HTTP probes and browser renderer failed: %w", rendererErr)
	}
	s.renderer = renderer

	urls, err = s.discoverViaBrowser(ctx, baseURL, opts)
	if err != nil {
		return nil, "", fmt.Errorf("browser discovery failed: %w", err)
	}

	return urls, "browser-crawl", nil
}

// discoverViaHTTPProbes tries all HTTP-based discovery methods
func (s *GitHubPagesStrategy) discoverViaHTTPProbes(ctx context.Context, baseURL string) ([]string, string, error) {
	probes := GetDiscoveryProbes()

	for _, probe := range probes {
		select {
		case <-ctx.Done():
			return nil, "", ctx.Err()
		default:
		}

		probeURL := strings.TrimSuffix(baseURL, "/") + probe.Path

		resp, err := s.fetcher.Get(ctx, probeURL)
		if err != nil {
			s.logger.Debug().Str("probe", probe.Name).Str("url", probeURL).Err(err).Msg("Probe failed")
			continue
		}

		if resp.StatusCode != 200 {
			s.logger.Debug().Str("probe", probe.Name).Int("status", resp.StatusCode).Msg("Probe returned non-200")
			continue
		}

		urls, err := probe.Parser(resp.Body, baseURL)
		if err != nil {
			s.logger.Debug().Str("probe", probe.Name).Err(err).Msg("Failed to parse probe response")
			continue
		}

		if len(urls) > 0 {
			s.logger.Info().
				Str("probe", probe.Name).
				Int("urls", len(urls)).
				Msg("Discovery probe succeeded")
			return urls, probe.Name, nil
		}
	}

	return nil, "", fmt.Errorf("all HTTP probes failed")
}

// discoverViaBrowser uses browser rendering to crawl and discover URLs
func (s *GitHubPagesStrategy) discoverViaBrowser(ctx context.Context, baseURL string, opts Options) ([]string, error) {
	visited := make(map[string]bool)
	toVisit := []string{baseURL}
	var discovered []string

	maxDepth := opts.MaxDepth
	if maxDepth <= 0 {
		maxDepth = 3
	}

	for depth := 0; depth < maxDepth && len(toVisit) > 0; depth++ {
		var nextLevel []string

		for _, pageURL := range toVisit {
			if visited[pageURL] {
				continue
			}
			visited[pageURL] = true
			discovered = append(discovered, pageURL)

			// Check limit during discovery
			if opts.Limit > 0 && len(discovered) >= opts.Limit*2 {
				return discovered, nil
			}

			// Render page and extract links
			links, err := s.extractLinksFromRenderedPage(ctx, pageURL, baseURL)
			if err != nil {
				s.logger.Debug().Err(err).Str("url", pageURL).Msg("Failed to extract links")
				continue
			}

			for _, link := range links {
				if !visited[link] {
					nextLevel = append(nextLevel, link)
				}
			}
		}

		toVisit = nextLevel
		s.logger.Debug().Int("depth", depth+1).Int("queued", len(toVisit)).Msg("Browser crawl depth completed")
	}

	return discovered, nil
}

// extractLinksFromRenderedPage renders a page and extracts internal links
func (s *GitHubPagesStrategy) extractLinksFromRenderedPage(ctx context.Context, pageURL, baseURL string) ([]string, error) {
	html, err := s.renderPage(ctx, pageURL)
	if err != nil {
		return nil, err
	}

	return s.extractLinksWithGoquery(html, baseURL)
}

// extractLinksWithGoquery uses goquery for robust HTML parsing
func (s *GitHubPagesStrategy) extractLinksWithGoquery(html, baseURL string) ([]string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return nil, err
	}

	parsedBase, _ := url.Parse(baseURL)
	baseHost := parsedBase.Host

	seen := make(map[string]bool)
	var links []string

	// Extract from navigation elements first (most relevant)
	selectors := []string{
		"nav a[href]",
		"[role='navigation'] a[href]",
		".sidebar a[href]",
		".menu a[href]",
		".nav a[href]",
		".toc a[href]",
		"a[href]",
	}

	for _, selector := range selectors {
		doc.Find(selector).Each(func(_ int, sel *goquery.Selection) {
			href, exists := sel.Attr("href")
			if !exists || href == "" {
				return
			}

			// Skip non-navigable links
			if strings.HasPrefix(href, "#") ||
				strings.HasPrefix(href, "javascript:") ||
				strings.HasPrefix(href, "mailto:") ||
				strings.HasPrefix(href, "tel:") {
				return
			}

			// Resolve relative URLs
			resolved, err := url.Parse(href)
			if err != nil {
				return
			}

			if !resolved.IsAbs() {
				resolved = parsedBase.ResolveReference(resolved)
			}

			// Filter to same host
			if resolved.Host != baseHost {
				return
			}

			// Normalize: remove fragment, trailing slash
			resolved.Fragment = ""
			normalized := resolved.String()
			normalized = strings.TrimSuffix(normalized, "/")

			if !seen[normalized] && !ShouldSkipGitHubPagesURL(normalized) {
				seen[normalized] = true
				links = append(links, normalized)
			}
		})

		// If we found links in nav elements, prefer those
		if len(links) > 10 && selector != "a[href]" {
			break
		}
	}

	return links, nil
}

// renderPage renders a page using the browser
func (s *GitHubPagesStrategy) renderPage(ctx context.Context, pageURL string) (string, error) {
	return s.renderer.Render(ctx, pageURL, domain.RenderOptions{
		Timeout:     90 * time.Second,
		WaitStable:  3 * time.Second,
		ScrollToEnd: true,
	})
}

// filterURLs applies filter and exclude patterns
func (s *GitHubPagesStrategy) filterURLs(urls []string, baseURL string, opts Options) []string {
	var excludeRegexps []*regexp.Regexp
	for _, pattern := range opts.Exclude {
		if re, err := regexp.Compile(pattern); err == nil {
			excludeRegexps = append(excludeRegexps, re)
		}
	}

	var filtered []string
	for _, u := range urls {
		// Apply base URL filter
		if opts.FilterURL != "" && !strings.HasPrefix(u, opts.FilterURL) {
			continue
		}

		// Apply exclude patterns
		excluded := false
		for _, re := range excludeRegexps {
			if re.MatchString(u) {
				excluded = true
				break
			}
		}
		if excluded {
			continue
		}

		// Skip non-content URLs
		if ShouldSkipGitHubPagesURL(u) {
			continue
		}

		filtered = append(filtered, u)
	}

	return filtered
}

// processURLs processes all URLs using HTTP-first extraction with browser fallback
func (s *GitHubPagesStrategy) processURLs(ctx context.Context, urls []string, opts Options) error {
	bar := utils.NewProgressBar(len(urls), utils.DescExtracting)

	// Limit browser concurrency for stability
	concurrency := opts.Concurrency
	if concurrency > 5 {
		concurrency = 5
	}
	if concurrency <= 0 {
		concurrency = 2
	}

	var mu sync.Mutex
	var processedCount, successCount int

	errors := utils.ParallelForEach(ctx, urls, concurrency, func(ctx context.Context, pageURL string) error {
		defer func() {
			mu.Lock()
			bar.Add(1)
			processedCount++
			mu.Unlock()
		}()

		// Check if already exists
		if !opts.Force && s.writer.Exists(pageURL) {
			return nil
		}

		// HTTP-first fetch with browser fallback
		html, usedBrowser, err := s.fetchOrRenderPage(ctx, pageURL, opts)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to fetch/render page")
			return nil
		}

		// Validate content
		if s.isEmptyOrErrorContent(html) {
			s.logger.Debug().Str("url", pageURL).Msg("Empty or error content, skipping")
			return nil
		}

		// Convert HTML to document
		doc, err := s.converter.Convert(ctx, html, pageURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to convert page")
			return nil
		}

		// Validate converted content
		if len(strings.TrimSpace(doc.Content)) < 50 {
			s.logger.Debug().Str("url", pageURL).Msg("Converted content too short, skipping")
			return nil
		}

		// Set metadata
		doc.SourceStrategy = s.Name()
		doc.FetchedAt = time.Now()
		doc.RenderedWithJS = usedBrowser

		// Write document
		if !opts.DryRun {
			if err := s.deps.WriteDocument(ctx, doc); err != nil {
				s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to write document")
				return nil
			}
			mu.Lock()
			successCount++
			mu.Unlock()
		}

		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().
		Int("processed", processedCount).
		Int("success", successCount).
		Int("total", len(urls)).
		Msg("GitHub Pages extraction completed")

	return nil
}

// fetchOrRenderPage attempts HTTP fetch first, falls back to browser rendering if needed
func (s *GitHubPagesStrategy) fetchOrRenderPage(ctx context.Context, pageURL string, opts Options) (string, bool, error) {
	// Try HTTP fetch first (unless RenderJS is forced)
	if !opts.RenderJS {
		resp, err := s.fetcher.Get(ctx, pageURL)
		if err == nil && resp.StatusCode == 200 {
			html := string(resp.Body)

			// Check if content looks like a valid page (not SPA shell)
			if !s.looksLikeSPAShell(html) && !renderer.NeedsJSRendering(html) {
				return html, false, nil
			}

			s.logger.Debug().Str("url", pageURL).Msg("Content appears to be SPA shell, using browser")
		}
	}

	r, err := s.deps.GetRenderer()
	if err != nil {
		return "", false, fmt.Errorf("browser renderer not available: %w", err)
	}
	s.renderer = r

	rendered, err := s.renderPage(ctx, pageURL)
	if err != nil {
		return "", false, err
	}

	return rendered, true, nil
}

// looksLikeSPAShell checks if HTML looks like an empty SPA shell
func (s *GitHubPagesStrategy) looksLikeSPAShell(html string) bool {
	// Check for minimal content indicators
	if len(html) < 500 {
		return true
	}

	lower := strings.ToLower(html)

	// Check for empty body or just script tags
	spaIndicators := []string{
		`<div id="app"></div>`,
		`<div id="root"></div>`,
		`<div id="__next"></div>`,
		`<div id="__nuxt"></div>`,
		"<body></body>",
		"<body> </body>",
	}

	for _, indicator := range spaIndicators {
		if strings.Contains(lower, indicator) {
			// Check if there's actual content after the indicator
			doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
			if err != nil {
				return true
			}

			bodyText := strings.TrimSpace(doc.Find("body").Text())
			if len(bodyText) < 100 {
				return true
			}
		}
	}

	return false
}

// isEmptyOrErrorContent checks if rendered content is empty or an error page
func (s *GitHubPagesStrategy) isEmptyOrErrorContent(html string) bool {
	if len(html) < 300 {
		return true
	}

	lower := strings.ToLower(html)
	errorIndicators := []string{
		"301 moved permanently",
		"302 found",
		"404 not found",
		"page not found",
		"access denied",
		"403 forbidden",
	}

	for _, indicator := range errorIndicators {
		if strings.Contains(lower, indicator) {
			return true
		}
	}

	// Check for minimal content (just boilerplate)
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return true
	}

	// Check if body has meaningful text content
	bodyText := strings.TrimSpace(doc.Find("body").Text())
	if len(bodyText) < 60 {
		return true
	}

	return false
}

// normalizeBaseURL normalizes the input URL to a base URL
func (s *GitHubPagesStrategy) normalizeBaseURL(inputURL string) (string, error) {
	parsed, err := url.Parse(inputURL)
	if err != nil {
		return "", err
	}

	if parsed.Scheme == "" {
		parsed.Scheme = "https"
	}

	// Keep the path for project subpaths (e.g., /goose/)
	base := fmt.Sprintf("%s://%s", parsed.Scheme, parsed.Host)

	if parsed.Path != "" && parsed.Path != "/" {
		// Clean path: remove trailing slash, keep structure
		path := strings.TrimSuffix(parsed.Path, "/")
		base += path
	}

	return base, nil
}
</file>
<file path="internal/strategies/github_pages_discovery.go">
package strategies

import (
	"encoding/json"
	"encoding/xml"
	"fmt"
	"net/url"
	"strings"
)

// SitemapXMLForDiscovery represents the XML structure of a sitemap (for discovery)
type SitemapXMLForDiscovery struct {
	XMLName xml.Name                 `xml:"urlset"`
	URLs    []SitemapURLForDiscovery `xml:"url"`
}

// SitemapURLForDiscovery represents a URL entry in a sitemap
type SitemapURLForDiscovery struct {
	Loc string `xml:"loc"`
}

// SitemapIndexXMLForDiscovery represents the XML structure of a sitemap index
type SitemapIndexXMLForDiscovery struct {
	XMLName  xml.Name                      `xml:"sitemapindex"`
	Sitemaps []SitemapLocationForDiscovery `xml:"sitemap"`
}

// SitemapLocationForDiscovery represents a sitemap location in an index
type SitemapLocationForDiscovery struct {
	Loc string `xml:"loc"`
}

// DiscoveryProbe defines a URL discovery mechanism for GitHub Pages sites
type DiscoveryProbe struct {
	Path   string
	Parser func(content []byte, baseURL string) ([]string, error)
	Name   string
}

// GetDiscoveryProbes returns all discovery probes in priority order
func GetDiscoveryProbes() []DiscoveryProbe {
	return []DiscoveryProbe{
		// Tier 1: LLM-optimized (highest quality)
		{"/llms.txt", ParseLLMsTxt, "llms.txt"},

		// Tier 2: Sitemaps (most common)
		{"/sitemap.xml", ParseSitemapXML, "sitemap.xml"},
		{"/sitemap-0.xml", ParseSitemapXML, "sitemap-0.xml"},
		{"/sitemap_index.xml", ParseSitemapIndexXML, "sitemap_index.xml"},

		// Tier 3: MkDocs (very reliable)
		{"/search/search_index.json", ParseMkDocsIndex, "mkdocs-search"},

		// Tier 4: Docusaurus
		{"/search-index.json", ParseDocusaurusIndex, "docusaurus-search"},

		// Tier 5: Hugo / Generic
		{"/index.json", ParseHugoIndex, "hugo-index"},
		{"/search.json", ParseGenericSearchIndex, "search.json"},

		// Tier 6: Modern SSGs
		{"/hashmap.json", ParseVitePressHashmap, "vitepress"},
	}
}

// ParseLLMsTxt parses llms.txt format (markdown links)
func ParseLLMsTxt(content []byte, baseURL string) ([]string, error) {
	// Use the parseLLMSLinks function from llms.go (same package)
	links := parseLLMSLinks(string(content), baseURL)
	if len(links) == 0 {
		return nil, fmt.Errorf("no links found in llms.txt")
	}

	urls := make([]string, 0, len(links))
	for _, link := range links {
		urls = append(urls, link.URL)
	}
	return urls, nil
}

// FilterAndDeduplicateURLs filters URLs to the same host and deduplicates them
func FilterAndDeduplicateURLs(urls []string, baseURL string) []string {
	parsed, err := url.Parse(baseURL)
	if err != nil {
		return urls
	}
	baseHost := parsed.Host

	seen := make(map[string]bool)
	var result []string

	for _, u := range urls {
		parsedURL, err := url.Parse(u)
		if err != nil {
			continue
		}

		// Filter to same host
		if parsedURL.Host != "" && parsedURL.Host != baseHost {
			continue
		}

		// Normalize: remove fragment, trailing slash
		parsedURL.Fragment = ""
		normalized := parsedURL.String()
		normalized = strings.TrimSuffix(normalized, "/")

		if !seen[normalized] {
			seen[normalized] = true
			result = append(result, normalized)
		}
	}

	return result
}

// ShouldSkipGitHubPagesURL returns true for URLs that typically don't contain documentation
func ShouldSkipGitHubPagesURL(u string) bool {
	lower := strings.ToLower(u)
	skipPatterns := []string{
		"/assets/", "/static/", "/_next/", "/_nuxt/",
		"/img/", "/images/", "/media/",
		"/css/", "/js/", "/fonts/",
		".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp",
		".css", ".js", ".woff", ".woff2", ".ttf", ".eot",
		".pdf", ".zip", ".tar", ".gz",
		"/feed.xml", "/rss.xml", "/atom.xml",
	}

	for _, pattern := range skipPatterns {
		if strings.Contains(lower, pattern) {
			return true
		}
	}
	return false
}

// ParseSitemapXML parses standard sitemap.xml format
func ParseSitemapXML(content []byte, baseURL string) ([]string, error) {
	var sitemap SitemapXMLForDiscovery
	if err := xml.Unmarshal(content, &sitemap); err != nil {
		return nil, err
	}

	if len(sitemap.URLs) == 0 {
		return nil, fmt.Errorf("empty sitemap")
	}

	urls := make([]string, 0, len(sitemap.URLs))
	for _, u := range sitemap.URLs {
		if u.Loc != "" {
			urls = append(urls, u.Loc)
		}
	}
	return urls, nil
}

// ParseSitemapIndexXML parses sitemap index and fetches nested sitemaps
func ParseSitemapIndexXML(content []byte, baseURL string) ([]string, error) {
	var index SitemapIndexXMLForDiscovery
	if err := xml.Unmarshal(content, &index); err != nil {
		return nil, err
	}

	if len(index.Sitemaps) == 0 {
		return nil, fmt.Errorf("empty sitemap index")
	}

	// Return sitemap URLs for the caller to process
	urls := make([]string, 0, len(index.Sitemaps))
	for _, sm := range index.Sitemaps {
		urls = append(urls, sm.Loc)
	}
	return urls, nil
}

// MkDocsSearchIndex represents MkDocs search_index.json structure
type MkDocsSearchIndex struct {
	Docs []struct {
		Location string `json:"location"`
		Title    string `json:"title"`
		Text     string `json:"text"`
	} `json:"docs"`
}

// ParseMkDocsIndex parses MkDocs /search/search_index.json
func ParseMkDocsIndex(content []byte, baseURL string) ([]string, error) {
	var index MkDocsSearchIndex
	if err := json.Unmarshal(content, &index); err != nil {
		return nil, err
	}

	if len(index.Docs) == 0 {
		return nil, fmt.Errorf("empty MkDocs index")
	}

	seen := make(map[string]bool)
	var urls []string

	for _, doc := range index.Docs {
		loc := strings.Split(doc.Location, "#")[0]
		if loc == "" || loc == "." {
			loc = ""
		}

		fullURL := strings.TrimSuffix(baseURL, "/") + "/" + strings.TrimPrefix(loc, "/")

		if !seen[fullURL] {
			seen[fullURL] = true
			urls = append(urls, fullURL)
		}
	}

	return urls, nil
}

// DocusaurusSearchEntry represents a Docusaurus search index entry
type DocusaurusSearchEntry struct {
	URL     string `json:"url"`
	Title   string `json:"title"`
	Content string `json:"content"`
}

// ParseDocusaurusIndex parses Docusaurus /search-index.json
func ParseDocusaurusIndex(content []byte, baseURL string) ([]string, error) {
	var entries []DocusaurusSearchEntry
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty Docusaurus index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		if entry.URL != "" {
			urls = append(urls, resolveDiscoveryURL(entry.URL, baseURL))
		}
	}
	return urls, nil
}

// HugoSearchEntry represents a Hugo search index entry
type HugoSearchEntry struct {
	Permalink string `json:"permalink"`
	URL       string `json:"url"`
	Title     string `json:"title"`
}

// ParseHugoIndex parses Hugo /index.json
func ParseHugoIndex(content []byte, baseURL string) ([]string, error) {
	var entries []HugoSearchEntry
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty Hugo index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		urlStr := entry.Permalink
		if urlStr == "" {
			urlStr = entry.URL
		}
		if urlStr != "" {
			urls = append(urls, resolveDiscoveryURL(urlStr, baseURL))
		}
	}
	return urls, nil
}

// ParseGenericSearchIndex parses generic search.json format
func ParseGenericSearchIndex(content []byte, baseURL string) ([]string, error) {
	// Try array format first
	var entries []map[string]interface{}
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty search index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		// Try common URL field names
		for _, field := range []string{"url", "permalink", "href", "location", "path"} {
			if val, ok := entry[field].(string); ok && val != "" {
				urls = append(urls, resolveDiscoveryURL(val, baseURL))
				break
			}
		}
	}

	if len(urls) == 0 {
		return nil, fmt.Errorf("no URLs found in search index")
	}
	return urls, nil
}

// ParseVitePressHashmap parses VitePress hashmap.json
func ParseVitePressHashmap(content []byte, baseURL string) ([]string, error) {
	var hashmap map[string]string
	if err := json.Unmarshal(content, &hashmap); err != nil {
		return nil, err
	}

	if len(hashmap) == 0 {
		return nil, fmt.Errorf("empty VitePress hashmap")
	}

	urls := make([]string, 0, len(hashmap))
	for path := range hashmap {
		// VitePress hashmap keys are like "guide_getting-started.md"
		// Convert to URL path: /guide/getting-started
		urlPath := strings.ReplaceAll(path, "_", "/")
		urlPath = strings.TrimSuffix(urlPath, ".md")

		fullURL := strings.TrimSuffix(baseURL, "/") + "/" + urlPath
		urls = append(urls, fullURL)
	}
	return urls, nil
}

// resolveDiscoveryURL resolves a potentially relative URL against a base URL
func resolveDiscoveryURL(href, baseURL string) string {
	if strings.HasPrefix(href, "http://") || strings.HasPrefix(href, "https://") {
		return href
	}

	parsed, err := url.Parse(baseURL)
	if err != nil {
		return baseURL + "/" + strings.TrimPrefix(href, "/")
	}

	ref, err := url.Parse(href)
	if err != nil {
		return baseURL + "/" + strings.TrimPrefix(href, "/")
	}

	return parsed.ResolveReference(ref).String()
}
</file>
<file path="internal/strategies/llms.go">
package strategies

import (
	"context"
	"fmt"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type LLMSStrategy struct {
	deps            *Dependencies
	fetcher         domain.Fetcher
	converter       *converter.Pipeline
	markdownReader  *converter.MarkdownReader
	plainTextReader *converter.PlainTextReader
	writer          *output.Writer
	logger          *utils.Logger
}

func NewLLMSStrategy(deps *Dependencies) *LLMSStrategy {
	if deps == nil {
		return &LLMSStrategy{
			markdownReader:  converter.NewMarkdownReader(),
			plainTextReader: converter.NewPlainTextReader(),
		}
	}
	return &LLMSStrategy{
		deps:            deps,
		fetcher:         deps.Fetcher,
		converter:       deps.Converter,
		markdownReader:  converter.NewMarkdownReader(),
		plainTextReader: converter.NewPlainTextReader(),
		writer:          deps.Writer,
		logger:          deps.Logger,
	}
}

// Name returns the strategy name
func (s *LLMSStrategy) Name() string {
	return "llms"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *LLMSStrategy) CanHandle(url string) bool {
	// Only handle HTTP/HTTPS URLs
	if !strings.HasPrefix(url, "http://") && !strings.HasPrefix(url, "https://") {
		return false
	}

	lowerURL := strings.ToLower(url)
	return strings.HasSuffix(lowerURL, "/llms.txt") || strings.HasSuffix(lowerURL, "llms.txt")
}

// Execute runs the LLMS extraction strategy
func (s *LLMSStrategy) Execute(ctx context.Context, url string, opts Options) error {
	// Check context cancellation early
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	if s.fetcher == nil {
		return fmt.Errorf("llms strategy fetcher is nil")
	}
	if s.converter == nil {
		return fmt.Errorf("llms strategy converter is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("llms strategy writer is nil")
	}
	if s.logger == nil {
		return fmt.Errorf("llms strategy logger is nil")
	}

	s.logger.Info().Str("url", url).Msg("Fetching llms.txt")

	if opts.FilterURL != "" {
		s.logger.Info().Str("filter", opts.FilterURL).Msg("URL filter active - only downloading URLs under this path")
	}

	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	links := parseLLMSLinks(string(resp.Body), url)
	s.logger.Info().Int("count", len(links)).Msg("Found links in llms.txt")

	if opts.FilterURL != "" {
		links = filterLLMSLinks(links, opts.FilterURL)
		s.logger.Info().Int("count", len(links)).Str("filter", opts.FilterURL).Msg("Links after filter")
	}

	if opts.Limit > 0 && len(links) > opts.Limit {
		links = links[:opts.Limit]
	}

	// Create progress bar
	bar := utils.NewProgressBar(len(links), utils.DescExtracting)

	// Process links concurrently
	errors := utils.ParallelForEach(ctx, links, opts.Concurrency, func(ctx context.Context, link domain.LLMSLink) error {
		defer bar.Add(1)

		// Check if already exists
		if !opts.Force && s.writer.Exists(link.URL) {
			return nil
		}

		// Fetch page
		pageResp, err := s.fetcher.Get(ctx, link.URL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to fetch page")
			return nil // Continue with other pages
		}

		var doc *domain.Document
		if converter.IsMarkdownContent(pageResp.ContentType, link.URL) {
			doc, err = s.markdownReader.Read(string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to read markdown")
				return nil
			}
		} else if converter.IsPlainTextContent(pageResp.ContentType, link.URL) {
			doc, err = s.plainTextReader.Read(string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to read plain text")
				return nil
			}
		} else {
			doc, err = s.converter.Convert(ctx, string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to convert page")
				return nil
			}
		}

		// Set metadata
		doc.SourceStrategy = s.Name()
		doc.CacheHit = pageResp.FromCache
		doc.FetchedAt = time.Now()

		// Use title from llms.txt if document title is empty
		if doc.Title == "" && link.Title != "" {
			doc.Title = link.Title
		}

		if !opts.DryRun {
			if s.deps != nil {
				if err := s.deps.WriteDocument(ctx, doc); err != nil {
					s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to write document")
					return nil
				}
			} else {
				if err := s.writer.Write(ctx, doc); err != nil {
					s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to write document")
					return nil
				}
			}
		}

		return nil
	})

	// Check for errors
	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Msg("LLMS extraction completed")
	return nil
}

// linkRegex matches markdown links: [Title](url)
var linkRegex = regexp.MustCompile(`\[([^\]]+)\]\(([^)]+)\)`)

func parseLLMSLinks(content string, baseURL string) []domain.LLMSLink {
	links := make([]domain.LLMSLink, 0)

	matches := linkRegex.FindAllStringSubmatch(content, -1)
	for _, match := range matches {
		if len(match) == 3 {
			title := strings.TrimSpace(match[1])
			rawURL := strings.TrimSpace(match[2])

			if rawURL == "" || strings.HasPrefix(rawURL, "#") {
				continue
			}

			// Resolve relative URLs against base URL
			resolvedURL := rawURL
			if !strings.HasPrefix(rawURL, "http://") && !strings.HasPrefix(rawURL, "https://") {
				resolved, err := utils.ResolveURL(baseURL, rawURL)
				if err != nil {
					// Skip URLs that can't be resolved
					continue
				}
				resolvedURL = resolved
			}

			links = append(links, domain.LLMSLink{
				Title: title,
				URL:   resolvedURL,
			})
		}
	}

	return links
}

func filterLLMSLinks(links []domain.LLMSLink, filterURL string) []domain.LLMSLink {
	// Empty filter means no filtering - return all
	if filterURL == "" {
		return links
	}

	filtered := make([]domain.LLMSLink, 0, len(links))
	for _, link := range links {
		// Try HasBaseURL first (works with full URLs)
		if utils.HasBaseURL(link.URL, filterURL) {
			filtered = append(filtered, link)
			continue
		}

		// For path-only filters (e.g., "/docs"), check if URL path contains the filter
		if strings.HasPrefix(filterURL, "/") && strings.Contains(link.URL, filterURL) {
			filtered = append(filtered, link)
		}
	}
	return filtered
}
</file>
<file path="internal/strategies/pkggo.go">
package strategies

import (
	"context"
	"fmt"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// PkgGoStrategy extracts documentation from pkg.go.dev
type PkgGoStrategy struct {
	deps      *Dependencies
	fetcher   domain.Fetcher
	converter *converter.Pipeline
	writer    *output.Writer
	logger    *utils.Logger
}

// NewPkgGoStrategy creates a new pkg.go.dev strategy
func NewPkgGoStrategy(deps *Dependencies) *PkgGoStrategy {
	if deps == nil {
		return &PkgGoStrategy{}
	}
	return &PkgGoStrategy{
		deps:      deps,
		fetcher:   deps.Fetcher,
		converter: deps.Converter,
		writer:    deps.Writer,
		logger:    deps.Logger,
	}
}

// Name returns the strategy name
func (s *PkgGoStrategy) Name() string {
	return "pkggo"
}

// SetFetcher allows setting a custom fetcher (used for testing)
func (s *PkgGoStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

// CanHandle returns true if this strategy can handle the given URL
func (s *PkgGoStrategy) CanHandle(url string) bool {
	return strings.Contains(url, "pkg.go.dev")
}

// Execute runs the pkg.go.dev extraction strategy
func (s *PkgGoStrategy) Execute(ctx context.Context, url string, opts Options) error {
	// Check context cancellation early
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	if s.fetcher == nil {
		return fmt.Errorf("pkggo strategy fetcher is nil")
	}
	if s.converter == nil {
		return fmt.Errorf("pkggo strategy converter is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("pkggo strategy writer is nil")
	}
	if s.logger == nil {
		return fmt.Errorf("pkggo strategy logger is nil")
	}

	s.logger.Info().Str("url", url).Msg("Fetching pkg.go.dev documentation")

	// Fetch page
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	// Parse HTML
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(string(resp.Body)))
	if err != nil {
		return err
	}

	// Extract package name
	packageName := doc.Find("h1.UnitHeader-title").First().Text()
	packageName = strings.TrimSpace(packageName)

	// If split option is enabled, extract sections separately
	if opts.Split {
		return s.extractSections(ctx, doc, url, packageName, opts)
	}

	// Extract main documentation content
	content := doc.Find("div.Documentation-content").First()
	if content.Length() == 0 {
		// Fallback to main content area
		content = doc.Find("main").First()
	}

	contentHTML, err := content.Html()
	if err != nil {
		return err
	}

	// Convert to document
	document, err := s.converter.Convert(ctx, contentHTML, url)
	if err != nil {
		return err
	}

	// Set metadata
	document.Title = packageName
	document.SourceStrategy = s.Name()
	document.CacheHit = resp.FromCache
	document.FetchedAt = time.Now()

	if !opts.DryRun {
		if s.deps != nil {
			return s.deps.WriteDocument(ctx, document)
		}
		return s.writer.Write(ctx, document)
	}

	return nil
}

// extractSections extracts documentation split by sections
func (s *PkgGoStrategy) extractSections(ctx context.Context, doc *goquery.Document, baseURL, packageName string, opts Options) error {
	sections := []struct {
		selector string
		name     string
	}{
		{"#pkg-overview", "Overview"},
		{"#pkg-index", "Index"},
		{"#pkg-constants", "Constants"},
		{"#pkg-variables", "Variables"},
		{"#pkg-functions", "Functions"},
		{"#pkg-types", "Types"},
	}

	for _, section := range sections {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		content := doc.Find(section.selector).First()
		if content.Length() == 0 {
			continue
		}

		// Get section HTML
		sectionHTML, err := content.Html()
		if err != nil {
			continue
		}

		// Skip empty sections
		if strings.TrimSpace(sectionHTML) == "" {
			continue
		}

		// Create section URL
		sectionURL := baseURL + section.selector

		// Convert to document
		document, err := s.converter.Convert(ctx, sectionHTML, sectionURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to convert section")
			continue
		}

		// Set metadata
		document.Title = packageName + " - " + section.name
		document.SourceStrategy = s.Name()
		document.FetchedAt = time.Now()

		if !opts.DryRun {
			if s.deps != nil {
				if err := s.deps.WriteDocument(ctx, document); err != nil {
					s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to write section")
				}
			} else {
				if err := s.writer.Write(ctx, document); err != nil {
					s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to write section")
				}
			}
		}
	}

	s.logger.Info().Msg("pkg.go.dev extraction completed")
	return nil
}
</file>
<file path="internal/strategies/sitemap.go">
package strategies

import (
	"compress/gzip"
	"context"
	"encoding/xml"
	"io"
	"sort"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// SitemapStrategy extracts documentation from sitemap XML files
type SitemapStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// NewSitemapStrategy creates a new sitemap strategy
func NewSitemapStrategy(deps *Dependencies) *SitemapStrategy {
	if deps == nil {
		return &SitemapStrategy{
			markdownReader: converter.NewMarkdownReader(),
		}
	}
	return &SitemapStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *SitemapStrategy) Name() string {
	return "sitemap"
}

// SetFetcher allows setting a custom fetcher (used for testing)
func (s *SitemapStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

// CanHandle returns true if this strategy can handle the given URL
func (s *SitemapStrategy) CanHandle(url string) bool {
	lower := strings.ToLower(url)
	return strings.HasSuffix(lower, "sitemap.xml") ||
		strings.HasSuffix(lower, "sitemap.xml.gz") ||
		strings.Contains(lower, "sitemap")
}

// Execute runs the sitemap extraction strategy
func (s *SitemapStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Fetching sitemap")

	// Fetch sitemap
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	// Decompress if gzipped
	content := resp.Body
	if strings.HasSuffix(strings.ToLower(url), ".gz") {
		content, err = decompressGzip(resp.Body)
		if err != nil {
			return err
		}
	}

	// Parse sitemap
	sitemap, err := parseSitemap(content, url)
	if err != nil {
		return err
	}

	// If it's a sitemap index, process each sitemap
	if sitemap.IsIndex {
		return s.processSitemapIndex(ctx, sitemap, opts)
	}

	// Sort by lastmod (most recent first)
	sortURLsByLastMod(sitemap.URLs)

	// Apply limit
	urls := sitemap.URLs
	if opts.Limit > 0 && len(urls) > opts.Limit {
		urls = urls[:opts.Limit]
	}

	s.logger.Info().Int("count", len(urls)).Msg("Processing URLs from sitemap")

	return s.processURLs(ctx, urls, opts)
}

// processSitemapIndex processes a sitemap index file by collecting all URLs first,
// then processing them with a single progress bar for consistent display.
func (s *SitemapStrategy) processSitemapIndex(ctx context.Context, sitemap *domain.Sitemap, opts Options) error {
	s.logger.Info().Int("count", len(sitemap.Sitemaps)).Msg("Processing sitemap index")

	// Collect all URLs from nested sitemaps first
	var allURLs []domain.SitemapURL
	for _, sitemapURL := range sitemap.Sitemaps {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		urls, err := s.collectURLsFromSitemap(ctx, sitemapURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", sitemapURL).Msg("Failed to fetch nested sitemap")
			continue
		}
		allURLs = append(allURLs, urls...)
	}

	if len(allURLs) == 0 {
		s.logger.Warn().Msg("No URLs found in sitemap index")
		return nil
	}

	// Sort by lastmod (most recent first)
	sortURLsByLastMod(allURLs)

	// Apply limit
	if opts.Limit > 0 && len(allURLs) > opts.Limit {
		allURLs = allURLs[:opts.Limit]
	}

	s.logger.Info().Int("count", len(allURLs)).Msg("Processing URLs from sitemap index")

	// Process all URLs with a single progress bar
	return s.processURLs(ctx, allURLs, opts)
}

// collectURLsFromSitemap fetches and parses a sitemap, returning its URLs.
// For sitemap indexes, it recursively collects URLs from all nested sitemaps.
func (s *SitemapStrategy) collectURLsFromSitemap(ctx context.Context, url string) ([]domain.SitemapURL, error) {
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return nil, err
	}

	// Decompress if gzipped
	content := resp.Body
	if strings.HasSuffix(strings.ToLower(url), ".gz") {
		content, err = decompressGzip(resp.Body)
		if err != nil {
			return nil, err
		}
	}

	// Parse sitemap
	sitemap, err := parseSitemap(content, url)
	if err != nil {
		return nil, err
	}

	// If it's a nested index, recursively collect URLs
	if sitemap.IsIndex {
		var allURLs []domain.SitemapURL
		for _, nestedURL := range sitemap.Sitemaps {
			select {
			case <-ctx.Done():
				return nil, ctx.Err()
			default:
			}

			urls, err := s.collectURLsFromSitemap(ctx, nestedURL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", nestedURL).Msg("Failed to fetch nested sitemap")
				continue
			}
			allURLs = append(allURLs, urls...)
		}
		return allURLs, nil
	}

	return sitemap.URLs, nil
}

func (s *SitemapStrategy) processURLs(ctx context.Context, urls []domain.SitemapURL, opts Options) error {
	bar := utils.NewProgressBar(len(urls), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, urls, opts.Concurrency, func(ctx context.Context, sitemapURL domain.SitemapURL) error {
		defer bar.Add(1)

		if !opts.Force && s.writer.Exists(sitemapURL.Loc) {
			return nil
		}

		pageResp, err := s.fetcher.Get(ctx, sitemapURL.Loc)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to fetch page")
			return nil
		}

		var doc *domain.Document
		if converter.IsMarkdownContent(pageResp.ContentType, sitemapURL.Loc) {
			doc, err = s.markdownReader.Read(string(pageResp.Body), sitemapURL.Loc)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to read markdown")
				return nil
			}
		} else {
			html := string(pageResp.Body)

			if opts.RenderJS || renderer.NeedsJSRendering(html) {
				if r, err := s.deps.GetRenderer(); err == nil {
					s.renderer = r
					rendered, err := s.renderer.Render(ctx, sitemapURL.Loc, domain.RenderOptions{
						Timeout:     60 * time.Second,
						WaitStable:  2 * time.Second,
						ScrollToEnd: true,
					})
					if err == nil {
						html = rendered
					}
				}
			}

			doc, err = s.converter.Convert(ctx, html, sitemapURL.Loc)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to convert page")
				return nil
			}
		}

		doc.SourceStrategy = s.Name()
		doc.CacheHit = pageResp.FromCache
		doc.FetchedAt = time.Now()

		if !opts.DryRun {
			if err := s.deps.WriteDocument(ctx, doc); err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to write document")
				return nil
			}
		}

		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Msg("Sitemap extraction completed")
	return nil
}

// sitemapXML represents the XML structure of a sitemap
type sitemapXML struct {
	XMLName xml.Name     `xml:"urlset"`
	URLs    []sitemapURL `xml:"url"`
}

type sitemapURL struct {
	Loc        string `xml:"loc"`
	LastMod    string `xml:"lastmod"`
	ChangeFreq string `xml:"changefreq"`
	Priority   string `xml:"priority"`
}

// sitemapIndexXML represents the XML structure of a sitemap index
type sitemapIndexXML struct {
	XMLName  xml.Name          `xml:"sitemapindex"`
	Sitemaps []sitemapLocation `xml:"sitemap"`
}

type sitemapLocation struct {
	Loc     string `xml:"loc"`
	LastMod string `xml:"lastmod"`
}

// parseSitemap parses sitemap XML content
func parseSitemap(content []byte, sourceURL string) (*domain.Sitemap, error) {
	// Try to parse as sitemap index first
	var index sitemapIndexXML
	if err := xml.Unmarshal(content, &index); err == nil && len(index.Sitemaps) > 0 {
		var sitemaps []string
		for _, sm := range index.Sitemaps {
			sitemaps = append(sitemaps, sm.Loc)
		}
		return &domain.Sitemap{
			IsIndex:   true,
			Sitemaps:  sitemaps,
			SourceURL: sourceURL,
		}, nil
	}

	// Parse as regular sitemap
	var sitemap sitemapXML
	if err := xml.Unmarshal(content, &sitemap); err != nil {
		return nil, err
	}

	var urls []domain.SitemapURL
	for _, u := range sitemap.URLs {
		lastMod, _ := parseLastMod(u.LastMod)
		urls = append(urls, domain.SitemapURL{
			Loc:        u.Loc,
			LastMod:    lastMod,
			LastModStr: u.LastMod,
			ChangeFreq: u.ChangeFreq,
		})
	}

	return &domain.Sitemap{
		URLs:      urls,
		IsIndex:   false,
		SourceURL: sourceURL,
	}, nil
}

// parseLastMod parses a lastmod date string
func parseLastMod(s string) (time.Time, error) {
	formats := []string{
		time.RFC3339,
		"2006-01-02T15:04:05-07:00",
		"2006-01-02T15:04:05Z",
		"2006-01-02",
	}

	for _, format := range formats {
		if t, err := time.Parse(format, s); err == nil {
			return t, nil
		}
	}

	return time.Time{}, nil
}

// sortURLsByLastMod sorts URLs by lastmod date (most recent first)
func sortURLsByLastMod(urls []domain.SitemapURL) {
	sort.Slice(urls, func(i, j int) bool {
		return urls[i].LastMod.After(urls[j].LastMod)
	})
}

// decompressGzip decompresses gzip content
func decompressGzip(data []byte) ([]byte, error) {
	reader, err := gzip.NewReader(strings.NewReader(string(data)))
	if err != nil {
		return nil, err
	}
	defer reader.Close()

	return io.ReadAll(reader)
}
</file>
<file path="internal/strategies/strategy.go">
package strategies

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/cache"
	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/fetcher"
	"github.com/quantmind-br/repodocs-go/internal/llm"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// Strategy defines the interface for documentation extraction strategies
type Strategy interface {
	// Name returns the strategy name
	Name() string
	// CanHandle returns true if this strategy can handle the given URL
	CanHandle(url string) bool
	// Execute runs the extraction strategy
	Execute(ctx context.Context, url string, opts Options) error
}

// Options contains common options for all strategies
type Options struct {
	domain.CommonOptions
	Output          string
	Concurrency     int
	MaxDepth        int
	Exclude         []string
	NoFolders       bool
	Split           bool
	IncludeAssets   bool
	ContentSelector string
	ExcludeSelector string
	CacheTTL        string
	FilterURL       string
}

// DefaultOptions returns default strategy options
func DefaultOptions() Options {
	return Options{
		CommonOptions: domain.DefaultCommonOptions(),
		Output:        "./docs",
		Concurrency:   5,
		MaxDepth:      3,
		NoFolders:     false,
		Split:         false,
	}
}

// Dependencies contains shared dependencies for all strategies
type Dependencies struct {
	Fetcher          domain.Fetcher
	Renderer         domain.Renderer
	Cache            domain.Cache
	Converter        *converter.Pipeline
	Writer           *output.Writer
	Logger           *utils.Logger
	LLMProvider      domain.LLMProvider
	MetadataEnhancer *llm.MetadataEnhancer
	Collector        *output.MetadataCollector
	HTTPClient       *http.Client
	StateManager     *state.Manager

	rendererOnce sync.Once
	rendererOpts renderer.RendererOptions
	rendererErr  error
}

// NewDependencies creates new dependencies for strategies
func NewDependencies(opts DependencyOptions) (*Dependencies, error) {
	// Create fetcher
	fetcherClient, err := fetcher.NewClient(fetcher.ClientOptions{
		Timeout:     opts.Timeout,
		MaxRetries:  3,
		EnableCache: opts.EnableCache,
		CacheTTL:    opts.CacheTTL,
		UserAgent:   opts.UserAgent,
	})
	if err != nil {
		return nil, err
	}

	// Create cache if enabled
	var cacheImpl domain.Cache
	if opts.EnableCache {
		cacheImpl, err = cache.NewBadgerCache(cache.Options{
			Directory: opts.CacheDir,
		})
		if err != nil {
			return nil, err
		}
		fetcherClient.SetCache(cacheImpl)
	}

	// Prepare renderer options for lazy initialization
	rendererOpts := renderer.DefaultRendererOptions()
	if opts.RendererTimeout > 0 {
		rendererOpts.Timeout = opts.RendererTimeout
	}
	if opts.Concurrency > 0 {
		rendererOpts.MaxTabs = opts.Concurrency
	}

	// Create renderer eagerly only if explicitly requested
	var rendererImpl domain.Renderer
	if opts.EnableRenderer {
		r, err := renderer.NewRenderer(rendererOpts)
		if err == nil {
			rendererImpl = r
		}
	}

	// Create converter
	converterPipeline := converter.NewPipeline(converter.PipelineOptions{
		BaseURL:         "",
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
	})

	var collector *output.MetadataCollector
	if opts.JSONMetadata {
		collector = output.NewMetadataCollector(output.CollectorOptions{
			BaseDir:   opts.OutputDir,
			SourceURL: opts.SourceURL,
			Enabled:   true,
		})
	}

	// Create writer
	writer := output.NewWriter(output.WriterOptions{
		BaseDir:      opts.OutputDir,
		Flat:         opts.Flat,
		JSONMetadata: opts.JSONMetadata,
		Force:        opts.Force,
		DryRun:       opts.DryRun,
		Collector:    collector,
	})

	// Create logger
	logger := utils.NewLogger(utils.LoggerOptions{
		Level:   "info",
		Format:  "pretty",
		Verbose: opts.Verbose,
	})

	var llmProvider domain.LLMProvider
	var metadataEnhancer *llm.MetadataEnhancer
	if opts.LLMConfig != nil && opts.LLMConfig.EnhanceMetadata && opts.LLMConfig.Provider != "" {
		baseProvider, err := llm.NewProviderFromConfig(opts.LLMConfig)
		if err != nil {
			logger.Warn().Err(err).Msg("Failed to create LLM provider, metadata enhancement disabled")
		} else {
			if opts.LLMConfig.RateLimit.Enabled {
				llmProvider = llm.NewRateLimitedProvider(
					baseProvider,
					llm.RateLimitedProviderConfig{
						RequestsPerMinute:        opts.LLMConfig.RateLimit.RequestsPerMinute,
						BurstSize:                opts.LLMConfig.RateLimit.BurstSize,
						MaxRetries:               opts.LLMConfig.RateLimit.MaxRetries,
						InitialDelay:             opts.LLMConfig.RateLimit.InitialDelay,
						MaxDelay:                 opts.LLMConfig.RateLimit.MaxDelay,
						Multiplier:               opts.LLMConfig.RateLimit.Multiplier,
						CircuitBreakerEnabled:    opts.LLMConfig.RateLimit.CircuitBreaker.Enabled,
						FailureThreshold:         opts.LLMConfig.RateLimit.CircuitBreaker.FailureThreshold,
						SuccessThresholdHalfOpen: opts.LLMConfig.RateLimit.CircuitBreaker.SuccessThresholdHalfOpen,
						ResetTimeout:             opts.LLMConfig.RateLimit.CircuitBreaker.ResetTimeout,
					},
					logger,
				)
				logger.Info().
					Str("provider", opts.LLMConfig.Provider).
					Int("requests_per_minute", opts.LLMConfig.RateLimit.RequestsPerMinute).
					Int("burst_size", opts.LLMConfig.RateLimit.BurstSize).
					Msg("LLM metadata enhancement enabled with rate limiting")
			} else {
				llmProvider = baseProvider
				logger.Info().Str("provider", opts.LLMConfig.Provider).Msg("LLM metadata enhancement enabled")
			}
			metadataEnhancer = llm.NewMetadataEnhancer(llmProvider)
		}
	}

	var stateManager *state.Manager
	if opts.Sync && !opts.FullSync {
		stateManager = state.NewManager(state.ManagerOptions{
			BaseDir:   opts.OutputDir,
			SourceURL: opts.SourceURL,
			Logger:    logger,
			Disabled:  false,
		})
		if err := stateManager.Load(context.Background()); err != nil {
			if !errors.Is(err, state.ErrStateNotFound) {
				logger.Warn().Err(err).Msg("Failed to load state, starting fresh")
			}
		}
	}

	return &Dependencies{
		Fetcher:          fetcherClient,
		Renderer:         rendererImpl,
		Cache:            cacheImpl,
		Converter:        converterPipeline,
		Writer:           writer,
		Logger:           logger,
		LLMProvider:      llmProvider,
		MetadataEnhancer: metadataEnhancer,
		Collector:        collector,
		StateManager:     stateManager,
		rendererOpts:     rendererOpts,
	}, nil
}

// Close releases all resources
func (d *Dependencies) Close() error {
	if d.Fetcher != nil {
		d.Fetcher.Close()
	}
	if d.Renderer != nil {
		d.Renderer.Close()
	}
	if d.Cache != nil {
		d.Cache.Close()
	}
	if d.LLMProvider != nil {
		d.LLMProvider.Close()
	}
	return nil
}

func (d *Dependencies) FlushMetadata() error {
	if d.Collector != nil {
		return d.Collector.Flush()
	}
	return nil
}

func (d *Dependencies) SaveState(ctx context.Context) error {
	if d.StateManager != nil {
		return d.StateManager.Save(ctx)
	}
	return nil
}

func (d *Dependencies) PruneDeletedFiles(ctx context.Context) (int, error) {
	if d.StateManager == nil || d.StateManager.IsDisabled() {
		return 0, nil
	}

	deleted := d.StateManager.GetDeletedPages()
	if len(deleted) == 0 {
		return 0, nil
	}

	var pruned int
	for _, page := range deleted {
		if err := os.Remove(page.FilePath); err != nil {
			if !os.IsNotExist(err) {
				d.Logger.Warn().Err(err).Str("file", page.FilePath).Msg("Failed to remove deleted page")
				continue
			}
		}
		pruned++
		d.Logger.Info().Str("file", page.FilePath).Msg("Removed deleted page")
	}

	d.StateManager.RemoveDeletedFromState()
	return pruned, nil
}

func (d *Dependencies) GetStateManager() *state.Manager {
	return d.StateManager
}

func (d *Dependencies) SetStrategy(name string) {
	if d.Collector != nil {
		d.Collector.SetStrategy(name)
	}
}

func (d *Dependencies) SetSourceURL(url string) {
	if d.Collector != nil {
		d.Collector.SetSourceURL(url)
	}
}

func (d *Dependencies) GetRenderer() (domain.Renderer, error) {
	if d.Renderer != nil {
		return d.Renderer, nil
	}

	d.rendererOnce.Do(func() {
		opts := d.rendererOpts
		if opts.Timeout == 0 {
			opts = renderer.DefaultRendererOptions()
		}
		r, err := renderer.NewRenderer(opts)
		if err != nil {
			d.rendererErr = err
			d.Logger.Debug().Err(err).Msg("Failed to initialize browser renderer on demand")
			return
		}
		d.Renderer = r
		d.Logger.Info().Msg("Browser renderer initialized on demand")
	})

	if d.rendererErr != nil {
		return nil, d.rendererErr
	}
	return d.Renderer, nil
}

// WriteDocument enhances metadata (if configured) and writes the document
func (d *Dependencies) WriteDocument(ctx context.Context, doc *domain.Document) error {
	if d.MetadataEnhancer != nil {
		if err := d.MetadataEnhancer.Enhance(ctx, doc); err != nil {
			d.Logger.Warn().Err(err).Str("url", doc.URL).Msg("Failed to enhance metadata, writing without enhancement")
		}
	}

	if d.Writer == nil {
		return fmt.Errorf("writer is not configured")
	}

	if err := d.Writer.Write(ctx, doc); err != nil {
		return err
	}

	if d.StateManager != nil && doc.ContentHash != "" {
		filePath := d.Writer.GetPath(doc.URL)
		d.StateManager.Update(doc.URL, state.PageState{
			ContentHash: doc.ContentHash,
			FetchedAt:   doc.FetchedAt,
			FilePath:    filePath,
		})
	}

	return nil
}

// DependencyOptions contains options for creating dependencies
type DependencyOptions struct {
	domain.CommonOptions
	Timeout         time.Duration
	EnableCache     bool
	CacheTTL        time.Duration
	CacheDir        string
	UserAgent       string
	EnableRenderer  bool
	RendererTimeout time.Duration
	Concurrency     int
	ContentSelector string
	ExcludeSelector string
	OutputDir       string
	Flat            bool
	JSONMetadata    bool
	LLMConfig       *config.LLMConfig
	SourceURL       string
}
</file>
<file path="internal/strategies/wiki.go">
package strategies

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/go-git/go-git/v5"
	githttp "github.com/go-git/go-git/v5/plumbing/transport/http"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	internalgit "github.com/quantmind-br/repodocs-go/internal/git"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// WikiStrategy extracts documentation from GitHub wiki repositories
type WikiStrategy struct {
	deps      *Dependencies
	writer    *output.Writer
	logger    *utils.Logger
	gitClient internalgit.Client
}

// NewWikiStrategy creates a new wiki strategy
func NewWikiStrategy(deps *Dependencies) *WikiStrategy {
	if deps == nil {
		return &WikiStrategy{
			gitClient: internalgit.NewClient(),
		}
	}
	return &WikiStrategy{
		deps:      deps,
		writer:    deps.Writer,
		logger:    deps.Logger,
		gitClient: internalgit.NewClient(),
	}
}

// SetGitClient sets the git client (useful for testing)
func (s *WikiStrategy) SetGitClient(client internalgit.Client) {
	s.gitClient = client
}

// Name returns the strategy name
func (s *WikiStrategy) Name() string {
	return "wiki"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *WikiStrategy) CanHandle(url string) bool {
	return IsWikiURL(url)
}

// IsWikiURL checks if a URL points to a wiki (GitHub or Bitbucket)
func IsWikiURL(url string) bool {
	lower := strings.ToLower(url)

	// Pattern 1: github.com/{owner}/{repo}/wiki
	if strings.Contains(lower, "github.com") && strings.Contains(lower, "/wiki") {
		return true
	}

	// Pattern 2: bitbucket.org/{owner}/{repo}/wiki
	if strings.Contains(lower, "bitbucket.org") && strings.Contains(lower, "/wiki") {
		return true
	}

	// Pattern 3: {repo}.wiki.git
	if strings.HasSuffix(lower, ".wiki.git") {
		return true
	}

	return false
}

// Execute runs the wiki extraction strategy
func (s *WikiStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Starting wiki extraction")

	// Step 1: Parse wiki URL
	wikiInfo, err := ParseWikiURL(url)
	if err != nil {
		return fmt.Errorf("failed to parse wiki URL: %w", err)
	}

	s.logger.Debug().
		Str("owner", wikiInfo.Owner).
		Str("repo", wikiInfo.Repo).
		Str("clone_url", wikiInfo.CloneURL).
		Msg("Parsed wiki URL")

	// Step 2: Create temporary directory
	tmpDir, err := os.MkdirTemp("", "repodocs-wiki-*")
	if err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tmpDir)

	// Step 3: Clone wiki repository
	if err := s.cloneWiki(ctx, wikiInfo.CloneURL, tmpDir); err != nil {
		return fmt.Errorf("failed to clone wiki: %w", err)
	}

	// Step 4: Parse wiki structure
	structure, err := s.parseWikiStructure(tmpDir)
	if err != nil {
		return fmt.Errorf("failed to parse wiki structure: %w", err)
	}

	s.logger.Info().
		Int("pages", len(structure.Pages)).
		Int("sections", len(structure.Sections)).
		Bool("has_sidebar", structure.HasSidebar).
		Msg("Parsed wiki structure")

	// Step 5: Process and write documents
	return s.processPages(ctx, structure, wikiInfo, opts)
}

// cloneWiki clones the wiki repository
func (s *WikiStrategy) cloneWiki(ctx context.Context, cloneURL, destDir string) error {
	s.logger.Info().Str("url", cloneURL).Msg("Cloning wiki repository")

	cloneOpts := &git.CloneOptions{
		URL:      cloneURL,
		Depth:    1, // Shallow clone for speed
		Progress: nil,
	}

	// Use HTTPS auth if available
	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		cloneOpts.Auth = &githttp.BasicAuth{
			Username: "token",
			Password: token,
		}
	}

	_, err := s.gitClient.PlainCloneContext(ctx, destDir, false, cloneOpts)
	if err != nil {
		// Check if wiki doesn't exist
		if strings.Contains(err.Error(), "not found") ||
			strings.Contains(err.Error(), "404") ||
			strings.Contains(err.Error(), "repository not found") {
			return fmt.Errorf("wiki not found or not enabled for this repository")
		}
		return err
	}

	return nil
}

// parseWikiStructure parses the wiki file structure and sidebar
func (s *WikiStrategy) parseWikiStructure(dir string) (*WikiStructure, error) {
	structure := &WikiStructure{
		Pages:    make(map[string]*WikiPage),
		Sections: []WikiSection{},
	}

	// Read all markdown files
	entries, err := os.ReadDir(dir)
	if err != nil {
		return nil, err
	}

	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}

		name := entry.Name()
		ext := strings.ToLower(filepath.Ext(name))

		// Only process markdown files
		if ext != ".md" && ext != ".markdown" {
			continue
		}

		content, err := os.ReadFile(filepath.Join(dir, name))
		if err != nil {
			s.logger.Warn().Err(err).Str("file", name).Msg("Failed to read file")
			continue
		}

		page := &WikiPage{
			Filename:  name,
			Title:     FilenameToTitle(name),
			Content:   string(content),
			IsHome:    strings.EqualFold(name, "Home.md"),
			IsSpecial: strings.HasPrefix(name, "_"),
		}

		structure.Pages[name] = page
	}

	if sidebarPage, exists := structure.Pages["_Sidebar.md"]; exists {
		structure.HasSidebar = true
		structure.Sections = ParseSidebarContent(sidebarPage.Content, structure.Pages)
	} else {
		structure.Sections = CreateDefaultStructure(structure.Pages)
	}

	return structure, nil
}

// processPages processes all wiki pages and writes them to output
func (s *WikiStrategy) processPages(
	ctx context.Context,
	structure *WikiStructure,
	wikiInfo *WikiInfo,
	opts Options,
) error {
	// Count processable pages (exclude special files)
	var processablePages []*WikiPage
	for _, page := range structure.Pages {
		if !page.IsSpecial {
			processablePages = append(processablePages, page)
		}
	}

	if len(processablePages) == 0 {
		s.logger.Warn().Msg("No processable pages found in wiki")
		return nil
	}

	// Apply limit
	if opts.Limit > 0 && len(processablePages) > opts.Limit {
		processablePages = processablePages[:opts.Limit]
	}

	// Create progress bar
	bar := utils.NewProgressBar(len(processablePages), utils.DescExtracting)

	// Build base wiki URL for references
	baseWikiURL := fmt.Sprintf("https://github.com/%s/%s/wiki", wikiInfo.Owner, wikiInfo.Repo)

	// Process each page
	for _, page := range processablePages {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		if err := s.processPage(ctx, page, structure, baseWikiURL, opts); err != nil {
			s.logger.Warn().Err(err).Str("page", page.Filename).Msg("Failed to process page")
		}
		bar.Add(1)
	}

	s.logger.Info().
		Int("processed", len(processablePages)).
		Msg("Wiki extraction completed")

	return nil
}

// processPage processes a single wiki page
func (s *WikiStrategy) processPage(
	ctx context.Context,
	page *WikiPage,
	structure *WikiStructure,
	baseWikiURL string,
	opts Options,
) error {
	content := ConvertWikiLinks(page.Content, structure.Pages)

	pageName := strings.TrimSuffix(page.Filename, filepath.Ext(page.Filename))
	pageURL := baseWikiURL
	if !page.IsHome {
		pageURL = fmt.Sprintf("%s/%s", baseWikiURL, pageName)
	}

	relativePath := BuildRelativePath(page, structure, opts.NoFolders)

	// Create document
	doc := &domain.Document{
		URL:            pageURL,
		Title:          page.Title,
		Content:        content,
		FetchedAt:      time.Now(),
		WordCount:      len(strings.Fields(content)),
		CharCount:      len(content),
		SourceStrategy: s.Name(),
		RelativePath:   relativePath,
	}

	if !opts.DryRun {
		return s.deps.WriteDocument(ctx, doc)
	}

	return nil
}
</file>
<file path="internal/strategies/wiki_parser.go">
package strategies

import (
	"fmt"
	"path/filepath"
	"regexp"
	"sort"
	"strings"
)

type WikiPage struct {
	Filename  string
	Title     string
	Content   string
	Section   string
	Order     int
	Links     []string
	IsHome    bool
	IsSpecial bool
}

type WikiStructure struct {
	Sections   []WikiSection
	Pages      map[string]*WikiPage
	HasSidebar bool
}

type WikiSection struct {
	Name  string
	Order int
	Pages []string
}

type WikiInfo struct {
	Owner      string
	Repo       string
	CloneURL   string
	Platform   string
	TargetPage string
}

func ParseWikiURL(rawURL string) (*WikiInfo, error) {
	url := strings.TrimSuffix(rawURL, "/")

	// Check for SSH format: git@github.com:owner/repo.wiki.git
	if strings.HasPrefix(url, "git@") {
		return nil, fmt.Errorf("invalid wiki URL format (SSH not supported): %s", rawURL)
	}

	// github.com/{owner}/{repo}/wiki[/{page}] or {repo}.wiki.git
	wikiPattern := regexp.MustCompile(
		`github\.com[:/]([^/]+)/([^/]+?)(?:\.wiki)?(?:/wiki)?(?:/([^/]+))?(?:\.git)?$`,
	)

	matches := wikiPattern.FindStringSubmatch(url)
	if len(matches) < 3 {
		return nil, fmt.Errorf("invalid wiki URL format: %s", rawURL)
	}

	owner := matches[1]
	repo := strings.TrimSuffix(matches[2], ".wiki")

	var targetPage string
	if len(matches) > 3 && matches[3] != "" {
		targetPage = matches[3]
	}

	cloneURL := fmt.Sprintf("https://github.com/%s/%s.wiki.git", owner, repo)

	return &WikiInfo{
		Owner:      owner,
		Repo:       repo,
		CloneURL:   cloneURL,
		Platform:   "github",
		TargetPage: targetPage,
	}, nil
}

func FilenameToTitle(filename string) string {
	// Extract base name without extension
	ext := filepath.Ext(filename)
	name := strings.TrimSuffix(filename, ext)

	// If name is empty after removing extension, return empty
	if name == "" {
		return ""
	}

	name = strings.ReplaceAll(name, "-", " ")
	name = strings.ReplaceAll(name, "_", " ")

	words := strings.Fields(name)
	for i, word := range words {
		if len(word) > 0 {
			words[i] = strings.ToUpper(word[:1]) + word[1:]
		}
	}

	return strings.Join(words, " ")
}

func TitleToFilename(title string) string {
	return strings.ReplaceAll(title, " ", "-")
}

func ParseSidebarContent(content string, pages map[string]*WikiPage) []WikiSection {
	var sections []WikiSection
	var currentSection *WikiSection

	lines := strings.Split(content, "\n")
	sectionOrder := 0
	pageOrder := 0

	headerPattern := regexp.MustCompile(`^#+\s*(.+)$`)
	wikiLinkPattern := regexp.MustCompile(`\[\[([^\]|]+)(?:\|[^\]]+)?\]\]`)
	mdLinkPattern := regexp.MustCompile(`\[([^\]]+)\]\(([^)]+)\)`)

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if matches := headerPattern.FindStringSubmatch(trimmed); len(matches) > 1 {
			if currentSection != nil && len(currentSection.Pages) > 0 {
				sections = append(sections, *currentSection)
			}

			sectionOrder++
			pageOrder = 0
			currentSection = &WikiSection{
				Name:  strings.TrimSpace(matches[1]),
				Order: sectionOrder,
				Pages: []string{},
			}
			continue
		}

		if wikiMatches := wikiLinkPattern.FindAllStringSubmatch(trimmed, -1); len(wikiMatches) > 0 {
			if currentSection == nil {
				sectionOrder++
				currentSection = &WikiSection{
					Name:  "General",
					Order: sectionOrder,
					Pages: []string{},
				}
			}
			for _, match := range wikiMatches {
				pageName := match[1]
				filename := findPageFilename(pageName, pages)
				if filename != "" {
					pageOrder++
					if page, exists := pages[filename]; exists {
						page.Section = currentSection.Name
						page.Order = pageOrder
					}
					if currentSection != nil {
						currentSection.Pages = append(currentSection.Pages, filename)
					}
				}
			}
			continue
		}

		if mdMatches := mdLinkPattern.FindAllStringSubmatch(trimmed, -1); len(mdMatches) > 0 {
			if currentSection == nil {
				sectionOrder++
				currentSection = &WikiSection{
					Name:  "General",
					Order: sectionOrder,
					Pages: []string{},
				}
			}
			for _, match := range mdMatches {
				pageName := match[2]
				pageName = strings.TrimSuffix(pageName, ".md")
				filename := findPageFilename(pageName, pages)
				if filename != "" {
					pageOrder++
					if page, exists := pages[filename]; exists {
						page.Section = currentSection.Name
						page.Order = pageOrder
					}
					if currentSection != nil {
						currentSection.Pages = append(currentSection.Pages, filename)
					}
				}
			}
		}
	}

	if currentSection != nil && len(currentSection.Pages) > 0 {
		sections = append(sections, *currentSection)
	}

	return sections
}

func findPageFilename(pageName string, pages map[string]*WikiPage) string {
	// Exact match first
	if _, exists := pages[pageName]; exists {
		return pageName
	}

	// Match with .md extension
	if _, exists := pages[pageName+".md"]; exists {
		return pageName + ".md"
	}

	// Hyphenated match
	hyphenated := strings.ReplaceAll(pageName, " ", "-") + ".md"
	if _, exists := pages[hyphenated]; exists {
		return hyphenated
	}

	// Case-insensitive match
	for filename := range pages {
		baseName := strings.TrimSuffix(filename, ".md")
		if strings.EqualFold(baseName, pageName) ||
			strings.EqualFold(baseName, strings.ReplaceAll(pageName, " ", "-")) {
			return filename
		}
	}

	return ""
}

func CreateDefaultStructure(pages map[string]*WikiPage) []WikiSection {
	var pageNames []string
	for filename, page := range pages {
		if !page.IsSpecial {
			pageNames = append(pageNames, filename)
		}
	}

	sort.Strings(pageNames)

	for i, name := range pageNames {
		if strings.EqualFold(name, "Home.md") {
			pageNames = append([]string{name}, append(pageNames[:i], pageNames[i+1:]...)...)
			break
		}
	}

	for i, filename := range pageNames {
		if page, exists := pages[filename]; exists {
			page.Order = i + 1
			page.Section = "Documentation"
		}
	}

	return []WikiSection{
		{
			Name:  "Documentation",
			Order: 1,
			Pages: pageNames,
		},
	}
}

func ConvertWikiLinks(content string, _ map[string]*WikiPage) string {
	// [[Page Name|Custom Text]] -> [Custom Text](./page-name.md)
	pattern1 := regexp.MustCompile(`\[\[([^\]|]+)\|([^\]]+)\]\]`)
	content = pattern1.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern1.FindStringSubmatch(match)
		if len(matches) == 3 {
			pageName := matches[1]
			linkText := matches[2]
			filename := TitleToFilename(pageName) + ".md"
			return fmt.Sprintf("[%s](./%s)", linkText, strings.ToLower(filename))
		}
		return match
	})

	// [[Page Name#Section]] -> [Page Name](./page-name.md#section)
	pattern2 := regexp.MustCompile(`\[\[([^\]#]+)#([^\]]+)\]\]`)
	content = pattern2.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern2.FindStringSubmatch(match)
		if len(matches) == 3 {
			pageName := matches[1]
			section := matches[2]
			filename := TitleToFilename(pageName) + ".md"
			anchor := strings.ToLower(strings.ReplaceAll(section, " ", "-"))
			return fmt.Sprintf("[%s](./%s#%s)", pageName, strings.ToLower(filename), anchor)
		}
		return match
	})

	// [[Page Name]] -> [Page Name](./page-name.md)
	pattern3 := regexp.MustCompile(`\[\[([^\]]+)\]\]`)
	content = pattern3.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern3.FindStringSubmatch(match)
		if len(matches) == 2 {
			pageName := matches[1]
			filename := TitleToFilename(pageName) + ".md"
			return fmt.Sprintf("[%s](./%s)", pageName, strings.ToLower(filename))
		}
		return match
	})

	return content
}

func BuildRelativePath(page *WikiPage, structure *WikiStructure, flat bool) string {
	if page.IsHome {
		return "index.md"
	}

	if flat || len(structure.Sections) == 0 || page.Section == "" {
		return strings.ToLower(page.Filename)
	}

	sectionDir := strings.ToLower(strings.ReplaceAll(page.Section, " ", "-"))
	filename := strings.ToLower(page.Filename)

	return filepath.Join(sectionDir, filename)
}
</file>
<file path="internal/tui/app.go">
package tui

import (
	"fmt"
	"strings"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/huh"
	"github.com/charmbracelet/lipgloss"

	"github.com/quantmind-br/repodocs-go/internal/config"
)

type state int

const (
	stateMenu state = iota
	stateSubMenu
	stateForm
	stateConfirm
	stateSaved
	stateError
)

type Model struct {
	state           state
	values          *ConfigValues
	originalConfig  *config.Config
	menuIndex       int
	currentForm     *huh.Form
	err             error
	width           int
	height          int
	dirty           bool
	saveFunc        func(*config.Config) error
	accessible      bool
	subMenuIndex    int
	parentCategory  *Category
	cameFromSubMenu bool
}

type Options struct {
	Config     *config.Config
	SaveFunc   func(*config.Config) error
	Accessible bool
}

func NewModel(opts Options) Model {
	cfg := opts.Config
	if cfg == nil {
		cfg = config.Default()
	}

	return Model{
		state:          stateMenu,
		values:         FromConfig(cfg),
		originalConfig: cfg,
		saveFunc:       opts.SaveFunc,
		accessible:     opts.Accessible,
	}
}

func (m Model) Init() tea.Cmd {
	return nil
}

func (m Model) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.WindowSizeMsg:
		m.width = msg.Width
		m.height = msg.Height
		if m.state == stateForm && m.currentForm != nil {
			form, cmd := m.currentForm.Update(msg)
			if f, ok := form.(*huh.Form); ok {
				m.currentForm = f
			}
			return m, cmd
		}
		return m, nil

	case tea.KeyMsg:
		switch m.state {
		case stateMenu:
			return m.updateMenu(msg)
		case stateSubMenu:
			return m.updateSubMenu(msg)
		case stateForm:
			return m.updateForm(msg)
		case stateConfirm:
			return m.updateConfirm(msg)
		case stateSaved, stateError:
			return m, tea.Quit
		}
	}

	if m.state == stateForm && m.currentForm != nil {
		form, cmd := m.currentForm.Update(msg)
		if f, ok := form.(*huh.Form); ok {
			m.currentForm = f
		}
		if m.currentForm.State == huh.StateCompleted {
			m.dirty = true
			m.state = stateMenu
			return m, nil
		}
		return m, cmd
	}

	return m, nil
}

func (m Model) updateMenu(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "q", "ctrl+c":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit

	case "up", "k":
		if m.menuIndex > 0 {
			m.menuIndex--
		}

	case "down", "j":
		if m.menuIndex < len(Categories) {
			m.menuIndex++
		}

	case "enter":
		if m.menuIndex == len(Categories) {
			return m.handleSave()
		}
		category := Categories[m.menuIndex]

		// Check if category has sub-categories
		if category.HasSubCategories() {
			m.state = stateSubMenu
			m.parentCategory = &Categories[m.menuIndex]
			m.subMenuIndex = 0
			return m, nil
		}

		// No sub-categories - open form directly (existing behavior)
		m.state = stateForm
		m.currentForm = GetFormForCategory(category.ID, m.values)
		m.cameFromSubMenu = false
		if m.accessible {
			m.currentForm = m.currentForm.WithAccessible(true)
		}
		return m, m.currentForm.Init()

	case "s":
		return m.handleSave()

	case "esc":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit
	}

	return m, nil
}

func (m Model) updateSubMenu(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "q", "ctrl+c":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit

	case "up", "k":
		if m.subMenuIndex > 0 {
			m.subMenuIndex--
		}

	case "down", "j":
		if m.subMenuIndex < len(m.parentCategory.SubCategories)-1 {
			m.subMenuIndex++
		}

	case "enter":
		subCat := m.parentCategory.SubCategories[m.subMenuIndex]
		m.state = stateForm
		m.currentForm = GetFormForCategory(subCat.ID, m.values)
		m.cameFromSubMenu = true
		if m.accessible {
			m.currentForm = m.currentForm.WithAccessible(true)
		}
		return m, m.currentForm.Init()

	case "esc":
		m.state = stateMenu
		m.parentCategory = nil
		return m, nil
	}
	return m, nil
}

func (m Model) updateForm(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	if msg.String() == "esc" {
		if m.cameFromSubMenu && m.parentCategory != nil {
			m.state = stateSubMenu
		} else {
			m.state = stateMenu
		}
		return m, nil
	}
	if m.currentForm != nil {
		form, cmd := m.currentForm.Update(msg)
		if f, ok := form.(*huh.Form); ok {
			m.currentForm = f
		}
		if m.currentForm.State == huh.StateCompleted {
			m.dirty = true
			if m.cameFromSubMenu && m.parentCategory != nil {
				m.state = stateSubMenu
			} else {
				m.state = stateMenu
			}
			return m, nil
		}
		return m, cmd
	}
	return m, nil
}

func (m Model) updateConfirm(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "y", "Y":
		return m.handleSave()
	case "n", "N", "esc":
		return m, tea.Quit
	case "c":
		m.state = stateMenu
	}
	return m, nil
}

func (m Model) handleSave() (tea.Model, tea.Cmd) {
	cfg, err := m.values.ToConfig()
	if err != nil {
		m.state = stateError
		m.err = err
		return m, nil
	}

	if m.saveFunc != nil {
		if err := m.saveFunc(cfg); err != nil {
			m.state = stateError
			m.err = err
			return m, nil
		}
	}

	m.state = stateSaved
	m.dirty = false
	return m, nil
}

func (m Model) View() string {
	var s strings.Builder

	header := TitleStyle.Render("RepoDocs Configuration")
	s.WriteString(header)
	s.WriteString("\n\n")

	switch m.state {
	case stateMenu:
		s.WriteString(m.renderMenu())
	case stateSubMenu:
		s.WriteString(m.renderSubMenu())
	case stateForm:
		if m.currentForm != nil {
			s.WriteString(m.currentForm.View())
		}
	case stateConfirm:
		s.WriteString(m.renderConfirm())
	case stateSaved:
		s.WriteString(SuccessStyle.Render("Configuration saved successfully!"))
		s.WriteString("\n\nPress any key to exit.")
	case stateError:
		s.WriteString(ErrorStyle.Render(fmt.Sprintf("Error: %v", m.err)))
		s.WriteString("\n\nPress any key to exit.")
	}

	return s.String()
}

func (m Model) renderMenu() string {
	var s strings.Builder

	for i, cat := range Categories {
		cursor := "  "
		style := UnselectedStyle
		if i == m.menuIndex {
			cursor = "> "
			style = SelectedStyle
		}
		line := fmt.Sprintf("%s%s %s", cursor, cat.Icon, cat.Name)
		s.WriteString(style.Render(line))
		if i == m.menuIndex {
			s.WriteString(DescriptionStyle.Render("  " + cat.Description))
		}
		s.WriteString("\n")
	}

	saveStyle := UnselectedStyle
	saveCursor := "  "
	if m.menuIndex == len(Categories) {
		saveCursor = "> "
		saveStyle = SelectedStyle
	}
	saveText := fmt.Sprintf("%s Save Configuration", saveCursor)
	if m.dirty {
		saveText += " *"
	}
	s.WriteString("\n")
	s.WriteString(saveStyle.Render(saveText))
	s.WriteString("\n\n")

	help := HelpStyle.Render("↑/↓ navigate • enter select • s save • q quit")
	s.WriteString(help)

	return s.String()
}

func (m Model) renderSubMenu() string {
	var s strings.Builder

	s.WriteString(DescriptionStyle.Render(m.parentCategory.Name + " Settings"))
	s.WriteString("\n\n")

	for i, subCat := range m.parentCategory.SubCategories {
		cursor := "  "
		style := UnselectedStyle
		if i == m.subMenuIndex {
			cursor = "> "
			style = SelectedStyle
		}
		line := fmt.Sprintf("%s%s %s", cursor, subCat.Icon, subCat.Name)
		s.WriteString(style.Render(line))
		if i == m.subMenuIndex {
			s.WriteString(DescriptionStyle.Render("  " + subCat.Description))
		}
		s.WriteString("\n")
	}

	s.WriteString("\n")
	help := HelpStyle.Render("↑/↓ navigate • enter select • esc back")
	s.WriteString(help)

	return s.String()
}

func (m Model) renderConfirm() string {
	box := lipgloss.NewStyle().
		Border(lipgloss.RoundedBorder()).
		BorderForeground(warnColor).
		Padding(1, 2).
		Render("You have unsaved changes.\n\nSave before quitting?\n\n[y] Yes  [n] No  [c] Cancel")

	return box
}

func Run(opts Options) error {
	p := tea.NewProgram(NewModel(opts), tea.WithAltScreen())
	_, err := p.Run()
	return err
}
</file>
<file path="internal/tui/categories.go">
package tui

type Category struct {
	ID            string
	Name          string
	Description   string
	Icon          string
	SubCategories []Category
}

var Categories = []Category{
	{ID: "output", Name: "Output", Description: "Output directory and format settings", Icon: ""},
	{ID: "exclude", Name: "Exclude Patterns", Description: "URL/path patterns to skip", Icon: ""},
	{ID: "concurrency", Name: "Concurrency", Description: "Workers, timeout, and depth limits", Icon: ""},
	{ID: "cache", Name: "Cache", Description: "Caching behavior and TTL", Icon: ""},
	{ID: "rendering", Name: "Rendering", Description: "JavaScript rendering options", Icon: ""},
	{ID: "stealth", Name: "Stealth", Description: "User-agent and delay settings", Icon: ""},
	{ID: "logging", Name: "Logging", Description: "Log level and format", Icon: ""},
	{ID: "llm", Name: "LLM", Description: "AI provider configuration", Icon: "", SubCategories: []Category{
		{ID: "llm_basic", Name: "Basic Settings", Description: "Provider, API key, and model", Icon: ""},
		{ID: "llm_rate_limit", Name: "Rate Limit", Description: "Request throttling settings", Icon: ""},
		{ID: "llm_circuit_breaker", Name: "Circuit Breaker", Description: "Failure protection settings", Icon: ""},
	}},
}

func (c *Category) HasSubCategories() bool {
	return len(c.SubCategories) > 0
}

func GetCategoryByID(id string) *Category {
	for i := range Categories {
		if Categories[i].ID == id {
			return &Categories[i]
		}
	}
	return nil
}

func GetCategoryNames() []string {
	names := make([]string, len(Categories))
	for i, c := range Categories {
		names[i] = c.Name
	}
	return names
}
</file>
<file path="internal/tui/config_adapter.go">
package tui

import (
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
)

// ConfigValues holds form values that map to Config struct.
// Numeric and duration fields are stored as strings for form editing.
type ConfigValues struct {
	OutputDirectory string
	OutputFlat      bool
	OutputOverwrite bool
	JSONMetadata    bool

	Workers  string
	Timeout  string
	MaxDepth string

	CacheEnabled   bool
	CacheTTL       string
	CacheDirectory string

	ForceJS     bool
	JSTimeout   string
	ScrollToEnd bool

	UserAgent      string
	RandomDelayMin string
	RandomDelayMax string

	LogLevel  string
	LogFormat string

	LLMProvider        string
	LLMAPIKey          string
	LLMBaseURL         string
	LLMModel           string
	LLMMaxTokens       string
	LLMTemperature     string
	LLMTimeout         string
	LLMEnhanceMetadata bool

	ExcludePatterns string

	RateLimitEnabled           bool
	RateLimitRequestsPerMinute string
	RateLimitBurstSize         string
	RateLimitMaxRetries        string
	RateLimitInitialDelay      string
	RateLimitMaxDelay          string
	RateLimitMultiplier        string

	CircuitBreakerEnabled          bool
	CircuitBreakerFailureThreshold string
	CircuitBreakerSuccessThreshold string
	CircuitBreakerResetTimeout     string

	Exclude []string
}

// FromConfig converts a Config to ConfigValues for form editing
func FromConfig(cfg *config.Config) *ConfigValues {
	return &ConfigValues{
		OutputDirectory: cfg.Output.Directory,
		OutputFlat:      cfg.Output.Flat,
		OutputOverwrite: cfg.Output.Overwrite,
		JSONMetadata:    cfg.Output.JSONMetadata,

		Workers:  strconv.Itoa(cfg.Concurrency.Workers),
		Timeout:  formatDuration(cfg.Concurrency.Timeout),
		MaxDepth: strconv.Itoa(cfg.Concurrency.MaxDepth),

		CacheEnabled:   cfg.Cache.Enabled,
		CacheTTL:       formatDuration(cfg.Cache.TTL),
		CacheDirectory: cfg.Cache.Directory,

		ForceJS:     cfg.Rendering.ForceJS,
		JSTimeout:   formatDuration(cfg.Rendering.JSTimeout),
		ScrollToEnd: cfg.Rendering.ScrollToEnd,

		UserAgent:      cfg.Stealth.UserAgent,
		RandomDelayMin: formatDuration(cfg.Stealth.RandomDelayMin),
		RandomDelayMax: formatDuration(cfg.Stealth.RandomDelayMax),

		LogLevel:  cfg.Logging.Level,
		LogFormat: cfg.Logging.Format,

		LLMProvider:        cfg.LLM.Provider,
		LLMAPIKey:          cfg.LLM.APIKey,
		LLMBaseURL:         cfg.LLM.BaseURL,
		LLMModel:           cfg.LLM.Model,
		LLMMaxTokens:       strconv.Itoa(cfg.LLM.MaxTokens),
		LLMTemperature:     strconv.FormatFloat(cfg.LLM.Temperature, 'f', 2, 64),
		LLMTimeout:         formatDuration(cfg.LLM.Timeout),
		LLMEnhanceMetadata: cfg.LLM.EnhanceMetadata,

		ExcludePatterns: strings.Join(cfg.Exclude, "\n"),

		RateLimitEnabled:           cfg.LLM.RateLimit.Enabled,
		RateLimitRequestsPerMinute: strconv.Itoa(cfg.LLM.RateLimit.RequestsPerMinute),
		RateLimitBurstSize:         strconv.Itoa(cfg.LLM.RateLimit.BurstSize),
		RateLimitMaxRetries:        strconv.Itoa(cfg.LLM.RateLimit.MaxRetries),
		RateLimitInitialDelay:      formatDuration(cfg.LLM.RateLimit.InitialDelay),
		RateLimitMaxDelay:          formatDuration(cfg.LLM.RateLimit.MaxDelay),
		RateLimitMultiplier:        strconv.FormatFloat(cfg.LLM.RateLimit.Multiplier, 'f', 2, 64),

		CircuitBreakerEnabled:          cfg.LLM.RateLimit.CircuitBreaker.Enabled,
		CircuitBreakerFailureThreshold: strconv.Itoa(cfg.LLM.RateLimit.CircuitBreaker.FailureThreshold),
		CircuitBreakerSuccessThreshold: strconv.Itoa(cfg.LLM.RateLimit.CircuitBreaker.SuccessThresholdHalfOpen),
		CircuitBreakerResetTimeout:     formatDuration(cfg.LLM.RateLimit.CircuitBreaker.ResetTimeout),

		Exclude: cfg.Exclude,
	}
}

// ToConfig converts ConfigValues back to a Config struct
func (v *ConfigValues) ToConfig() (*config.Config, error) {
	workers, err := parseIntOrDefault(v.Workers, config.DefaultWorkers)
	if err != nil {
		return nil, fmt.Errorf("invalid workers: %w", err)
	}

	maxDepth, err := parseIntOrDefault(v.MaxDepth, config.DefaultMaxDepth)
	if err != nil {
		return nil, fmt.Errorf("invalid max_depth: %w", err)
	}

	timeout, err := parseDurationOrDefault(v.Timeout, config.DefaultTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid timeout: %w", err)
	}

	cacheTTL, err := parseDurationOrDefault(v.CacheTTL, config.DefaultCacheTTL)
	if err != nil {
		return nil, fmt.Errorf("invalid cache_ttl: %w", err)
	}

	jsTimeout, err := parseDurationOrDefault(v.JSTimeout, config.DefaultJSTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid js_timeout: %w", err)
	}

	delayMin, err := parseDurationOrDefault(v.RandomDelayMin, config.DefaultRandomDelayMin)
	if err != nil {
		return nil, fmt.Errorf("invalid random_delay_min: %w", err)
	}

	delayMax, err := parseDurationOrDefault(v.RandomDelayMax, config.DefaultRandomDelayMax)
	if err != nil {
		return nil, fmt.Errorf("invalid random_delay_max: %w", err)
	}

	llmMaxTokens, err := parseIntOrDefault(v.LLMMaxTokens, config.DefaultLLMMaxTokens)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_max_tokens: %w", err)
	}

	llmTemperature, err := parseFloatOrDefault(v.LLMTemperature, config.DefaultLLMTemperature)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_temperature: %w", err)
	}

	llmTimeout, err := parseDurationOrDefault(v.LLMTimeout, 30*time.Second)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_timeout: %w", err)
	}

	rateLimitRequestsPerMinute, err := parseIntOrDefault(v.RateLimitRequestsPerMinute, config.DefaultRateLimitRequestsPerMinute)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_requests_per_minute: %w", err)
	}

	rateLimitBurstSize, err := parseIntOrDefault(v.RateLimitBurstSize, config.DefaultRateLimitBurstSize)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_burst_size: %w", err)
	}

	rateLimitMaxRetries, err := parseIntOrDefault(v.RateLimitMaxRetries, config.DefaultRateLimitMaxRetries)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_max_retries: %w", err)
	}

	rateLimitInitialDelay, err := parseDurationOrDefault(v.RateLimitInitialDelay, config.DefaultRateLimitInitialDelay)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_initial_delay: %w", err)
	}

	rateLimitMaxDelay, err := parseDurationOrDefault(v.RateLimitMaxDelay, config.DefaultRateLimitMaxDelay)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_max_delay: %w", err)
	}

	rateLimitMultiplier, err := parseFloatOrDefault(v.RateLimitMultiplier, config.DefaultRateLimitMultiplier)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_multiplier: %w", err)
	}

	circuitBreakerFailureThreshold, err := parseIntOrDefault(v.CircuitBreakerFailureThreshold, config.DefaultCircuitBreakerFailureThreshold)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_failure_threshold: %w", err)
	}

	circuitBreakerSuccessThreshold, err := parseIntOrDefault(v.CircuitBreakerSuccessThreshold, config.DefaultCircuitBreakerSuccessThresholdHalfOpen)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_success_threshold: %w", err)
	}

	circuitBreakerResetTimeout, err := parseDurationOrDefault(v.CircuitBreakerResetTimeout, config.DefaultCircuitBreakerResetTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_reset_timeout: %w", err)
	}

	excludePatterns := strings.Split(v.ExcludePatterns, "\n")
	var excludeList []string
	for _, pattern := range excludePatterns {
		trimmed := strings.TrimSpace(pattern)
		if trimmed != "" {
			excludeList = append(excludeList, trimmed)
		}
	}

	cfg := &config.Config{
		Output: config.OutputConfig{
			Directory:    v.OutputDirectory,
			Flat:         v.OutputFlat,
			Overwrite:    v.OutputOverwrite,
			JSONMetadata: v.JSONMetadata,
		},
		Concurrency: config.ConcurrencyConfig{
			Workers:  workers,
			Timeout:  timeout,
			MaxDepth: maxDepth,
		},
		Cache: config.CacheConfig{
			Enabled:   v.CacheEnabled,
			TTL:       cacheTTL,
			Directory: v.CacheDirectory,
		},
		Rendering: config.RenderingConfig{
			ForceJS:     v.ForceJS,
			JSTimeout:   jsTimeout,
			ScrollToEnd: v.ScrollToEnd,
		},
		Stealth: config.StealthConfig{
			UserAgent:      v.UserAgent,
			RandomDelayMin: delayMin,
			RandomDelayMax: delayMax,
		},
		Logging: config.LoggingConfig{
			Level:  v.LogLevel,
			Format: v.LogFormat,
		},
		LLM: config.LLMConfig{
			Provider:        v.LLMProvider,
			APIKey:          v.LLMAPIKey,
			BaseURL:         v.LLMBaseURL,
			Model:           v.LLMModel,
			MaxTokens:       llmMaxTokens,
			Temperature:     llmTemperature,
			Timeout:         llmTimeout,
			EnhanceMetadata: v.LLMEnhanceMetadata,
			RateLimit: config.RateLimitConfig{
				Enabled:           v.RateLimitEnabled,
				RequestsPerMinute: rateLimitRequestsPerMinute,
				BurstSize:         rateLimitBurstSize,
				MaxRetries:        rateLimitMaxRetries,
				InitialDelay:      rateLimitInitialDelay,
				MaxDelay:          rateLimitMaxDelay,
				Multiplier:        rateLimitMultiplier,
				CircuitBreaker: config.CircuitBreakerConfig{
					Enabled:                  v.CircuitBreakerEnabled,
					FailureThreshold:         circuitBreakerFailureThreshold,
					SuccessThresholdHalfOpen: circuitBreakerSuccessThreshold,
					ResetTimeout:             circuitBreakerResetTimeout,
				},
			},
		},
		Exclude: excludeList,
	}

	return cfg, nil
}

func formatDuration(d time.Duration) string {
	if d == 0 {
		return ""
	}
	return d.String()
}

func parseDurationOrDefault(s string, defaultVal time.Duration) (time.Duration, error) {
	if s == "" {
		return defaultVal, nil
	}
	return time.ParseDuration(s)
}

func parseIntOrDefault(s string, defaultVal int) (int, error) {
	if s == "" {
		return defaultVal, nil
	}
	return strconv.Atoi(s)
}

func parseFloatOrDefault(s string, defaultVal float64) (float64, error) {
	if s == "" {
		return defaultVal, nil
	}
	return strconv.ParseFloat(s, 64)
}
</file>
<file path="internal/tui/forms.go">
package tui

import (
	"github.com/charmbracelet/huh"
)

func CreateOutputForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("directory").
				Title("Output Directory").
				Description("Where to save extracted documentation").
				Value(&values.OutputDirectory).
				Placeholder("./docs").
				CharLimit(256),

			huh.NewConfirm().
				Key("flat").
				Title("Flat Structure").
				Description("Save all files in a single directory (no subdirectories)").
				Value(&values.OutputFlat),

			huh.NewConfirm().
				Key("overwrite").
				Title("Overwrite Existing").
				Description("Overwrite existing files without prompting").
				Value(&values.OutputOverwrite),

			huh.NewConfirm().
				Key("json_metadata").
				Title("JSON Metadata").
				Description("Generate .json metadata files alongside markdown").
				Value(&values.JSONMetadata),
		),
	).WithTheme(GetTheme())
}

func CreateConcurrencyForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("workers").
				Title("Workers").
				Description("Number of concurrent workers (1-50)").
				Value(&values.Workers).
				Placeholder("5").
				CharLimit(3).
				Validate(ValidateIntRange(1, 50)),

			huh.NewInput().
				Key("timeout").
				Title("Request Timeout").
				Description("HTTP request timeout (e.g., 30s, 1m)").
				Value(&values.Timeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("max_depth").
				Title("Max Crawl Depth").
				Description("Maximum depth for recursive crawling (1-100)").
				Value(&values.MaxDepth).
				Placeholder("4").
				CharLimit(3).
				Validate(ValidateIntRange(1, 100)),
		),
	).WithTheme(GetTheme())
}

func CreateCacheForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Cache").
				Description("Cache fetched pages to reduce network requests").
				Value(&values.CacheEnabled),

			huh.NewInput().
				Key("ttl").
				Title("Cache TTL").
				Description("How long to keep cached pages (e.g., 24h, 7d)").
				Value(&values.CacheTTL).
				Placeholder("24h").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("directory").
				Title("Cache Directory").
				Description("Directory for cache storage").
				Value(&values.CacheDirectory).
				Placeholder("~/.repodocs/cache").
				CharLimit(256),
		),
	).WithTheme(GetTheme())
}

func CreateRenderingForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("force_js").
				Title("Force JavaScript Rendering").
				Description("Always render pages with headless browser").
				Value(&values.ForceJS),

			huh.NewInput().
				Key("js_timeout").
				Title("JS Timeout").
				Description("Timeout for JavaScript rendering (e.g., 10s, 30s)").
				Value(&values.JSTimeout).
				Placeholder("10s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewConfirm().
				Key("scroll_to_end").
				Title("Scroll to End").
				Description("Scroll page to load lazy content").
				Value(&values.ScrollToEnd),
		),
	).WithTheme(GetTheme())
}

func CreateStealthForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("user_agent").
				Title("User Agent").
				Description("Custom User-Agent header (leave empty for default)").
				Value(&values.UserAgent).
				Placeholder("Mozilla/5.0...").
				CharLimit(256),

			huh.NewInput().
				Key("delay_min").
				Title("Min Random Delay").
				Description("Minimum delay between requests (e.g., 100ms, 1s)").
				Value(&values.RandomDelayMin).
				Placeholder("100ms").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("delay_max").
				Title("Max Random Delay").
				Description("Maximum delay between requests (e.g., 500ms, 2s)").
				Value(&values.RandomDelayMax).
				Placeholder("500ms").
				CharLimit(10).
				Validate(ValidateDuration),
		),
	).WithTheme(GetTheme())
}

func CreateLoggingForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewSelect[string]().
				Key("level").
				Title("Log Level").
				Description("Minimum log level to display").
				Options(
					huh.NewOption("Trace", "trace"),
					huh.NewOption("Debug", "debug"),
					huh.NewOption("Info", "info"),
					huh.NewOption("Warn", "warn"),
					huh.NewOption("Error", "error"),
				).
				Value(&values.LogLevel),

			huh.NewSelect[string]().
				Key("format").
				Title("Log Format").
				Description("Output format for logs").
				Options(
					huh.NewOption("Pretty (human-readable)", "pretty"),
					huh.NewOption("JSON (structured)", "json"),
					huh.NewOption("Text (plain)", "text"),
				).
				Value(&values.LogFormat),
		),
	).WithTheme(GetTheme())
}

func CreateLLMForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewSelect[string]().
				Key("provider").
				Title("LLM Provider").
				Description("AI provider for metadata enrichment").
				Options(
					huh.NewOption("None (disabled)", ""),
					huh.NewOption("OpenAI", "openai"),
					huh.NewOption("Anthropic", "anthropic"),
					huh.NewOption("Google", "google"),
				).
				Value(&values.LLMProvider),

			huh.NewInput().
				Key("api_key").
				Title("API Key").
				Description("API key for the selected provider").
				Value(&values.LLMAPIKey).
				EchoMode(huh.EchoModePassword),

			huh.NewInput().
				Key("base_url").
				Title("Base URL").
				Description("Custom API endpoint (leave empty for default)").
				Value(&values.LLMBaseURL).
				Placeholder("https://api.openai.com/v1").
				CharLimit(256),

			huh.NewInput().
				Key("model").
				Title("Model").
				Description("Model name to use").
				Value(&values.LLMModel).
				Placeholder("gpt-4o-mini").
				CharLimit(64),
		),
		huh.NewGroup(
			huh.NewInput().
				Key("max_tokens").
				Title("Max Tokens").
				Description("Maximum tokens for LLM response").
				Value(&values.LLMMaxTokens).
				Placeholder("1000").
				CharLimit(10).
				Validate(ValidatePositiveInt),

			huh.NewInput().
				Key("temperature").
				Title("Temperature").
				Description("Creativity level (0.0-2.0)").
				Value(&values.LLMTemperature).
				Placeholder("0.7").
				CharLimit(10).
				Validate(ValidateFloatRange(0, 2)),

			huh.NewInput().
				Key("timeout").
				Title("LLM Timeout").
				Description("Timeout for LLM requests").
				Value(&values.LLMTimeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewConfirm().
				Key("enhance_metadata").
				Title("Enhance Metadata").
				Description("Use LLM to generate summaries and tags").
				Value(&values.LLMEnhanceMetadata),
		),
	).WithTheme(GetTheme())
}

func CreateExcludeForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewText().
				Key("exclude_patterns").
				Title("Exclude Patterns").
				Description("Regex patterns to exclude (one per line)").
				Value(&values.ExcludePatterns).
				Placeholder(".*\\.pdf$\n.*/login.*"),
		),
	).WithTheme(GetTheme())
}

func CreateRateLimitForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Rate Limiting").
				Description("Enable rate limiting for LLM API calls").
				Value(&values.RateLimitEnabled),

			huh.NewInput().
				Key("requests_per_minute").
				Title("Requests Per Minute").
				Description("Maximum requests per minute (1-1000)").
				Value(&values.RateLimitRequestsPerMinute).
				Placeholder("60").
				CharLimit(4).
				Validate(ValidateIntRange(1, 1000)),

			huh.NewInput().
				Key("burst_size").
				Title("Burst Size").
				Description("Maximum burst requests (1-100)").
				Value(&values.RateLimitBurstSize).
				Placeholder("10").
				CharLimit(3).
				Validate(ValidateIntRange(1, 100)),

			huh.NewInput().
				Key("max_retries").
				Title("Max Retries").
				Description("Maximum retry attempts (0-10)").
				Value(&values.RateLimitMaxRetries).
				Placeholder("3").
				CharLimit(2).
				Validate(ValidateIntRange(0, 10)),
		),
		huh.NewGroup(
			huh.NewInput().
				Key("initial_delay").
				Title("Initial Delay").
				Description("Initial delay between retries (e.g., 1s)").
				Value(&values.RateLimitInitialDelay).
				Placeholder("1s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("max_delay").
				Title("Max Delay").
				Description("Maximum delay between retries (e.g., 1m)").
				Value(&values.RateLimitMaxDelay).
				Placeholder("1m0s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("multiplier").
				Title("Backoff Multiplier").
				Description("Backoff multiplier (1.0-5.0)").
				Value(&values.RateLimitMultiplier).
				Placeholder("2.0").
				CharLimit(10).
				Validate(ValidateFloatRange(1.0, 5.0)),
		),
	).WithTheme(GetTheme())
}

func CreateCircuitBreakerForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Circuit Breaker").
				Description("Enable circuit breaker for LLM API calls").
				Value(&values.CircuitBreakerEnabled),

			huh.NewInput().
				Key("failure_threshold").
				Title("Failure Threshold").
				Description("Failures before opening circuit (1-50)").
				Value(&values.CircuitBreakerFailureThreshold).
				Placeholder("5").
				CharLimit(2).
				Validate(ValidateIntRange(1, 50)),

			huh.NewInput().
				Key("success_threshold").
				Title("Success Threshold").
				Description("Successes in half-open to close (1-10)").
				Value(&values.CircuitBreakerSuccessThreshold).
				Placeholder("1").
				CharLimit(2).
				Validate(ValidateIntRange(1, 10)),

			huh.NewInput().
				Key("reset_timeout").
				Title("Reset Timeout").
				Description("Time before half-open state (e.g., 30s)").
				Value(&values.CircuitBreakerResetTimeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),
		),
	).WithTheme(GetTheme())
}

func GetFormForCategory(category string, values *ConfigValues) *huh.Form {
	switch category {
	case "output":
		return CreateOutputForm(values)
	case "exclude":
		return CreateExcludeForm(values)
	case "concurrency":
		return CreateConcurrencyForm(values)
	case "cache":
		return CreateCacheForm(values)
	case "rendering":
		return CreateRenderingForm(values)
	case "stealth":
		return CreateStealthForm(values)
	case "logging":
		return CreateLoggingForm(values)
	case "llm", "llm_basic":
		return CreateLLMForm(values)
	case "llm_rate_limit":
		return CreateRateLimitForm(values)
	case "llm_circuit_breaker":
		return CreateCircuitBreakerForm(values)
	default:
		return nil
	}
}
</file>
<file path="internal/tui/styles.go">
// Package tui provides an interactive terminal user interface for configuring RepoDocs.
package tui

import (
	"github.com/charmbracelet/huh"
	"github.com/charmbracelet/lipgloss"
)

var (
	// Theme colors
	primaryColor = lipgloss.AdaptiveColor{Light: "#5A56E0", Dark: "#7571F9"}
	successColor = lipgloss.AdaptiveColor{Light: "#02BA84", Dark: "#02BF87"}
	errorColor   = lipgloss.AdaptiveColor{Light: "#FE5F86", Dark: "#FE5F86"}
	mutedColor   = lipgloss.AdaptiveColor{Light: "#9B9B9B", Dark: "#5C5C5C"}
	warnColor    = lipgloss.AdaptiveColor{Light: "#FF9500", Dark: "#FFAA33"}

	// TitleStyle is used for main headers
	TitleStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(primaryColor).
			MarginBottom(1)

	// SubtitleStyle is used for section headers
	SubtitleStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(mutedColor)

	// DescriptionStyle is used for help text
	DescriptionStyle = lipgloss.NewStyle().
				Foreground(mutedColor)

	// SuccessStyle is used for success messages
	SuccessStyle = lipgloss.NewStyle().
			Foreground(successColor)

	// ErrorStyle is used for error messages
	ErrorStyle = lipgloss.NewStyle().
			Foreground(errorColor)

	// WarnStyle is used for warning messages
	WarnStyle = lipgloss.NewStyle().
			Foreground(warnColor)

	// BoxStyle is used for bordered containers
	BoxStyle = lipgloss.NewStyle().
			Border(lipgloss.RoundedBorder()).
			BorderForeground(primaryColor).
			Padding(1, 2)

	// SelectedStyle is used for highlighted menu items
	SelectedStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(primaryColor)

	// UnselectedStyle is used for normal menu items
	UnselectedStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("252"))

	// HelpStyle is used for keyboard shortcut hints
	HelpStyle = lipgloss.NewStyle().
			Foreground(mutedColor).
			MarginTop(1)
)

// GetTheme returns the huh theme for forms
func GetTheme() *huh.Theme {
	return huh.ThemeCharm()
}

// GetAccessibleTheme returns an accessible theme for screen readers
func GetAccessibleTheme() *huh.Theme {
	return huh.ThemeBase()
}
</file>
<file path="internal/tui/validation.go">
package tui

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"time"
)

// Validation error messages
var (
	ErrRequired      = errors.New("this field is required")
	ErrInvalidNumber = errors.New("must be a valid number")
	ErrPositiveInt   = errors.New("must be a positive integer")
	ErrInvalidRange  = errors.New("value out of valid range")
)

// ValidateRequired ensures a string value is not empty
func ValidateRequired(s string) error {
	if strings.TrimSpace(s) == "" {
		return ErrRequired
	}
	return nil
}

// ValidateDuration validates that a string can be parsed as a time.Duration
func ValidateDuration(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil // Empty is valid (will use default)
	}
	_, err := time.ParseDuration(s)
	if err != nil {
		return fmt.Errorf("invalid duration format (use: 30s, 5m, 1h): %w", err)
	}
	return nil
}

// ValidatePositiveInt validates that a string represents a positive integer
func ValidatePositiveInt(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil // Empty is valid (will use default)
	}
	n, err := strconv.Atoi(s)
	if err != nil {
		return ErrInvalidNumber
	}
	if n < 1 {
		return ErrPositiveInt
	}
	return nil
}

// ValidateIntRange validates that a string represents an integer within a range
func ValidateIntRange(min, max int) func(string) error {
	return func(s string) error {
		if strings.TrimSpace(s) == "" {
			return nil
		}
		n, err := strconv.Atoi(s)
		if err != nil {
			return ErrInvalidNumber
		}
		if n < min || n > max {
			return fmt.Errorf("%w: must be between %d and %d", ErrInvalidRange, min, max)
		}
		return nil
	}
}

// ValidateFloat validates that a string represents a valid float64
func ValidateFloat(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil
	}
	_, err := strconv.ParseFloat(s, 64)
	if err != nil {
		return fmt.Errorf("must be a valid decimal number")
	}
	return nil
}

// ValidateFloatRange validates that a string represents a float within a range
func ValidateFloatRange(min, max float64) func(string) error {
	return func(s string) error {
		if strings.TrimSpace(s) == "" {
			return nil
		}
		n, err := strconv.ParseFloat(s, 64)
		if err != nil {
			return fmt.Errorf("must be a valid decimal number")
		}
		if n < min || n > max {
			return fmt.Errorf("%w: must be between %.2f and %.2f", ErrInvalidRange, min, max)
		}
		return nil
	}
}

// ValidateLogLevel validates log level values
func ValidateLogLevel(s string) error {
	validLevels := map[string]bool{
		"trace": true,
		"debug": true,
		"info":  true,
		"warn":  true,
		"error": true,
		"fatal": true,
		"panic": true,
	}
	if !validLevels[strings.ToLower(s)] {
		return fmt.Errorf("invalid log level: must be one of trace, debug, info, warn, error, fatal, panic")
	}
	return nil
}

// ValidateLogFormat validates log format values
func ValidateLogFormat(s string) error {
	validFormats := map[string]bool{
		"json":   true,
		"pretty": true,
		"text":   true,
	}
	if !validFormats[strings.ToLower(s)] {
		return fmt.Errorf("invalid log format: must be json, pretty, or text")
	}
	return nil
}

// ValidateLLMProvider validates LLM provider values
func ValidateLLMProvider(s string) error {
	if s == "" {
		return nil // Empty is valid (LLM disabled)
	}
	validProviders := map[string]bool{
		"openai":    true,
		"anthropic": true,
		"google":    true,
	}
	if !validProviders[strings.ToLower(s)] {
		return fmt.Errorf("invalid LLM provider: must be openai, anthropic, or google")
	}
	return nil
}
</file>
<file path="internal/utils/fs.go">
package utils

import (
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"unicode"
)

// MaxFilenameLength is the maximum length for a filename
const MaxFilenameLength = 200

// Windows reserved names
var windowsReserved = map[string]bool{
	"CON": true, "PRN": true, "AUX": true, "NUL": true,
	"COM1": true, "COM2": true, "COM3": true, "COM4": true,
	"COM5": true, "COM6": true, "COM7": true, "COM8": true, "COM9": true,
	"LPT1": true, "LPT2": true, "LPT3": true, "LPT4": true,
	"LPT5": true, "LPT6": true, "LPT7": true, "LPT8": true, "LPT9": true,
}

// invalidCharsRegex matches invalid filename characters
var invalidCharsRegex = regexp.MustCompile(`[<>:"|?*\\/]`)

// multipleSpacesRegex matches multiple consecutive spaces/dashes
var multipleSpacesRegex = regexp.MustCompile(`[-_\s]+`)

// SanitizeFilename sanitizes a string for use as a filename
func SanitizeFilename(name string) string {
	original := name

	// Remove invalid characters
	name = invalidCharsRegex.ReplaceAllString(name, "-")

	// Replace multiple spaces/dashes with single dash
	name = multipleSpacesRegex.ReplaceAllString(name, "-")

	// Separate extension from base name
	ext := filepath.Ext(name)
	baseName := strings.TrimSuffix(name, ext)

	// Trim leading/trailing dashes and spaces from base name
	baseName = strings.Trim(baseName, "- ")

	// Check if we had invalid character substitutions
	// If original had invalid chars that created dashes before extension,
	// and the extension exists, preserve one dash before extension
	hadSubstitutions := (original != name) && invalidCharsRegex.MatchString(original)
	if hadSubstitutions && ext != "" && strings.HasSuffix(name, "-."+ext[1:]) {
		// Reconstruct with dash before extension
		name = baseName + "-" + ext
	} else {
		// Reconstruct normally
		if ext != "" {
			name = baseName + ext
		} else {
			name = baseName
		}
	}

	// Check for Windows reserved names
	upper := strings.ToUpper(name)
	baseNameUpper := strings.TrimSuffix(upper, filepath.Ext(upper))
	if windowsReserved[baseNameUpper] {
		name = "_" + name
	}

	// Limit length
	if len(name) > MaxFilenameLength {
		ext := filepath.Ext(name)
		name = name[:MaxFilenameLength-len(ext)] + ext
	}

	// Ensure the name is not empty
	if name == "" {
		name = "untitled"
	}

	return name
}

// URLToFilename converts a URL to a safe filename
func URLToFilename(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return SanitizeFilename(rawURL)
	}

	// Get path and remove leading/trailing slashes
	path := strings.Trim(u.Path, "/")
	if path == "" {
		path = "index"
	}

	// Replace path separators with dashes for flat structure
	path = strings.ReplaceAll(path, "/", "-")

	// Remove common file extensions
	path = strings.TrimSuffix(path, ".html")
	path = strings.TrimSuffix(path, ".htm")
	path = strings.TrimSuffix(path, ".php")

	// Sanitize and add .md extension
	filename := SanitizeFilename(path)
	if !strings.HasSuffix(filename, ".md") {
		filename += ".md"
	}

	return filename
}

// URLToPath converts a URL to a nested directory path
func URLToPath(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return SanitizeFilename(rawURL) + ".md"
	}

	// Get path and remove leading/trailing slashes
	path := strings.Trim(u.Path, "/")
	if path == "" {
		path = "index"
	}

	// Remove common file extensions
	path = strings.TrimSuffix(path, ".html")
	path = strings.TrimSuffix(path, ".htm")
	path = strings.TrimSuffix(path, ".php")

	// Split path and sanitize each component
	parts := strings.Split(path, "/")
	for i, part := range parts {
		parts[i] = SanitizeFilename(part)
	}

	// Join with OS-specific separator
	result := filepath.Join(parts...)

	// Add .md extension if not present
	if !strings.HasSuffix(result, ".md") {
		result += ".md"
	}

	return result
}

// GeneratePath generates the output path for a URL
func GeneratePath(baseDir, rawURL string, flat bool) string {
	var relativePath string
	if flat {
		relativePath = URLToFilename(rawURL)
	} else {
		relativePath = URLToPath(rawURL)
	}
	return filepath.Join(baseDir, relativePath)
}

// GeneratePathFromRelative generates the output path from a relative file path
// Used for Git-sourced files to preserve the repository's directory structure
func GeneratePathFromRelative(baseDir, relPath string, flat bool) string {
	if flat {
		// For flat mode, convert full path to filename by replacing "/" with "-"
		// Example: docs/developers/tools/memory.md → docs-developers-tools-memory.md

		// Normalize separators to forward slash
		normalized := filepath.ToSlash(relPath)

		// Remove .md/.mdx extension if present
		ext := filepath.Ext(normalized)
		if ext == ".md" || ext == ".mdx" {
			normalized = strings.TrimSuffix(normalized, ext)
		}

		// Replace "/" with "-" to create flat filename
		flatName := strings.ReplaceAll(normalized, "/", "-")

		// Sanitize the result
		flatName = SanitizeFilename(flatName)

		// Add .md extension
		if !strings.HasSuffix(flatName, ".md") {
			flatName += ".md"
		}

		return filepath.Join(baseDir, flatName)
	}

	// For nested mode, preserve directory structure
	// Ensure path uses OS-specific separators
	relPath = filepath.FromSlash(relPath)

	// Sanitize each component
	parts := strings.Split(relPath, string(filepath.Separator))
	for i, part := range parts {
		parts[i] = SanitizeFilename(part)
	}
	result := filepath.Join(parts...)

	// Add .md extension if not present
	if !strings.HasSuffix(result, ".md") {
		result += ".md"
	}

	return filepath.Join(baseDir, result)
}

// JSONPath returns the corresponding JSON metadata path for a markdown file
func JSONPath(mdPath string) string {
	return strings.TrimSuffix(mdPath, ".md") + ".json"
}

// IsValidFilename checks if a filename is valid
func IsValidFilename(name string) bool {
	if name == "" || name == "." || name == ".." {
		return false
	}

	// Check for invalid characters
	if invalidCharsRegex.MatchString(name) {
		return false
	}

	// Check for Windows reserved names
	upper := strings.ToUpper(name)
	baseName := strings.TrimSuffix(upper, filepath.Ext(upper))
	if windowsReserved[baseName] {
		return false
	}

	// Check for control characters
	for _, r := range name {
		if unicode.IsControl(r) {
			return false
		}
	}

	return true
}

// EnsureDir ensures a directory exists, creating it if necessary
func EnsureDir(path string) error {
	dir := filepath.Dir(path)
	return os.MkdirAll(dir, 0755)
}

// ExpandPath expands ~ to the user's home directory
func ExpandPath(path string) string {
	if strings.HasPrefix(path, "~/") {
		home, err := os.UserHomeDir()
		if err != nil {
			return path
		}
		return filepath.Join(home, path[2:])
	}
	if path == "~" {
		home, err := os.UserHomeDir()
		if err != nil {
			return path
		}
		return home
	}
	return path
}
</file>
<file path="internal/utils/logger.go">
package utils

import (
	"io"
	"os"
	"time"

	"github.com/rs/zerolog"
)

// Logger is a wrapper around zerolog.Logger
type Logger struct {
	zerolog.Logger
}

// LoggerOptions contains options for creating a logger
type LoggerOptions struct {
	Level   string
	Format  string // "pretty" or "json"
	Output  io.Writer
	Verbose bool
}

// NewLogger creates a new logger with the given options
func NewLogger(opts LoggerOptions) *Logger {
	var output io.Writer = os.Stderr
	if opts.Output != nil {
		output = opts.Output
	}

	// Set up pretty or JSON output
	if opts.Format == "pretty" {
		output = zerolog.ConsoleWriter{
			Out:        output,
			TimeFormat: time.RFC3339,
		}
	}

	// Parse log level
	level := parseLogLevel(opts.Level)
	if opts.Verbose {
		level = zerolog.DebugLevel
	}

	// Create logger
	logger := zerolog.New(output).
		Level(level).
		With().
		Timestamp().
		Logger()

	return &Logger{Logger: logger}
}

// NewDefaultLogger creates a logger with default settings
func NewDefaultLogger() *Logger {
	return NewLogger(LoggerOptions{
		Level:  "info",
		Format: "pretty",
	})
}

// NewVerboseLogger creates a verbose logger
func NewVerboseLogger() *Logger {
	return NewLogger(LoggerOptions{
		Level:   "debug",
		Format:  "pretty",
		Verbose: true,
	})
}

// parseLogLevel parses a log level string
func parseLogLevel(level string) zerolog.Level {
	switch level {
	case "debug":
		return zerolog.DebugLevel
	case "info":
		return zerolog.InfoLevel
	case "warn":
		return zerolog.WarnLevel
	case "error":
		return zerolog.ErrorLevel
	default:
		return zerolog.InfoLevel
	}
}

// WithComponent returns a logger with a component field
func (l *Logger) WithComponent(component string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("component", component).Logger(),
	}
}

// WithURL returns a logger with a URL field
func (l *Logger) WithURL(url string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("url", url).Logger(),
	}
}

// WithStrategy returns a logger with a strategy field
func (l *Logger) WithStrategy(strategy string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("strategy", strategy).Logger(),
	}
}

// SetGlobalLevel sets the global log level
func SetGlobalLevel(level string) {
	zerolog.SetGlobalLevel(parseLogLevel(level))
}
</file>
<file path="internal/utils/progress.go">
package utils

import "github.com/schollz/progressbar/v3"

// Standard progress bar descriptions
const (
	DescCrawling    = "Crawling"
	DescDownloading = "Downloading"
	DescProcessing  = "Processing"
	DescExtracting  = "Extracting"
)

// NewProgressBar creates a consistently styled progress bar.
//
// Parameters:
//   - total: Total number of items. Use -1 for unknown totals (indeterminate/spinner mode).
//   - description: Text description to show before the progress bar (e.g., DescCrawling, DescDownloading).
//
// Behavior:
//   - For unknown totals (total < 0): Uses spinner type 14 with blank state rendering.
//   - For known totals (total >= 0): Shows count and iterations/second (its).
//   - All progress bars show count.
//
// Example:
//
//	bar := utils.NewProgressBar(len(items), utils.DescDownloading)
//	defer bar.Finish()
//
//	for _, item := range items {
//	    // Process item
//	    bar.Add(1)
//	}
func NewProgressBar(total int, description string) *progressbar.ProgressBar {
	// Build common options
	opts := []progressbar.Option{
		progressbar.OptionSetDescription(description),
		progressbar.OptionShowCount(),
	}

	// Add options based on whether total is known
	if total < 0 {
		// Unknown total: use spinner mode
		opts = append(opts,
			progressbar.OptionSpinnerType(14),
			progressbar.OptionSetRenderBlankState(true),
		)
	} else {
		// Known total: show iterations/second
		opts = append(opts,
			progressbar.OptionShowIts(),
		)
	}

	return progressbar.NewOptions(total, opts...)
}
</file>
<file path="internal/utils/url.go">
package utils

import (
	"net/url"
	"path"
	"regexp"
	"strings"
)

// NormalizeURL normalizes a URL for consistent handling
func NormalizeURL(rawURL string) (string, error) {
	// If no scheme is present, prepend https:// before parsing
	// This ensures the host is correctly identified
	if !strings.Contains(rawURL, "://") && !strings.HasPrefix(rawURL, "//") {
		rawURL = "https://" + rawURL
	}

	u, err := url.Parse(rawURL)
	if err != nil {
		return "", err
	}

	// Ensure scheme
	if u.Scheme == "" {
		u.Scheme = "https"
	}

	// Normalize host to lowercase
	u.Host = strings.ToLower(u.Host)

	// Remove default ports
	if (u.Scheme == "http" && u.Port() == "80") ||
		(u.Scheme == "https" && u.Port() == "443") {
		u.Host = u.Hostname()
	}

	// Clean path
	if u.Path == "" {
		u.Path = "/"
	} else {
		u.Path = path.Clean(u.Path)
	}

	// Remove trailing slash (except for root)
	if u.Path != "/" && strings.HasSuffix(u.Path, "/") {
		u.Path = strings.TrimSuffix(u.Path, "/")
	}

	// Remove fragment
	u.Fragment = ""

	// Build the result manually to ensure trailing slash for root path
	result := u.String()

	// Ensure root path has trailing slash
	if u.Path == "/" && u.RawQuery == "" && !strings.HasSuffix(result, "/") {
		result += "/"
	}

	return result, nil
}

// NormalizeURLWithoutQuery normalizes a URL and removes query parameters
func NormalizeURLWithoutQuery(rawURL string) (string, error) {
	normalized, err := NormalizeURL(rawURL)
	if err != nil {
		return "", err
	}

	u, err := url.Parse(normalized)
	if err != nil {
		return "", err
	}

	u.RawQuery = ""
	return u.String(), nil
}

// ResolveURL resolves a relative URL against a base URL
func ResolveURL(base, ref string) (string, error) {
	// If the base doesn't end with / and doesn't have a file extension,
	// treat it as a directory by adding a trailing slash
	// This ensures that "../page" from "/docs/api" resolves to "/docs/page" not "/page"
	if !strings.HasSuffix(base, "/") && !strings.Contains(path.Base(base), ".") {
		base += "/"
	}

	baseURL, err := url.Parse(base)
	if err != nil {
		return "", err
	}

	refURL, err := url.Parse(ref)
	if err != nil {
		return "", err
	}

	resolved := baseURL.ResolveReference(refURL)
	return resolved.String(), nil
}

// GetDomain extracts the domain from a URL
func GetDomain(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return ""
	}
	return u.Host
}

// GetBaseDomain extracts the base domain from a URL, removing only the "www" prefix
// For example: "www.example.com" -> "example.com", "docs.example.com" -> "docs.example.com"
func GetBaseDomain(rawURL string) string {
	host := GetDomain(rawURL)
	if host == "" {
		return ""
	}

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	// Only strip "www." prefix, keep other subdomains
	if strings.HasPrefix(strings.ToLower(host), "www.") {
		return host[4:]
	}

	return host
}

// extractRootDomain extracts the root domain (domain + TLD) without any subdomains
// For example: "docs.example.com" -> "example.com", "www.example.com" -> "example.com"
func extractRootDomain(rawURL string) string {
	host := GetDomain(rawURL)
	if host == "" {
		return ""
	}

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	// Split by dots
	parts := strings.Split(strings.ToLower(host), ".")

	// Need at least 2 parts (domain + TLD)
	if len(parts) < 2 {
		return host
	}

	// Extract the last 2 parts (domain + TLD)
	// This works for most TLDs like .com, .org, .net
	// Note: This is a simplified approach and won't work perfectly for
	// compound TLDs like .co.uk, but covers the common cases
	return strings.Join(parts[len(parts)-2:], ".")
}

// IsSameDomain checks if two URLs have the same domain
func IsSameDomain(url1, url2 string) bool {
	return strings.EqualFold(GetDomain(url1), GetDomain(url2))
}

// IsSameBaseDomain checks if two URLs have the same base domain (ignoring subdomains)
// For example: "docs.example.com" and "api.example.com" return true
func IsSameBaseDomain(url1, url2 string) bool {
	return extractRootDomain(url1) == extractRootDomain(url2)
}

// IsAbsoluteURL checks if a URL is absolute
func IsAbsoluteURL(rawURL string) bool {
	// Protocol-relative URLs (starting with //) are considered absolute
	if strings.HasPrefix(rawURL, "//") {
		return true
	}

	u, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	return u.IsAbs()
}

// IsHTTPURL checks if a URL uses HTTP or HTTPS scheme
func IsHTTPURL(rawURL string) bool {
	u, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	return u.Scheme == "http" || u.Scheme == "https"
}

// IsGitURL checks if a URL is a git repository URL
func IsGitURL(rawURL string) bool {
	return strings.HasPrefix(rawURL, "git@") ||
		strings.HasSuffix(rawURL, ".git") ||
		strings.Contains(rawURL, "github.com") ||
		strings.Contains(rawURL, "gitlab.com") ||
		strings.Contains(rawURL, "bitbucket.org")
}

// IsSitemapURL checks if a URL points to a sitemap
func IsSitemapURL(rawURL string) bool {
	lower := strings.ToLower(rawURL)
	return strings.HasSuffix(lower, "sitemap.xml") ||
		strings.HasSuffix(lower, "sitemap.xml.gz") ||
		strings.Contains(lower, "sitemap")
}

// IsLLMSURL checks if a URL points to an llms.txt file
func IsLLMSURL(rawURL string) bool {
	lower := strings.ToLower(rawURL)
	return strings.HasSuffix(lower, "/llms.txt") ||
		strings.HasSuffix(lower, "llms.txt")
}

// IsPkgGoDevURL checks if a URL is a pkg.go.dev URL
func IsPkgGoDevURL(rawURL string) bool {
	return strings.Contains(rawURL, "pkg.go.dev")
}

// ExtractLinks extracts all href links from HTML content
// This is a simple regex-based extraction, use goquery for more robust parsing
func ExtractLinks(html, baseURL string) []string {
	linkRegex := regexp.MustCompile(`href=["']([^"']+)["']`)
	matches := linkRegex.FindAllStringSubmatch(html, -1)

	links := make([]string, 0, len(matches))
	for _, match := range matches {
		if len(match) > 1 {
			link := match[1]
			// Skip anchors, javascript, mailto, etc.
			if strings.HasPrefix(link, "#") ||
				strings.HasPrefix(link, "javascript:") ||
				strings.HasPrefix(link, "mailto:") ||
				strings.HasPrefix(link, "tel:") {
				continue
			}

			// Resolve relative URLs
			if !IsAbsoluteURL(link) {
				resolved, err := ResolveURL(baseURL, link)
				if err != nil {
					continue
				}
				link = resolved
			}

			links = append(links, link)
		}
	}

	return links
}

// GenerateOutputDirFromURL generates an output directory name from a URL
// Examples:
//   - https://github.com/QwenLM/qwen-code -> docs_qwen-code
//   - https://docs.crawl4ai.com/sitemap.xml -> docs_docscrawl4aicom
//   - https://docs.factory.ai/llms.txt -> docs_docsfactoryai
//   - https://pkg.go.dev/github.com/user/package -> docs_package
//   - https://docs.rs/ratatui/latest/ratatui/ -> docs_ratatui
func GenerateOutputDirFromURL(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return "docs"
	}

	host := strings.ToLower(u.Host)
	pathStr := strings.Trim(u.Path, "/")

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	var name string

	// Handle Git repository URLs (GitHub, GitLab, Bitbucket)
	if strings.Contains(host, "github.com") ||
		strings.Contains(host, "gitlab.com") ||
		strings.Contains(host, "bitbucket.org") {
		// Extract repository name from path
		parts := strings.Split(pathStr, "/")
		if len(parts) >= 2 {
			// Use the repo name (second part: owner/repo)
			name = parts[1]
			// Remove .git suffix if present
			name = strings.TrimSuffix(name, ".git")
		} else if len(parts) == 1 && parts[0] != "" {
			name = parts[0]
		}
	}

	// Handle pkg.go.dev URLs
	if name == "" && strings.Contains(host, "pkg.go.dev") {
		// Path is like: /github.com/user/package or /package
		parts := strings.Split(pathStr, "/")
		if len(parts) > 0 {
			// Use the last significant part
			for i := len(parts) - 1; i >= 0; i-- {
				if parts[i] != "" && !strings.Contains(parts[i], ".") {
					name = parts[i]
					break
				}
			}
			// Fallback to last part
			if name == "" && len(parts) > 0 {
				name = parts[len(parts)-1]
			}
		}
	}

	// Handle docs.rs URLs
	if name == "" && strings.Contains(host, "docs.rs") {
		parts := strings.Split(pathStr, "/")
		if len(parts) >= 1 && parts[0] != "" {
			if parts[0] == "crate" && len(parts) >= 2 {
				name = parts[1]
			} else {
				name = parts[0]
			}
		}
	}

	// For other URLs, use sanitized hostname
	if name == "" {
		// Remove common prefixes
		host = strings.TrimPrefix(host, "www.")

		// Concatenate all parts of the domain (removes dots)
		// example.com -> examplecom
		// docs.crawl4ai.com -> docscrawl4aicom
		// This creates a clean directory name from the full domain
		name = sanitizeForDirName(host)
	}

	// Ensure we have a valid name
	if name == "" {
		return "docs"
	}

	// Sanitize the name for filesystem
	name = sanitizeForDirName(name)

	return "docs_" + name
}

// sanitizeForDirName removes characters that are not safe for directory names
func sanitizeForDirName(s string) string {
	// Remove dots, spaces, and special characters
	var result strings.Builder
	for _, r := range strings.ToLower(s) {
		if (r >= 'a' && r <= 'z') || (r >= '0' && r <= '9') || r == '-' || r == '_' {
			result.WriteRune(r)
		}
	}
	return result.String()
}

// HasBaseURL checks if a URL starts with the given base URL path
// Example: HasBaseURL("https://example.com/docs/api", "https://example.com/docs") returns true
// Example: HasBaseURL("https://example.com/blog", "https://example.com/docs") returns false
func HasBaseURL(targetURL, baseURL string) bool {
	if baseURL == "" {
		return true
	}

	targetParsed, err := url.Parse(targetURL)
	if err != nil {
		return false
	}

	baseParsed, err := url.Parse(baseURL)
	if err != nil {
		return false
	}

	// Must be same host
	if strings.ToLower(targetParsed.Host) != strings.ToLower(baseParsed.Host) {
		return false
	}

	// Normalize paths
	targetPath := strings.TrimSuffix(targetParsed.Path, "/")
	basePath := strings.TrimSuffix(baseParsed.Path, "/")

	// Target path must start with base path
	if basePath == "" || basePath == "/" {
		return true
	}

	return targetPath == basePath || strings.HasPrefix(targetPath, basePath+"/")
}

// FilterLinks filters links based on patterns
func FilterLinks(links []string, excludePatterns []string) []string {
	var regexps []*regexp.Regexp
	for _, pattern := range excludePatterns {
		re, err := regexp.Compile(pattern)
		if err != nil {
			continue
		}
		regexps = append(regexps, re)
	}

	filtered := make([]string, 0, len(links))
	for _, link := range links {
		excluded := false
		for _, re := range regexps {
			if re.MatchString(link) {
				excluded = true
				break
			}
		}
		if !excluded {
			filtered = append(filtered, link)
		}
	}

	return filtered
}
</file>
<file path="internal/utils/workerpool.go">
package utils

import (
	"context"
	"sync"
)

// Task represents a unit of work
type Task[T any] struct {
	Data   T
	Result any
	Err    error
}

// Worker is a function that processes a task
type Worker[T any] func(ctx context.Context, data T) (any, error)

// Pool is a worker pool for concurrent task processing
type Pool[T any] struct {
	workers    int
	taskQueue  chan *Task[T]
	resultChan chan *Task[T]
	wg         sync.WaitGroup
	worker     Worker[T]
	stopOnce   sync.Once
}

// NewPool creates a new worker pool
func NewPool[T any](workers int, worker Worker[T]) *Pool[T] {
	return &Pool[T]{
		workers:    workers,
		taskQueue:  make(chan *Task[T], workers*2),
		resultChan: make(chan *Task[T], workers*2),
		worker:     worker,
	}
}

// Start starts the worker pool
func (p *Pool[T]) Start(ctx context.Context) {
	for i := 0; i < p.workers; i++ {
		p.wg.Add(1)
		go p.runWorker(ctx)
	}
}

// runWorker runs a single worker
func (p *Pool[T]) runWorker(ctx context.Context) {
	defer p.wg.Done()

	for {
		select {
		case <-ctx.Done():
			return
		case task, ok := <-p.taskQueue:
			if !ok {
				return
			}
			result, err := p.worker(ctx, task.Data)
			task.Result = result
			task.Err = err

			select {
			case p.resultChan <- task:
			case <-ctx.Done():
				return
			}
		}
	}
}

// Submit submits a task to the pool
func (p *Pool[T]) Submit(data T) {
	p.taskQueue <- &Task[T]{Data: data}
}

// Results returns the results channel
func (p *Pool[T]) Results() <-chan *Task[T] {
	return p.resultChan
}

// Stop stops the pool and waits for workers to finish
func (p *Pool[T]) Stop() {
	p.stopOnce.Do(func() {
		close(p.taskQueue)
		p.wg.Wait()
		close(p.resultChan)
	})
}

// Process processes a slice of data items concurrently
func (p *Pool[T]) Process(ctx context.Context, items []T) ([]*Task[T], error) {
	// Handle empty slice case
	if len(items) == 0 {
		return []*Task[T]{}, nil
	}

	p.Start(ctx)

	// Submit all items
	go func() {
		for _, item := range items {
			select {
			case <-ctx.Done():
				return
			default:
				p.Submit(item)
			}
		}
		close(p.taskQueue)
	}()

	// Collect results with context awareness
	results := make([]*Task[T], 0, len(items))
	collectDone := false
	for !collectDone {
		select {
		case <-ctx.Done():
			collectDone = true
		case task, ok := <-p.resultChan:
			if !ok {
				collectDone = true
			} else {
				results = append(results, task)
				if len(results) == len(items) {
					collectDone = true
				}
			}
		}
	}

	p.wg.Wait()

	// Drain remaining results to avoid goroutine leak
	go func() {
		for range p.resultChan {
		}
	}()
	close(p.resultChan)

	// Check for context error
	if ctx.Err() != nil {
		return results, ctx.Err()
	}

	return results, nil
}

// SimplePool is a simpler worker pool without generics for basic use cases
type SimplePool struct {
	workers int
	wg      sync.WaitGroup
}

// NewSimplePool creates a new simple worker pool
func NewSimplePool(workers int) *SimplePool {
	return &SimplePool{workers: workers}
}

// Run runs tasks concurrently with the given function
func (p *SimplePool) Run(ctx context.Context, tasks []func(context.Context) error) []error {
	errors := make([]error, len(tasks))
	taskChan := make(chan int, len(tasks))
	var mu sync.Mutex

	// Start workers
	for i := 0; i < p.workers; i++ {
		p.wg.Add(1)
		go func() {
			defer p.wg.Done()
			for {
				select {
				case <-ctx.Done():
					return
				case idx, ok := <-taskChan:
					if !ok {
						return
					}
					err := tasks[idx](ctx)
					mu.Lock()
					errors[idx] = err
					mu.Unlock()
				}
			}
		}()
	}

	// Submit tasks
	for i := range tasks {
		select {
		case <-ctx.Done():
			close(taskChan)
			p.wg.Wait()
			return errors
		case taskChan <- i:
		}
	}

	close(taskChan)
	p.wg.Wait()

	return errors
}

// ParallelForEach executes a function for each item in parallel
func ParallelForEach[T any](ctx context.Context, items []T, workers int, fn func(context.Context, T) error) []error {
	if workers <= 0 {
		workers = 1
	}
	if workers > len(items) {
		workers = len(items)
	}

	errors := make([]error, len(items))
	taskChan := make(chan int, len(items))
	var wg sync.WaitGroup
	var mu sync.Mutex

	// Start workers
	for i := 0; i < workers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for {
				select {
				case <-ctx.Done():
					return
				case idx, ok := <-taskChan:
					if !ok {
						return
					}
					err := fn(ctx, items[idx])
					mu.Lock()
					errors[idx] = err
					mu.Unlock()
				}
			}
		}()
	}

	// Submit tasks
	for i := range items {
		select {
		case <-ctx.Done():
			close(taskChan)
			wg.Wait()
			return errors
		case taskChan <- i:
		}
	}

	close(taskChan)
	wg.Wait()

	return errors
}

// FirstError returns the first non-nil error from a slice of errors
func FirstError(errors []error) error {
	for _, err := range errors {
		if err != nil {
			return err
		}
	}
	return nil
}

// CollectErrors collects all non-nil errors from a slice
func CollectErrors(errors []error) []error {
	var result []error
	for _, err := range errors {
		if err != nil {
			result = append(result, err)
		}
	}
	return result
}
</file>
<file path="pkg/version/version.go">
package version

import (
	"fmt"
	"runtime"
)

// Build-time variables (set via ldflags)
var (
	Version   = "dev"
	BuildTime = "unknown"
	Commit    = "unknown"
)

// Info contains version information
type Info struct {
	Version   string `json:"version"`
	BuildTime string `json:"build_time"`
	Commit    string `json:"commit"`
	GoVersion string `json:"go_version"`
	OS        string `json:"os"`
	Arch      string `json:"arch"`
}

// Get returns the current version info
func Get() Info {
	return Info{
		Version:   Version,
		BuildTime: BuildTime,
		Commit:    Commit,
		GoVersion: runtime.Version(),
		OS:        runtime.GOOS,
		Arch:      runtime.GOARCH,
	}
}

// String returns a formatted version string
func (i Info) String() string {
	return fmt.Sprintf("repodocs %s (commit: %s, built: %s, %s %s/%s)",
		i.Version, i.Commit, i.BuildTime, i.GoVersion, i.OS, i.Arch)
}

// Short returns a short version string
func Short() string {
	return Version
}

// Full returns a full version string
func Full() string {
	return Get().String()
}
</file>
<file path="scripts/release.sh">
#!/usr/bin/env bash
set -euo pipefail

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
DIM='\033[2m'
NC='\033[0m'

cd "$(dirname "${BASH_SOURCE[0]}")/.."

LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "v0.0.0")
VERSION="${LATEST_TAG#v}"
IFS='.' read -r MAJOR MINOR PATCH <<< "$VERSION"

NEXT_PATCH="v${MAJOR}.${MINOR}.$((PATCH + 1))"
NEXT_MINOR="v${MAJOR}.$((MINOR + 1)).0"
NEXT_MAJOR="v$((MAJOR + 1)).0.0"

echo ""
echo -e "Current version: ${GREEN}${LATEST_TAG}${NC}"
echo ""
echo -e "  ${CYAN}1)${NC} patch  → ${GREEN}${NEXT_PATCH}${NC}"
echo -e "  ${CYAN}2)${NC} minor  → ${GREEN}${NEXT_MINOR}${NC}"
echo -e "  ${CYAN}3)${NC} major  → ${GREEN}${NEXT_MAJOR}${NC}"
echo -e "  ${CYAN}4)${NC} custom"
echo -e "  ${CYAN}q)${NC} quit"
echo ""

read -rp "Select [1-4/q]: " choice

case $choice in
    1) NEW_VERSION="$NEXT_PATCH" ;;
    2) NEW_VERSION="$NEXT_MINOR" ;;
    3) NEW_VERSION="$NEXT_MAJOR" ;;
    4) read -rp "Version (e.g., v1.2.3): " NEW_VERSION ;;
    q|Q) echo "Aborted."; exit 0 ;;
    *) echo -e "${RED}Invalid option${NC}"; exit 1 ;;
esac

if [[ ! "$NEW_VERSION" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
    echo -e "${RED}Error: Invalid semver format${NC}"
    exit 1
fi

if git rev-parse "$NEW_VERSION" >/dev/null 2>&1; then
    echo -e "${RED}Error: Tag $NEW_VERSION already exists${NC}"
    exit 1
fi

if [[ -n "$(git status --porcelain)" ]]; then
    echo -e "${RED}Error: Uncommitted changes. Commit first.${NC}"
    exit 1
fi

echo ""
echo -e "${CYAN}Commits since ${LATEST_TAG}:${NC}"
echo ""

CHANGELOG=$(git log "${LATEST_TAG}..HEAD" --pretty=format:"- %s" --no-merges 2>/dev/null || echo "- Initial release")
echo -e "${DIM}${CHANGELOG}${NC}"

echo ""
echo -e "Release: ${YELLOW}${LATEST_TAG}${NC} → ${GREEN}${NEW_VERSION}${NC}"
read -rp "Confirm? [y/N]: " confirm

if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
    echo "Aborted."
    exit 0
fi

TAG_MESSAGE="Release ${NEW_VERSION}

Changes since ${LATEST_TAG}:
${CHANGELOG}"

echo ""
echo -e "${CYAN}Creating tag...${NC}"
git tag -a "$NEW_VERSION" -m "$TAG_MESSAGE"

echo -e "${CYAN}Pushing to origin...${NC}"
git push origin "$NEW_VERSION"

echo ""
echo -e "${GREEN}✓ Release $NEW_VERSION created!${NC}"
echo ""
echo "GitHub Actions will build and publish the release."
echo -e "Monitor: ${CYAN}https://github.com/quantmind-br/repodocs-go/actions${NC}"
</file>
<file path="AGENTS.md">
# AGENTS.md - Guidelines for repodocs-go

## Build & Test

```bash
make build              # Build binary to ./build/repodocs
make test               # Unit tests (fast, -short)
make test-integration   # Integration tests
make test-e2e           # E2E tests
make lint               # golangci-lint (govet + misspell)
make fmt                # Format code
make deps               # Download and tidy dependencies

# Run single test
go test -v -run TestName ./internal/converter/...
```

## Architecture

**Flow**: URL → Detector → Strategy → Fetcher/Renderer → Converter → Writer

| Package | Purpose |
|---------|---------|
| `internal/app` | Orchestrator, Detector (routes URLs to strategies) |
| `internal/strategies` | crawler, git, sitemap, llms, pkggo |
| `internal/fetcher` | Stealth HTTP client (tls-client) |
| `internal/renderer` | Headless browser (Rod/Chromium) |
| `internal/converter` | HTML → Markdown pipeline |
| `internal/cache` | BadgerDB persistent cache |
| `internal/output` | Markdown writer with frontmatter |
| `internal/domain` | Interfaces, models, errors |

## Code Style

### Imports (3 groups, blank-line separated)
```go
import (
    "context"                                    // 1. Standard library

    "github.com/stretchr/testify/assert"         // 2. External deps

    "github.com/quantmind-br/repodocs-go/internal/domain"  // 3. Internal
)
```

### Naming
- Interfaces: `Fetcher`, `Renderer`, `Cache` (verb-er)
- Structs: `CrawlerStrategy`, `ClientOptions` (PascalCase)
- Constructors: `NewClient()`, `NewOrchestrator()`
- Options: `ClientOptions`, `RetrierOptions`
- Tests: `TestGet_NotFound`

### Error Handling
```go
// Sentinel errors in internal/domain/errors.go
var ErrCacheMiss = errors.New("cache miss")

// Wrap with context
return fmt.Errorf("failed: %w", err)

// Check errors
if errors.Is(err, domain.ErrCacheMiss) { ... }
var fetchErr *domain.FetchError
if errors.As(err, &fetchErr) { ... }
```

### Interfaces (context first, error last)
```go
type Cache interface {
    Get(ctx context.Context, key string) ([]byte, error)
    Set(ctx context.Context, key string, value []byte, ttl time.Duration) error
}

// Options pattern: DefaultXxxOptions() + validation in NewXxx()
type ClientOptions struct { Timeout time.Duration; MaxRetries int }
func DefaultClientOptions() ClientOptions {
    return ClientOptions{Timeout: 30 * time.Second, MaxRetries: 3}
}
```

### Testing
```go
require.NoError(t, err)       // Fails immediately
assert.Equal(t, expected, actual)  // Continues

// Table-driven
tests := []struct {
    name    string
    input   string
    wantErr bool
}{
    {name: "valid", input: "test", wantErr: false},
}
for _, tt := range tests {
    t.Run(tt.name, func(t *testing.T) { ... })
}

// Use t.TempDir() for temp files (auto-cleanup)
```

Test location: `tests/unit/<package>/` mirrors `internal/<package>/`

### Logging (zerolog via utils.Logger)
```go
s.logger.Info().Str("url", url).Msg("Starting extraction")
s.logger.Warn().Err(err).Str("url", url).Msg("Failed")
```

### Concurrency
- Always accept `context.Context` for cancellation
- Use `sync.Map` for concurrent maps, `sync.Mutex` for simple state
- Check context in loops: `select { case <-ctx.Done(): return ctx.Err() default: }`

## DO NOT

- Suppress errors silently (`_ = err`)
- Use `panic` for recoverable errors
- Import from `cmd/` in `internal/`
- Create circular dependencies
- Use type assertions without error checking

## Task Tracking (bd)

```bash
bd ready                    # Find unblocked work
bd create "Title" -t task -p 2    # Create issue
bd update bd-42 --status in_progress   # Update
bd close bd-42 --reason "Done"   # Complete
bd sync                    # Sync with git (run at session end)
```

**Priorities**: 0=Critical, 1=High, 2=Medium, 3=Low, 4=Backlog
**Types**: bug, feature, task, epic, chore

**Workflow**:
1. `bd ready` to find work
2. Claim and implement
3. Create linked issues for discovered work (`--deps discovered-from:<id>`)
4. Complete and sync

## Session Completion

Work is NOT complete until `git push` succeeds:
```bash
git pull --rebase && bd sync && git push
```
</file>
<file path="CLAUDE.md">
# CLAUDE.md

## Quick Commands

```bash
make build              # Build binary to ./build/repodocs
make test               # Unit tests (fast, -short)
make test-integration   # Integration tests
make test-e2e           # E2E tests
make lint               # golangci-lint (v2)
make fmt                # Format code
make vet                # Run go vet
```

## Architecture

**repodocs-go** extracts documentation from websites, Git repos, sitemaps, pkg.go.dev, llms.txt and converts to Markdown.

**Flow**: URL → Detector → Strategy → Fetcher/Renderer → Converter → Writer

### Strategy Pattern
Strategies implement `internal/strategies.Strategy`:
- `Name() string`
- `CanHandle(url string) bool`
- `Execute(ctx context.Context, url string, opts Options) error`

Detection order: LLMS → PkgGo → Sitemap → Git → Crawler

### Dependency Injection
`strategies.Dependencies` is composition root:
```go
type Dependencies struct {
    Fetcher   *fetcher.Client
    Renderer  domain.Renderer
    Cache     domain.Cache
    Converter *converter.Pipeline
    Writer    *output.Writer
    Logger    *utils.Logger
}
```

### Converter Pipeline
1. UTF-8 normalization
2. Content extraction (CSS selector or Readability)
3. HTML sanitization (remove scripts, nav, ads)
4. Markdown conversion
5. Metadata extraction

## Test Structure

```
tests/
├── unit/           # Fast unit tests (make test)
├── integration/    # Network-dependent tests
├── e2e/            # Full CLI tests
├── mocks/          # Generated mocks (go.uber.org/mock)
├── testutil/       # Shared helpers
└── fixtures/       # Test HTML/data
```

## Task Tracking

Use `bd` for task tracking (see AGENTS.md).
</file>
<file path="go.mod">
module github.com/quantmind-br/repodocs-go

go 1.24.1

require (
	github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0
	github.com/PuerkitoBio/goquery v1.11.0
	github.com/bogdanfinn/fhttp v0.6.2
	github.com/bogdanfinn/tls-client v1.11.2
	github.com/cenkalti/backoff/v4 v4.3.0
	github.com/dgraph-io/badger/v4 v4.8.0
	github.com/go-git/go-git/v5 v5.16.4
	github.com/go-rod/rod v0.116.2
	github.com/go-rod/stealth v0.4.9
	github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0
	github.com/gocolly/colly/v2 v2.3.0
	github.com/klauspost/compress v1.18.2
	github.com/rs/zerolog v1.34.0
	github.com/schollz/progressbar/v3 v3.18.0
	github.com/spf13/cobra v1.10.2
	github.com/spf13/viper v1.21.0
	github.com/stretchr/testify v1.11.1
	go.uber.org/mock v0.5.0
	golang.org/x/net v0.47.0
	golang.org/x/text v0.31.0
	gopkg.in/yaml.v3 v3.0.1
)

require (
	dario.cat/mergo v1.0.0 // indirect
	github.com/JohannesKaufmann/dom v0.2.0 // indirect
	github.com/Microsoft/go-winio v0.6.2 // indirect
	github.com/ProtonMail/go-crypto v1.1.6 // indirect
	github.com/andybalholm/brotli v1.1.1 // indirect
	github.com/andybalholm/cascadia v1.3.3 // indirect
	github.com/antchfx/htmlquery v1.3.5 // indirect
	github.com/antchfx/xmlquery v1.5.0 // indirect
	github.com/antchfx/xpath v1.3.5 // indirect
	github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de // indirect
	github.com/atotto/clipboard v0.1.4 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/bits-and-blooms/bitset v1.24.4 // indirect
	github.com/bogdanfinn/quic-go-utls v1.0.4-utls // indirect
	github.com/bogdanfinn/utls v1.7.4-barnius // indirect
	github.com/catppuccin/go v0.3.0 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7 // indirect
	github.com/charmbracelet/bubbletea v1.3.6 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/huh v0.8.0 // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.9.3 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13 // indirect
	github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0 // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/cloudflare/circl v1.6.1 // indirect
	github.com/cyphar/filepath-securejoin v0.4.1 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/dgraph-io/ristretto/v2 v2.2.0 // indirect
	github.com/dustin/go-humanize v1.0.1 // indirect
	github.com/emirpasic/gods v1.18.1 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/fsnotify/fsnotify v1.9.0 // indirect
	github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376 // indirect
	github.com/go-git/go-billy/v5 v5.6.2 // indirect
	github.com/go-logr/logr v1.4.3 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c // indirect
	github.com/go-viper/mapstructure/v2 v2.4.0 // indirect
	github.com/gobwas/glob v0.2.3 // indirect
	github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f // indirect
	github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/flatbuffers v25.2.10+incompatible // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99 // indirect
	github.com/kennygrant/sanitize v1.2.4 // indirect
	github.com/kevinburke/ssh_config v1.2.0 // indirect
	github.com/lucasb-eyer/go-colorful v1.3.0 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db // indirect
	github.com/mitchellh/hashstructure/v2 v2.0.2 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/nlnwa/whatwg-url v0.6.2 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/pjbgf/sha1cd v0.3.2 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/quic-go/qpack v0.5.1 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sagikazarmark/locafero v0.11.0 // indirect
	github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d // indirect
	github.com/sergi/go-diff v1.4.0 // indirect
	github.com/skeema/knownhosts v1.3.1 // indirect
	github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 // indirect
	github.com/spf13/afero v1.15.0 // indirect
	github.com/spf13/cast v1.10.0 // indirect
	github.com/spf13/pflag v1.0.10 // indirect
	github.com/stretchr/objx v0.5.2 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5 // indirect
	github.com/temoto/robotstxt v1.1.2 // indirect
	github.com/xanzy/ssh-agent v0.3.3 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	github.com/ysmood/fetchup v0.2.3 // indirect
	github.com/ysmood/goob v0.4.0 // indirect
	github.com/ysmood/got v0.40.0 // indirect
	github.com/ysmood/gson v0.7.3 // indirect
	github.com/ysmood/leakless v0.9.0 // indirect
	go.opentelemetry.io/auto/sdk v1.1.0 // indirect
	go.opentelemetry.io/otel v1.37.0 // indirect
	go.opentelemetry.io/otel/metric v1.37.0 // indirect
	go.opentelemetry.io/otel/trace v1.37.0 // indirect
	go.yaml.in/yaml/v3 v3.0.4 // indirect
	golang.org/x/crypto v0.44.0 // indirect
	golang.org/x/mod v0.29.0 // indirect
	golang.org/x/sync v0.18.0 // indirect
	golang.org/x/sys v0.38.0 // indirect
	golang.org/x/term v0.37.0 // indirect
	golang.org/x/tools v0.38.0 // indirect
	google.golang.org/appengine v1.6.8 // indirect
	google.golang.org/protobuf v1.36.10 // indirect
	gopkg.in/warnings.v0 v0.1.2 // indirect
)
</file>
<file path="go.sum">
dario.cat/mergo v1.0.0 h1:AGCNq9Evsj31mOgNPcLyXc+4PNABt905YmuqPYYpBWk=
dario.cat/mergo v1.0.0/go.mod h1:uNxQE+84aUszobStD9th8a29P2fMDhsBdgRYvZOxGmk=
github.com/JohannesKaufmann/dom v0.2.0 h1:1bragmEb19K8lHAqgFgqCpiPCFEZMTXzOIEjuxkUfLQ=
github.com/JohannesKaufmann/dom v0.2.0/go.mod h1:57iSUl5RKric4bUkgos4zu6Xt5LMHUnw3TF1l5CbGZo=
github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0 h1:mklaPbT4f/EiDr1Q+zPrEt9lgKAkVrIBtWf33d9GpVA=
github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0/go.mod h1:D56Cl9r8M5i3UwAchE+LlLc5hPN3kJtdZNVJn06lSHU=
github.com/Microsoft/go-winio v0.5.2/go.mod h1:WpS1mjBmmwHBEWmogvA2mj8546UReBk4v8QkMxJ6pZY=
github.com/Microsoft/go-winio v0.6.2 h1:F2VQgta7ecxGYO8k3ZZz3RS8fVIXVxONVUPlNERoyfY=
github.com/Microsoft/go-winio v0.6.2/go.mod h1:yd8OoFMLzJbo9gZq8j5qaps8bJ9aShtEA8Ipt1oGCvU=
github.com/ProtonMail/go-crypto v1.1.6 h1:ZcV+Ropw6Qn0AX9brlQLAUXfqLBc7Bl+f/DmNxpLfdw=
github.com/ProtonMail/go-crypto v1.1.6/go.mod h1:rA3QumHc/FZ8pAHreoekgiAbzpNsfQAosU5td4SnOrE=
github.com/PuerkitoBio/goquery v1.11.0 h1:jZ7pwMQXIITcUXNH83LLk+txlaEy6NVOfTuP43xxfqw=
github.com/PuerkitoBio/goquery v1.11.0/go.mod h1:wQHgxUOU3JGuj3oD/QFfxUdlzW6xPHfqyHre6VMY4DQ=
github.com/andybalholm/brotli v1.1.1 h1:PR2pgnyFznKEugtsUo0xLdDop5SKXd5Qf5ysW+7XdTA=
github.com/andybalholm/brotli v1.1.1/go.mod h1:05ib4cKhjx3OQYUY22hTVd34Bc8upXjOLL2rKwwZBoA=
github.com/andybalholm/cascadia v1.3.3 h1:AG2YHrzJIm4BZ19iwJ/DAua6Btl3IwJX+VI4kktS1LM=
github.com/andybalholm/cascadia v1.3.3/go.mod h1:xNd9bqTn98Ln4DwST8/nG+H0yuB8Hmgu1YHNnWw0GeA=
github.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be h1:9AeTilPcZAjCFIImctFaOjnTIavg87rW78vTPkQqLI8=
github.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be/go.mod h1:ySMOLuWl6zY27l47sB3qLNK6tF2fkHG55UZxx8oIVo4=
github.com/antchfx/htmlquery v1.3.5 h1:aYthDDClnG2a2xePf6tys/UyyM/kRcsFRm+ifhFKoU0=
github.com/antchfx/htmlquery v1.3.5/go.mod h1:5oyIPIa3ovYGtLqMPNjBF2Uf25NPCKsMjCnQ8lvjaoA=
github.com/antchfx/xmlquery v1.5.0 h1:uAi+mO40ZWfyU6mlUBxRVvL6uBNZ6LMU4M3+mQIBV4c=
github.com/antchfx/xmlquery v1.5.0/go.mod h1:lJfWRXzYMK1ss32zm1GQV3gMIW/HFey3xDZmkP1SuNc=
github.com/antchfx/xpath v1.3.5 h1:PqbXLC3TkfeZyakF5eeh3NTWEbYl4VHNVeufANzDbKQ=
github.com/antchfx/xpath v1.3.5/go.mod h1:i54GszH55fYfBmoZXapTHN8T8tkcHfRgLyVwwqzXNcs=
github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de h1:FxWPpzIjnTlhPwqqXc4/vE0f7GvRjuAsbW+HOIe8KnA=
github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de/go.mod h1:DCaWoUhZrYW9p1lxo/cm8EmUOOzAPSEZNGF2DK1dJgw=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5 h1:0CwZNZbxp69SHPdPJAN/hZIm0C4OItdklCFmMRWYpio=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5/go.mod h1:wHh0iHkYZB8zMSxRWpUBQtwG5a7fFgvEO+odwuTv2gs=
github.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=
github.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/bits-and-blooms/bitset v1.20.0/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=
github.com/bits-and-blooms/bitset v1.24.4 h1:95H15Og1clikBrKr/DuzMXkQzECs1M6hhoGXLwLQOZE=
github.com/bits-and-blooms/bitset v1.24.4/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=
github.com/bogdanfinn/fhttp v0.6.2 h1:qmFu9fxKmSRR+tcKfgxthmiu365tYspz3Mi404ytZPE=
github.com/bogdanfinn/fhttp v0.6.2/go.mod h1:0irhEtS+wJ4m8SGhWO0wmbXMjCbH3WZpU6UcymRYKuk=
github.com/bogdanfinn/quic-go-utls v1.0.4-utls h1:zPjusVVNeJFA2ORMAP0rjnrZrBkV4Dnia4e6ToOfUDA=
github.com/bogdanfinn/quic-go-utls v1.0.4-utls/go.mod h1:UONJOaHGWho08kZtkkgH7GjktEPjMemGxjTcNpVPZVA=
github.com/bogdanfinn/tls-client v1.11.2 h1:o6qX0L1cEi+4MaBqujxqOeK254VZM20t3QR+A34/V6I=
github.com/bogdanfinn/tls-client v1.11.2/go.mod h1:qQIsVGe35NdxYEozNh9JuDZ+aOaOEq2tKAsu2iYEGZg=
github.com/bogdanfinn/utls v1.7.4-barnius h1:1ldNJEpKdkrx7b8hEc6MRkjnZIF8f2lDcTtRVxqY9zw=
github.com/bogdanfinn/utls v1.7.4-barnius/go.mod h1:SUn0CoHGVp/akGNuaqh99yvovu64PCP2LbWd3Z/Laic=
github.com/catppuccin/go v0.3.0 h1:d+0/YicIq+hSTo5oPuRi5kOpqkVA5tAsU6dNhvRu+aY=
github.com/catppuccin/go v0.3.0/go.mod h1:8IHJuMGaUUjQM82qBrGNBv7LFq6JI3NnQCF6MOlZjpc=
github.com/cenkalti/backoff/v4 v4.3.0 h1:MyRJ/UdXutAwSAT+s3wNd7MfTIcy71VQueUuFK343L8=
github.com/cenkalti/backoff/v4 v4.3.0/go.mod h1:Y3VNntkOUPxTVeUxJ/G5vcM//AlwfmyYozVcomhLiZE=
github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7 h1:JFgG/xnwFfbezlUnFMJy0nusZvytYysV4SCS2cYbvws=
github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7/go.mod h1:ISC1gtLcVilLOf23wvTfoQuYbW2q0JevFxPfUzZ9Ybw=
github.com/charmbracelet/bubbletea v1.3.6 h1:VkHIxPJQeDt0aFJIsVxw8BQdh/F/L2KKZGsK6et5taU=
github.com/charmbracelet/bubbletea v1.3.6/go.mod h1:oQD9VCRQFF8KplacJLo28/jofOI2ToOfGYeFgBBxHOc=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/huh v0.8.0 h1:Xz/Pm2h64cXQZn/Jvele4J3r7DDiqFCNIVteYukxDvY=
github.com/charmbracelet/huh v0.8.0/go.mod h1:5YVc+SlZ1IhQALxRPpkGwwEKftN/+OlJlnJYlDRFqN4=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.9.3 h1:BXt5DHS/MKF+LjuK4huWrC6NCvHtexww7dMayh6GXd0=
github.com/charmbracelet/x/ansi v0.9.3/go.mod h1:3RQDQ6lDnROptfpWuUVIUG64bD2g2BgntdxH0Ya5TeE=
github.com/charmbracelet/x/cellbuf v0.0.13 h1:/KBBKHuVRbq1lYx5BzEHBAFBP8VcQzJejZ/IA3iR28k=
github.com/charmbracelet/x/cellbuf v0.0.13/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0 h1:qko3AQ4gK1MTS/de7F5hPGx6/k1u0w4TeYmBFwzYVP4=
github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0/go.mod h1:pBhA0ybfXv6hDjQUZ7hk1lVxBiUbupdw5R31yPUViVQ=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/chengxilo/virtualterm v1.0.4 h1:Z6IpERbRVlfB8WkOmtbHiDbBANU7cimRIof7mk9/PwM=
github.com/chengxilo/virtualterm v1.0.4/go.mod h1:DyxxBZz/x1iqJjFxTFcr6/x+jSpqN0iwWCOK1q10rlY=
github.com/cloudflare/circl v1.6.1 h1:zqIqSPIndyBh1bjLVVDHMPpVKqp8Su/V+6MeDzzQBQ0=
github.com/cloudflare/circl v1.6.1/go.mod h1:uddAzsPgqdMAYatqJ0lsjX1oECcQLIlRpzZh3pJrofs=
github.com/coreos/go-systemd/v22 v22.5.0/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/cyphar/filepath-securejoin v0.4.1 h1:JyxxyPEaktOD+GAnqIqTf9A8tHyAG22rowi7HkoSU1s=
github.com/cyphar/filepath-securejoin v0.4.1/go.mod h1:Sdj7gXlvMcPZsbhwhQ33GguGLDGQL7h7bg04C/+u9jI=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/dgraph-io/badger/v4 v4.8.0 h1:JYph1ChBijCw8SLeybvPINizbDKWZ5n/GYbz2yhN/bs=
github.com/dgraph-io/badger/v4 v4.8.0/go.mod h1:U6on6e8k/RTbUWxqKR0MvugJuVmkxSNc79ap4917h4w=
github.com/dgraph-io/ristretto/v2 v2.2.0 h1:bkY3XzJcXoMuELV8F+vS8kzNgicwQFAaGINAEJdWGOM=
github.com/dgraph-io/ristretto/v2 v2.2.0/go.mod h1:RZrm63UmcBAaYWC1DotLYBmTvgkrs0+XhBd7Npn7/zI=
github.com/dgryski/go-farm v0.0.0-20240924180020-3414d57e47da h1:aIftn67I1fkbMa512G+w+Pxci9hJPB8oMnkcP3iZF38=
github.com/dgryski/go-farm v0.0.0-20240924180020-3414d57e47da/go.mod h1:SqUrOPUnsFjfmXRMNPybcSiG0BgUW2AuFH8PAnS2iTw=
github.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=
github.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=
github.com/elazarl/goproxy v1.7.2 h1:Y2o6urb7Eule09PjlhQRGNsqRfPmYI3KKQLFpCAV3+o=
github.com/elazarl/goproxy v1.7.2/go.mod h1:82vkLNir0ALaW14Rc399OTTjyNREgmdL2cVoIbS6XaE=
github.com/emirpasic/gods v1.18.1 h1:FXtiHYKDGKCW2KzwZKx0iC0PQmdlorYgdFG9jPXJ1Bc=
github.com/emirpasic/gods v1.18.1/go.mod h1:8tpGGwCnJ5H4r6BWwaV6OrWmMoPhUl5jm/FMNAnJvWQ=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.9.0 h1:2Ml+OJNzbYCTzsxtv8vKSFD9PbJjmhYF14k/jKC7S9k=
github.com/fsnotify/fsnotify v1.9.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
github.com/gliderlabs/ssh v0.3.8 h1:a4YXD1V7xMF9g5nTkdfnja3Sxy1PVDCj1Zg4Wb8vY6c=
github.com/gliderlabs/ssh v0.3.8/go.mod h1:xYoytBv1sV0aL3CavoDuJIQNURXkkfPA/wxQ1pL1fAU=
github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376 h1:+zs/tPmkDkHx3U66DAb0lQFJrpS6731Oaa12ikc+DiI=
github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376/go.mod h1:an3vInlBmSxCcxctByoQdvwPiA7DTK7jaaFDBTtu0ic=
github.com/go-git/go-billy/v5 v5.6.2 h1:6Q86EsPXMa7c3YZ3aLAQsMA0VlWmy43r6FHqa/UNbRM=
github.com/go-git/go-billy/v5 v5.6.2/go.mod h1:rcFC2rAsp/erv7CMz9GczHcuD0D32fWzH+MJAU+jaUU=
github.com/go-git/go-git-fixtures/v4 v4.3.2-0.20231010084843-55a94097c399 h1:eMje31YglSBqCdIqdhKBW8lokaMrL3uTkpGYlE2OOT4=
github.com/go-git/go-git-fixtures/v4 v4.3.2-0.20231010084843-55a94097c399/go.mod h1:1OCfN199q1Jm3HZlxleg+Dw/mwps2Wbk9frAWm+4FII=
github.com/go-git/go-git/v5 v5.16.4 h1:7ajIEZHZJULcyJebDLo99bGgS0jRrOxzZG4uCk2Yb2Y=
github.com/go-git/go-git/v5 v5.16.4/go.mod h1:4Ge4alE/5gPs30F2H1esi2gPd69R0C39lolkucHBOp8=
github.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=
github.com/go-logr/logr v1.4.3 h1:CjnDlHq8ikf6E492q6eKboGOC0T8CDaOvkHCIg8idEI=
github.com/go-logr/logr v1.4.3/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=
github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=
github.com/go-rod/rod v0.113.0/go.mod h1:aiedSEFg5DwG/fnNbUOTPMTTWX3MRj6vIs/a684Mthw=
github.com/go-rod/rod v0.116.2 h1:A5t2Ky2A+5eD/ZJQr1EfsQSe5rms5Xof/qj296e+ZqA=
github.com/go-rod/rod v0.116.2/go.mod h1:H+CMO9SCNc2TJ2WfrG+pKhITz57uGNYU43qYHh438Mg=
github.com/go-rod/stealth v0.4.9 h1:X2PmQk4DUF2wzw6GOsWjW/glb8K5ebnftbEvLh7MlZ4=
github.com/go-rod/stealth v0.4.9/go.mod h1:eAzyvw8c0iAd5nJJsSWeh0fQ5z94vCIfdi1hUmYDimc=
github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c h1:wpkoddUomPfHiOziHZixGO5ZBS73cKqVzZipfrLmO1w=
github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c/go.mod h1:oVDCh3qjJMLVUSILBRwrm+Bc6RNXGZYtoh9xdvf1ffM=
github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0 h1:A3B75Yp163FAIf9nLlFMl4pwIj+T3uKxfI7mbvvY2Ls=
github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0/go.mod h1:suxK0Wpz4BM3/2+z1mnOVTIWHDiMCIOGoKDCRumSsk0=
github.com/go-viper/mapstructure/v2 v2.4.0 h1:EBsztssimR/CONLSZZ04E8qAkxNYq4Qp9LvH92wZUgs=
github.com/go-viper/mapstructure/v2 v2.4.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
github.com/gobwas/glob v0.2.3 h1:A4xDbljILXROh+kObIiy5kIaPYD8e96x1tgBhUI5J+Y=
github.com/gobwas/glob v0.2.3/go.mod h1:d3Ez4x06l9bZtSvzIay5+Yzi0fmZzPgnTbPcKjJAkT8=
github.com/gocolly/colly/v2 v2.3.0 h1:HSFh0ckbgVd2CSGRE+Y/iA4goUhGROJwyQDCMXGFBWM=
github.com/gocolly/colly/v2 v2.3.0/go.mod h1:Qp54s/kQbwCQvFVx8KzKCSTXVJ1wWT4QeAKEu33x1q8=
github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=
github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f h1:3BSP1Tbs2djlpprl7wCLuiqMaUh5SJkkzI2gDs+FgLs=
github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f/go.mod h1:Pcatq5tYkCW2Q6yrR2VRHlbHpZ/R4/7qyL1TCF7vl14=
github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8 h1:f+oWsMOmNPc8JmEHVZIycC7hBoQxHH9pNKQORJNozsQ=
github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8/go.mod h1:wcDNUvekVysuuOpQKo3191zZyTpiI6se1N1ULghS0sw=
github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
github.com/google/flatbuffers v25.2.10+incompatible h1:F3vclr7C3HpB1k9mxCGRMXq6FdUalZ6H/pNX4FP1v0Q=
github.com/google/flatbuffers v25.2.10+incompatible/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=
github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99 h1:BQSFePA1RWJOlocH6Fxy8MmwDt+yVQYULKfN0RoTN8A=
github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99/go.mod h1:1lJo3i6rXxKeerYnT8Nvf0QmHCRC1n8sfWVwXF2Frvo=
github.com/kennygrant/sanitize v1.2.4 h1:gN25/otpP5vAsO2djbMhF/LQX6R7+O1TB4yv8NzpJ3o=
github.com/kennygrant/sanitize v1.2.4/go.mod h1:LGsjYYtgxbetdg5owWB2mpgUL6e2nfw2eObZ0u0qvak=
github.com/kevinburke/ssh_config v1.2.0 h1:x584FjTGwHzMwvHx18PXxbBVzfnxogHaAReU4gf13a4=
github.com/kevinburke/ssh_config v1.2.0/go.mod h1:CT57kijsi8u/K/BOFA39wgDQJ9CxiF4nAY/ojJ6r6mM=
github.com/klauspost/compress v1.18.2 h1:iiPHWW0YrcFgpBYhsA6D1+fqHssJscY/Tm/y2Uqnapk=
github.com/klauspost/compress v1.18.2/go.mod h1:R0h/fSBs8DE4ENlcrlib3PsXS61voFxhIs2DeRhCvJ4=
github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lucasb-eyer/go-colorful v1.3.0 h1:2/yBRLdWBZKrf7gB40FoiKfAWYQ0lqNcbuQwVHXptag=
github.com/lucasb-eyer/go-colorful v1.3.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-colorable v0.1.13 h1:fFA4WZxdEF4tXPZVKMLwD8oUnCTTo08duU7wxecdEvA=
github.com/mattn/go-colorable v0.1.13/go.mod h1:7S9/ev0klgBDR4GtXTXX8a3vIGJpMovkB8vQcUbaXHg=
github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=
github.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-runewidth v0.0.10/go.mod h1:RAqKPSqVFrSLVXbA8x7dzmKdmGzieGRCM46jaSJTDAk=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db h1:62I3jR2EmQ4l5rM/4FEfDWcRD+abF5XlKShorW5LRoQ=
github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db/go.mod h1:l0dey0ia/Uv7NcFFVbCLtqEBQbrT4OCwCSKTEv6enCw=
github.com/mitchellh/hashstructure/v2 v2.0.2 h1:vGKWl0YJqUNxE8d+h8f6NJLcCJrgbhC4NcD46KavDd4=
github.com/mitchellh/hashstructure/v2 v2.0.2/go.mod h1:MG3aRVU/N29oo/V/IhBX8GR/zz4kQkprJgF2EVszyDE=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/nlnwa/whatwg-url v0.6.2 h1:jU61lU2ig4LANydbEJmA2nPrtCGiKdtgT0rmMd2VZ/Q=
github.com/nlnwa/whatwg-url v0.6.2/go.mod h1:x0FPXJzzOEieQtsBT/AKvbiBbQ46YlL6Xa7m02M1ECk=
github.com/onsi/gomega v1.34.1 h1:EUMJIKUjM8sKjYbtxQI9A4z2o+rruxnzNvpknOXie6k=
github.com/onsi/gomega v1.34.1/go.mod h1:kU1QgUvBDLXBJq618Xvm2LUX6rSAfRaFRTcdOeDLwwY=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pjbgf/sha1cd v0.3.2 h1:a9wb0bp1oC2TGwStyn0Umc/IGKQnEgF0vVaZ8QF8eo4=
github.com/pjbgf/sha1cd v0.3.2/go.mod h1:zQWigSxVmsHEZow5qaLtPYxpcKMMQpa09ixqBxuCS6A=
github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/quic-go/qpack v0.5.1 h1:giqksBPnT/HDtZ6VhtFKgoLOWmlyo9Ei6u9PqzIMbhI=
github.com/quic-go/qpack v0.5.1/go.mod h1:+PC4XFrEskIVkcLzpEkbLqq1uCoxPhQuvK5rH1ZgaEg=
github.com/rivo/uniseg v0.1.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/rogpeppe/go-internal v1.14.1 h1:UQB4HGPB6osV0SQTLymcB4TgvyWu6ZyliaW0tI/otEQ=
github.com/rogpeppe/go-internal v1.14.1/go.mod h1:MaRKkUm5W0goXpeCfT7UZI6fk/L7L7so1lCWt35ZSgc=
github.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=
github.com/rs/zerolog v1.34.0 h1:k43nTLIwcTVQAncfCw4KZ2VY6ukYoZaBPNOE8txlOeY=
github.com/rs/zerolog v1.34.0/go.mod h1:bJsvje4Z08ROH4Nhs5iH600c3IkWhwp44iRc54W6wYQ=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.11.0 h1:1iurJgmM9G3PA/I+wWYIOw/5SyBtxapeHDcg+AAIFXc=
github.com/sagikazarmark/locafero v0.11.0/go.mod h1:nVIGvgyzw595SUSUE6tvCp3YYTeHs15MvlmU87WwIik=
github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d h1:hrujxIzL1woJ7AwssoOcM/tq5JjjG2yYOc8odClEiXA=
github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d/go.mod h1:uugorj2VCxiV1x+LzaIdVa9b4S4qGAcH6cbhh4qVxOU=
github.com/schollz/progressbar/v3 v3.18.0 h1:uXdoHABRFmNIjUfte/Ex7WtuyVslrw2wVPQmCN62HpA=
github.com/schollz/progressbar/v3 v3.18.0/go.mod h1:IsO3lpbaGuzh8zIMzgY3+J8l4C8GjO0Y9S69eFvNsec=
github.com/scylladb/termtables v0.0.0-20191203121021-c4c0b6d42ff4/go.mod h1:C1a7PQSMz9NShzorzCiG2fk9+xuCgLkPeCvMHYR2OWg=
github.com/sebdah/goldie/v2 v2.8.0 h1:dZb9wR8q5++oplmEiJT+U/5KyotVD+HNGCAc5gNr8rc=
github.com/sebdah/goldie/v2 v2.8.0/go.mod h1:oZ9fp0+se1eapSRjfYbsV/0Hqhbuu3bJVvKI/NNtssI=
github.com/sergi/go-diff v1.4.0 h1:n/SP9D5ad1fORl+llWyN+D6qoUETXNZARKjyY2/KVCw=
github.com/sergi/go-diff v1.4.0/go.mod h1:A0bzQcvG0E7Rwjx0REVgAGH58e96+X0MeOfepqsbeW4=
github.com/sirupsen/logrus v1.7.0/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=
github.com/skeema/knownhosts v1.3.1 h1:X2osQ+RAjK76shCbvhHHHVl3ZlgDm8apHEHFqRjnBY8=
github.com/skeema/knownhosts v1.3.1/go.mod h1:r7KTdC8l4uxWRyK2TpQZ/1o5HaSzh06ePQNxPwTcfiY=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 h1:+jumHNA0Wrelhe64i8F6HNlS8pkoyMv5sreGx2Ry5Rw=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8/go.mod h1:3n1Cwaq1E1/1lhQhtRK2ts/ZwZEhjcQeJQ1RuC6Q/8U=
github.com/spf13/afero v1.15.0 h1:b/YBCLWAJdFWJTN9cLhiXXcD7mzKn9Dm86dNnfyQw1I=
github.com/spf13/afero v1.15.0/go.mod h1:NC2ByUVxtQs4b3sIUphxK0NioZnmxgyCrfzeuq8lxMg=
github.com/spf13/cast v1.10.0 h1:h2x0u2shc1QuLHfxi+cTJvs30+ZAHOGRic8uyGTDWxY=
github.com/spf13/cast v1.10.0/go.mod h1:jNfB8QC9IA6ZuY2ZjDp0KtFO2LZZlg4S/7bzP6qqeHo=
github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/pflag v1.0.10 h1:4EBh2KAYBwaONj6b2Ye1GiHfwjqyROoF4RwYO+vPwFk=
github.com/spf13/pflag v1.0.10/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.21.0 h1:x5S+0EU27Lbphp4UKm1C+1oQO+rKx36vfCoaVebLFSU=
github.com/spf13/viper v1.21.0/go.mod h1:P0lhsswPGWD/1lZJ9ny3fYnVqxiegrlNrEmgLjbTCAY=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.5.2 h1:xuMeJ0Sdp5ZMRXx/aWO6RZxdr3beISkG5/G/aIRr3pY=
github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5 h1:YqAladjX7xpA6BM04leXMWAEjS0mTZ5kUU9KRBriQJc=
github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5/go.mod h1:2JjD2zLQYH5HO74y5+aE3remJQvl6q4Sn6aWA2wD1Ng=
github.com/temoto/robotstxt v1.1.2 h1:W2pOjSJ6SWvldyEuiFXNxz3xZ8aiWX5LbfDiOFd7Fxg=
github.com/temoto/robotstxt v1.1.2/go.mod h1:+1AmkuG3IYkh1kv0d2qEB9Le88ehNO0zwOr3ujewlOo=
github.com/xanzy/ssh-agent v0.3.3 h1:+/15pJfg/RsTxqYcX6fHqOXZwwMP+2VyYWJeWM2qQFM=
github.com/xanzy/ssh-agent v0.3.3/go.mod h1:6dzNDKs0J9rVPHPhaGCukekBHKqfl+L3KghI1Bc68Uw=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
github.com/xyproto/randomstring v1.0.5 h1:YtlWPoRdgMu3NZtP45drfy1GKoojuR7hmRcnhZqKjWU=
github.com/xyproto/randomstring v1.0.5/go.mod h1:rgmS5DeNXLivK7YprL0pY+lTuhNQW3iGxZ18UQApw/E=
github.com/ysmood/fetchup v0.2.3 h1:ulX+SonA0Vma5zUFXtv52Kzip/xe7aj4vqT5AJwQ+ZQ=
github.com/ysmood/fetchup v0.2.3/go.mod h1:xhibcRKziSvol0H1/pj33dnKrYyI2ebIvz5cOOkYGns=
github.com/ysmood/goob v0.4.0 h1:HsxXhyLBeGzWXnqVKtmT9qM7EuVs/XOgkX7T6r1o1AQ=
github.com/ysmood/goob v0.4.0/go.mod h1:u6yx7ZhS4Exf2MwciFr6nIM8knHQIE22lFpWHnfql18=
github.com/ysmood/gop v0.0.2/go.mod h1:rr5z2z27oGEbyB787hpEcx4ab8cCiPnKxn0SUHt6xzk=
github.com/ysmood/gop v0.2.0 h1:+tFrG0TWPxT6p9ZaZs+VY+opCvHU8/3Fk6BaNv6kqKg=
github.com/ysmood/gop v0.2.0/go.mod h1:rr5z2z27oGEbyB787hpEcx4ab8cCiPnKxn0SUHt6xzk=
github.com/ysmood/got v0.34.1/go.mod h1:yddyjq/PmAf08RMLSwDjPyCvHvYed+WjHnQxpH851LM=
github.com/ysmood/got v0.40.0 h1:ZQk1B55zIvS7zflRrkGfPDrPG3d7+JOza1ZkNxcc74Q=
github.com/ysmood/got v0.40.0/go.mod h1:W7DdpuX6skL3NszLmAsC5hT7JAhuLZhByVzHTq874Qg=
github.com/ysmood/gotrace v0.6.0 h1:SyI1d4jclswLhg7SWTL6os3L1WOKeNn/ZtzVQF8QmdY=
github.com/ysmood/gotrace v0.6.0/go.mod h1:TzhIG7nHDry5//eYZDYcTzuJLYQIkykJzCRIo4/dzQM=
github.com/ysmood/gson v0.7.3 h1:QFkWbTH8MxyUTKPkVWAENJhxqdBa4lYTQWqZCiLG6kE=
github.com/ysmood/gson v0.7.3/go.mod h1:3Kzs5zDl21g5F/BlLTNcuAGAYLKt2lV5G8D1zF3RNmg=
github.com/ysmood/leakless v0.8.0/go.mod h1:R8iAXPRaG97QJwqxs74RdwzcRHT1SWCGTNqY8q0JvMQ=
github.com/ysmood/leakless v0.9.0 h1:qxCG5VirSBvmi3uynXFkcnLMzkphdh3xx5FtrORwDCU=
github.com/ysmood/leakless v0.9.0/go.mod h1:R8iAXPRaG97QJwqxs74RdwzcRHT1SWCGTNqY8q0JvMQ=
github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=
github.com/yuin/goldmark v1.7.13 h1:GPddIs617DnBLFFVJFgpo1aBfe/4xcvMc3SB5t/D0pA=
github.com/yuin/goldmark v1.7.13/go.mod h1:ip/1k0VRfGynBgxOz0yCqHrbZXhcjxyuS66Brc7iBKg=
go.opentelemetry.io/auto/sdk v1.1.0 h1:cH53jehLUN6UFLY71z+NDOiNJqDdPRaXzTel0sJySYA=
go.opentelemetry.io/auto/sdk v1.1.0/go.mod h1:3wSPjt5PWp2RhlCcmmOial7AvC4DQqZb7a7wCow3W8A=
go.opentelemetry.io/otel v1.37.0 h1:9zhNfelUvx0KBfu/gb+ZgeAfAgtWrfHJZcAqFC228wQ=
go.opentelemetry.io/otel v1.37.0/go.mod h1:ehE/umFRLnuLa/vSccNq9oS1ErUlkkK71gMcN34UG8I=
go.opentelemetry.io/otel/metric v1.37.0 h1:mvwbQS5m0tbmqML4NqK+e3aDiO02vsf/WgbsdpcPoZE=
go.opentelemetry.io/otel/metric v1.37.0/go.mod h1:04wGrZurHYKOc+RKeye86GwKiTb9FKm1WHtO+4EVr2E=
go.opentelemetry.io/otel/trace v1.37.0 h1:HLdcFNbRQBE2imdSEgm/kwqmQj1Or1l/7bW6mxVK7z4=
go.opentelemetry.io/otel/trace v1.37.0/go.mod h1:TlgrlQ+PtQO5XFerSPUYG0JSgGyryXewPGyayAWSBS0=
go.uber.org/mock v0.5.0 h1:KAMbZvZPyBPWgD14IrIQ38QCyjwpvVVV6K/bHl1IwQU=
go.uber.org/mock v0.5.0/go.mod h1:ge71pBPLYDk7QIi1LupWxdAykm7KIEFchiOqd6z7qMM=
go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
golang.org/x/crypto v0.0.0-20220622213112-05595931fe9d/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=
golang.org/x/crypto v0.13.0/go.mod h1:y6Z2r+Rw4iayiXXAIxJIDAJ1zMW4yaTpebo8fPOliYc=
golang.org/x/crypto v0.19.0/go.mod h1:Iy9bg/ha4yyC70EfRS8jz+B6ybOBKMaSxLj6P6oBDfU=
golang.org/x/crypto v0.23.0/go.mod h1:CKFgDieR+mRhux2Lsu27y0fO304Db0wZe70UKqHu0v8=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/crypto v0.32.0/go.mod h1:ZnnJkOaASj8g0AjIduWNlq2NRxL0PlBrbKVyZ6V/Ugc=
golang.org/x/crypto v0.44.0 h1:A97SsFvM3AIwEEmTBiaxPPTYpDC47w720rdiiUvgoAU=
golang.org/x/crypto v0.44.0/go.mod h1:013i+Nw79BMiQiMsOPcVCB5ZIJbYkerPrGnOa00tvmc=
golang.org/x/exp v0.0.0-20240719175910-8a7402abbf56 h1:2dVuKD2vS7b0QIHQbpyTISPd0LeHDbnYEryqj5Q1ug8=
golang.org/x/exp v0.0.0-20240719175910-8a7402abbf56/go.mod h1:M4RDyNAINzryxdtnbRXRL/OHtkFuWGRjvuhBJpk2IlY=
golang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=
golang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/mod v0.12.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/mod v0.15.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
golang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
golang.org/x/mod v0.29.0 h1:HV8lRxZC4l2cr3Zq1LvtOsi/ThTgWnUk/y64QSs8GwA=
golang.org/x/mod v0.29.0/go.mod h1:NyhrlYXJ2H4eJiRy/WDBO6HMqZQ6q9nk4JzS3NuCK+w=
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
golang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=
golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=
golang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=
golang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=
golang.org/x/net v0.15.0/go.mod h1:idbUs1IY1+zTqbi8yxTbhexhEEk5ur9LInksu6HrEpk=
golang.org/x/net v0.21.0/go.mod h1:bIjVDfnllIU7BJ2DNgfnXvpSvtn8VRwhlsaeUTyUS44=
golang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=
golang.org/x/net v0.33.0/go.mod h1:HXLR5J+9DxmrqMwG9qjGCxZ+zKXxBru04zlTvWlWuN4=
golang.org/x/net v0.34.0/go.mod h1:di0qlW3YNM5oh6GqDGQr92MyTozJPmybPK4Ev/Gm31k=
golang.org/x/net v0.47.0 h1:Mx+4dIFzqraBXUugkia1OOvlD6LemFo1ALMHjrXDOhY=
golang.org/x/net v0.47.0/go.mod h1:/jNxtkgq5yWUGYkaZGqo27cfGZ1c5Nen03aYrrKpVRU=
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=
golang.org/x/sync v0.6.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.18.0 h1:kr88TuHDroi+UVf+0hZnirlk8o8T+4MrK6mr60WkH/I=
golang.org/x/sync v0.18.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.20.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.29.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.38.0 h1:3yZWxaJjBmCWXqhN1qh02AkOnCQ1poK6oF+a7xWL6Gc=
golang.org/x/sys v0.38.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/telemetry v0.0.0-20240228155512-f48c80bd79b2/go.mod h1:TeRTkGYfJXctD9OcfyVLyj2J3IxLnKwHJR8f4D8a3YE=
golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=
golang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=
golang.org/x/term v0.8.0/go.mod h1:xPskH00ivmX89bAKVGSKKtLOWNx2+17Eiy94tnKShWo=
golang.org/x/term v0.12.0/go.mod h1:owVbMEjm3cBLCHdkQu9b1opXd4ETQWc3BhuQGKgXgvU=
golang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=
golang.org/x/term v0.20.0/go.mod h1:8UkIAJTvZgivsXaD6/pH6U9ecQzZ45awqEOzuCvwpFY=
golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=
golang.org/x/term v0.28.0/go.mod h1:Sw/lC2IAUZ92udQNf3WodGtn4k/XoLyZoh8v/8uiwek=
golang.org/x/term v0.37.0 h1:8EGAD0qCmHYZg6J17DvsMy9/wJ7/D/4pV/wfnld5lTU=
golang.org/x/term v0.37.0/go.mod h1:5pB4lxRNYYVZuTLmy8oR2BH8dflOR+IbTYFD8fi3254=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=
golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=
golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=
golang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=
golang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=
golang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=
golang.org/x/text v0.15.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=
golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=
golang.org/x/text v0.31.0 h1:aC8ghyu4JhP8VojJ2lEHBnochRno1sgL6nEi9WGFGMM=
golang.org/x/text v0.31.0/go.mod h1:tKRAlv61yKIjGGHX/4tP1LTbc13YSec1pxVEWXzfoeM=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
golang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=
golang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=
golang.org/x/tools v0.13.0/go.mod h1:HvlwmtVNQAhOuCjW7xxvovg8wbNq7LwfXh/k7wXUl58=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=
golang.org/x/tools v0.38.0 h1:Hx2Xv8hISq8Lm16jvBZ2VQf+RLmbd7wVUsALibYI/IQ=
golang.org/x/tools v0.38.0/go.mod h1:yEsQ/d/YK8cjh0L6rZlY8tgtlKiBNTL14pGDJPJpYQs=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
google.golang.org/appengine v1.6.8 h1:IhEN5q69dyKagZPYMSdIjS2HqprW324FRQZJcGqPAsM=
google.golang.org/appengine v1.6.8/go.mod h1:1jJ3jBArFh5pcgW8gCtRJnepW8FzD1V44FJffLiz/Ds=
google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
google.golang.org/protobuf v1.36.10 h1:AYd7cD/uASjIL6Q9LiTjz8JLcrh/88q5UObnmY3aOOE=
google.golang.org/protobuf v1.36.10/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j23XfzDpco=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/warnings.v0 v0.1.2 h1:wFXVbFY8DY5/xOe1ECiWdKCzZlxgshcYVNkBHstARME=
gopkg.in/warnings.v0 v0.1.2/go.mod h1:jksf8JmL6Qr/oQM2OXTHunEvvTAsrWBLb6OOjuVWRNI=
gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="LICENSE">
Creative Commons Attribution-NonCommercial 4.0 International

Copyright (c) 2025-2026

This work is licensed under the Creative Commons Attribution-NonCommercial 4.0
International License.

You are free to:
- Share — copy and redistribute the material in any medium or format
- Adapt — remix, transform, and build upon the material

Under the following terms:
- Attribution — You must give appropriate credit, provide a link to the license,
  and indicate if changes were made.
- NonCommercial — You may not use the material for commercial purposes.

No additional restrictions — You may not apply legal terms or technological measures
  that legally restrict others from doing anything the license permits.

To view a copy of this license, visit:
http://creativecommons.org/licenses/by-nc/4.0/

---

SUMMARY: This license allows you to use, modify, and distribute this software for
non-commercial purposes, as long as you credit the original author. Commercial use
is not permitted.
</file>
<file path="Makefile">
# Makefile

BINARY_NAME=repodocs
VERSION=$(shell git describe --tags --always --dirty 2>/dev/null || echo "dev")
BUILD_TIME=$(shell date -u '+%Y-%m-%dT%H:%M:%SZ')
COMMIT=$(shell git rev-parse --short HEAD 2>/dev/null || echo "unknown")
LDFLAGS=-ldflags "-X github.com/quantmind-br/repodocs-go/pkg/version.Version=$(VERSION) -X github.com/quantmind-br/repodocs-go/pkg/version.BuildTime=$(BUILD_TIME) -X github.com/quantmind-br/repodocs-go/pkg/version.Commit=$(COMMIT) -s -w"

BUILD_DIR=./build
INSTALL_DIR=$(HOME)/.local/bin
CONFIG_DIR=$(HOME)/.repodocs

.PHONY: build test test-all coverage lint deps install uninstall release release-dry clean help

build: ## Build the binary
	@mkdir -p $(BUILD_DIR)
	CGO_ENABLED=0 go build $(LDFLAGS) -o $(BUILD_DIR)/$(BINARY_NAME) ./cmd/repodocs

test: ## Run unit tests
	go test -v -race -short ./...

test-all: ## Run all tests (unit + integration + e2e)
	go test -v -race ./...

coverage: ## Generate coverage report (HTML in ./coverage/)
	@mkdir -p ./coverage
	go test -coverprofile=./coverage/coverage.out -covermode=atomic ./...
	go tool cover -html=./coverage/coverage.out -o ./coverage/coverage.html
	@go tool cover -func=./coverage/coverage.out | tail -1
	@echo "Report: ./coverage/coverage.html"

lint: ## Run linters and format code
	@which golangci-lint > /dev/null || go install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@latest
	gofmt -s -w .
	golangci-lint run ./...

deps: ## Download and tidy dependencies
	go mod download
	go mod tidy

install: build ## Build and install to ~/.local/bin
	@mkdir -p $(INSTALL_DIR) $(CONFIG_DIR)
	@install -m 755 $(BUILD_DIR)/$(BINARY_NAME) $(INSTALL_DIR)/
	@test -f $(CONFIG_DIR)/config.yaml || cp ./configs/config.yaml.template $(CONFIG_DIR)/config.yaml
	@echo "✓ Installed to $(INSTALL_DIR)/$(BINARY_NAME)"

uninstall: ## Remove from ~/.local/bin
	@rm -f $(INSTALL_DIR)/$(BINARY_NAME)
	@echo "✓ Uninstalled"

release: ## Create GitHub release (interactive)
	@./scripts/release.sh

release-dry: ## Test release build locally (creates ./dist/)
	@which goreleaser > /dev/null || go install github.com/goreleaser/goreleaser/v2@latest
	goreleaser release --snapshot --clean

clean: ## Remove build artifacts
	@rm -rf $(BUILD_DIR) ./coverage ./dist

help: ## Show this help
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-12s\033[0m %s\n", $$1, $$2}'

.DEFAULT_GOAL := help
</file>
<file path="opencode.json">
{
  "$schema": "https://opencode.ai/config.json",
  "mcp": {
    "serena": {
      "type": "local",
      "command": [
        "uvx",
        "--from", "git+https://github.com/oraios/serena",
        "serena",
        "start-mcp-server",
        "--context", "ide",
        "--project-from-cwd"
      ],
      "enabled": true
    }
  }
}
</file>
<file path="README.md">
# RepoDocs

RepoDocs is a powerful Go-based CLI tool and library designed to extract documentation from diverse sources—including websites, Git repositories, Sitemaps, and Wikis—and convert them into clean, structured Markdown. It is built to facilitate the creation of high-quality datasets for LLM training, RAG (Retrieval-Augmented Generation) pipelines, or local documentation mirrors.

## Features

-   **Multi-Source Extraction**: Automatically detects and handles various source types:
    -   **Web Crawler**: Recursive crawling of documentation sites.
    -   **Git/GitHub**: Cloning repositories or fetching specific paths.
    -   **Sitemaps**: Systematic discovery via `sitemap.xml`.
    -   **llms.txt**: Support for the emerging `llms.txt` standard for LLM-friendly discovery.
    -   **Package Docs**: Specialized handling for `pkg.go.dev`.
-   **Advanced Processing**:
    -   **HTML to Markdown**: Converts complex HTML into clean Markdown using a multi-stage pipeline.
    -   **Content Extraction**: Uses "readability" logic and CSS selectors to isolate main content and remove noise (navbars, footers, scripts).
    -   **JS Rendering**: Headless browser support (via `go-rod`) for Single Page Applications (SPAs) and JavaScript-heavy sites.
-   **Stealth & Robustness**:
    -   **Bot Avoidance**: User-Agent rotation and TLS fingerprinting to bypass basic bot detection.
    -   **Caching**: Persistent caching using BadgerDB to minimize network load and respect rate limits.
    -   **Retries**: Exponential backoff for transient network errors.
-   **AI Integration**: Optional metadata enrichment using LLMs (OpenAI, Anthropic, Google) to generate summaries, tags, and categories.
-   **Structured Output**: Generates Markdown files with YAML frontmatter and a consolidated `repodocs.json` index.

## Installation

### Prerequisites

-   **Go**: 1.21 or later.
-   **Chrome/Chromium**: Required if using the `--render-js` feature for JavaScript rendering.

### From Source

```bash
git clone https://github.com/yourusername/repodocs.git
cd repodocs
go build -o repodocs ./cmd/repodocs
```

### Dependency Check
Use the built-in "doctor" command to verify your environment:
```bash
./repodocs doctor
```

## Testing

RepoDocs has comprehensive test coverage with **64.8% overall coverage** and **9 packages above 90%**.

### Running Tests

```bash
# Unit tests (fast)
make test
# or
go test ./... -short

# Integration tests
make test-integration
# or
go test ./tests/integration/... -tags=integration

# Full test suite
go test ./...

# Coverage report
go test ./... -coverprofile=coverage.out
go tool cover -html=coverage.html
```

### Test Coverage

| Package | Coverage | Status |
|---------|----------|--------|
| git | 100.0% | ✅ Excellent |
| domain | 97.1% | ✅ Excellent |
| manifest | 97.0% | ✅ Excellent |
| state | 95.5% | ✅ Excellent |
| output | 94.4% | ✅ Excellent |
| llm | 93.0% | ✅ Excellent |
| cache | 91.3% | ✅ Excellent |
| utils | 90.1% | ✅ Excellent |
| renderer | 89.1% | ✅ Excellent |
| converter | 87.1% | ✅ Good |
| fetcher | 84.1% | ✅ Good |
| strategies/git | 79.5% | ✅ Good |
| config | 76.0% | ✅ Good |

See [TESTING.md](TESTING.md) for detailed testing guidelines, patterns, and best practices.

## Quick Start

Extract documentation from a URL to the default `./docs` directory:
```bash
repodocs https://docs.example.com
```

Extract a specific GitHub repository with a maximum depth of 2:
```bash
repodocs https://github.com/user/repo --max-depth 2
```

Force JavaScript rendering for a React-based documentation site:
```bash
repodocs https://spa-docs.com --render-js
```

Generate JSON metadata and limit to 10 pages:
```bash
repodocs https://example.com --json-meta --limit 10
```

Process multiple sources from a manifest file:
```bash
repodocs --manifest sources.yaml
```

## Batch Processing with Manifests

For processing multiple documentation sources, RepoDocs supports manifest files in YAML or JSON format. This enables reproducible, one-command data ingestion for complex RAG pipelines.

### Creating a Manifest

Create a `sources.yaml` file:

```yaml
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    max_depth: 3

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "README.md"

options:
  output: ./knowledge-base
  continue_on_error: true
```

### Running with a Manifest

```bash
repodocs --manifest sources.yaml
```

### Manifest Schema

#### Sources

Each source defines a documentation URL and optional configuration:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `url` | string | Yes | URL to extract documentation from |
| `strategy` | string | No | Force a specific strategy (`crawler`, `git`, `sitemap`, etc.) |
| `content_selector` | string | No | CSS selector for main content |
| `exclude_selector` | string | No | CSS selector for elements to remove |
| `exclude` | array | No | URL/path patterns to skip |
| `include` | array | No | Path patterns to include (git strategy) |
| `max_depth` | int | No | Maximum crawl depth |
| `render_js` | bool | No | Force JavaScript rendering |
| `limit` | int | No | Maximum pages from this source |

#### Options

Global options that apply to the entire manifest:

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `continue_on_error` | bool | `false` | Continue processing if a source fails |
| `output` | string | `./docs` | Output directory for all sources |
| `concurrency` | int | `5` | Number of concurrent workers |

### Error Handling

By default, execution stops on the first source failure. Use `continue_on_error: true` to process all sources regardless of individual failures:

```yaml
options:
  continue_on_error: true
```

With this option:
- Failed sources are logged but don't stop execution
- All sources are attempted
- Summary shows success/failure counts
- Exit code is non-zero if any source failed

### Example Manifests

See the `examples/manifests/` directory for sample manifest files:
- `simple.yaml` - Basic single-source manifest
- `multi-source.yaml` - Multiple sources with different strategies
- `error-tolerant.yaml` - Continues on errors
- `full-options.yaml` - All available options documented

## Architecture

RepoDocs follows a decoupled, interface-driven architecture structured as a processing pipeline:

1.  **Detection**: The `Orchestrator` uses a `Strategy Factory` to identify the correct approach (Git, Crawler, Sitemap, etc.) based on the input URL.
2.  **Execution**: The selected `Strategy` orchestrates fetching or cloning content.
3.  **Processing**: The `Converter Pipeline` transforms raw content:
    -   **Encoding**: Normalizes text to UTF-8.
    -   **Sanitization**: Removes unwanted HTML tags and noise.
    -   **Conversion**: Transforms cleaned HTML into Markdown.
4.  **Enhancement**: The `MetadataEnhancer` (optional) uses LLMs to enrich the document with summaries and tags.
5.  **Output**: The `Writer` persists the final Markdown and metadata to the local filesystem.

### Core Components

-   **Internal Domain**: Defines core models (`Document`, `Page`) and interfaces (`Fetcher`, `Renderer`, `Cache`).
-   **Fetcher**: High-level HTTP client with stealth capabilities and caching.
-   **Renderer**: Manages a pool of headless browser tabs for dynamic content.
-   **Strategies**: Specialized logic for different documentation sources.

## Configuration

RepoDocs can be configured via CLI flags, environment variables, or a configuration file (default: `~/.repodocs/config.yaml`).

### Interactive Configuration

Use the interactive TUI to configure RepoDocs:

```bash
repodocs config
```

This opens an interactive terminal interface where you can navigate through configuration categories, edit values with validation, and save changes.

### Configuration Commands

| Command | Description |
|---------|-------------|
| `repodocs config` | Open interactive configuration TUI |
| `repodocs config edit` | Open interactive configuration TUI |
| `repodocs config show` | Display current configuration as YAML |
| `repodocs config init` | Create default config file at ~/.repodocs/config.yaml |
| `repodocs config path` | Show configuration file path |

### Accessibility

For screen reader support, enable accessible mode:

```bash
# Using environment variable
ACCESSIBLE=1 repodocs config

# Using flag
repodocs config --accessible
```

### Configuration Categories

The interactive TUI organizes settings into categories:

- **Output**: Directory, flat structure, overwrite behavior, JSON metadata
- **Concurrency**: Workers, timeout, max crawl depth
- **Cache**: Enable/disable, TTL, cache directory
- **Rendering**: JavaScript rendering, JS timeout, scroll behavior
- **Stealth**: User-Agent, random delays
- **Logging**: Log level, log format
- **LLM**: Provider, API key, model, temperature, metadata enhancement

### Common Flags

| Flag | Short | Description | Default |
| :--- | :--- | :--- | :--- |
| `--manifest` | | Path to manifest file (YAML/JSON) for batch processing | |
| `--output` | `-o` | Output directory | `./docs` |
| `--concurrency` | `-j` | Number of concurrent workers | `5` |
| `--max-depth` | `-d` | Maximum crawl depth | `4` |
| `--limit` | `-l` | Maximum number of pages to process | `0` (unlimited) |
| `--render-js` | | Force JavaScript rendering | `false` |
| `--no-cache` | | Disable the BadgerDB caching layer | `false` |
| `--exclude` | | Regex patterns to exclude specific paths | |
| `--json-meta` | | Generate individual `.json` metadata files | `false` |

## Development

### Running Tests
Execute the test suite:
```bash
go test ./...
```

### Linting
The project follows standard Go formatting. Run the linter:
```bash
go vet ./...
```

### Building
Build the binary for your local architecture:
```bash
go build -o repodocs ./cmd/repodocs
```

## Contributing

1.  Ensure all core services are defined via interfaces in the `internal/domain` package.
2.  When adding new extraction logic, implement the `Strategy` interface in `internal/strategies`.
3.  Add unit tests for new components using `go.uber.org/mock` for dependency mocking.
4.  Ensure any changes to the HTML-to-Markdown pipeline are reflected in the `internal/converter` package.

## License

Refer to the `LICENSE` file for details.
</file>
<file path="TESTING.md">
# Testing Guide

This document provides comprehensive information about testing practices in the repodocs-go project.

## Test Coverage Overview

**Current Coverage**: 64.8% overall

### Coverage by Package

| Package | Coverage | Status |
|---------|----------|--------|
| git | 100.0% | ✅ Excellent |
| domain | 97.1% | ✅ Excellent |
| manifest | 97.0% | ✅ Excellent |
| state | 95.5% | ✅ Excellent |
| output | 94.4% | ✅ Excellent |
| llm | 92.9% | ✅ Excellent |
| utils | 90.9% | ✅ Excellent |
| cache | 91.3% | ✅ Excellent |
| config | 91.4% | ✅ Excellent |
| converter | 87.3% | ✅ Good |
| fetcher | 84.1% | ✅ Good |
| strategies/git | 79.5% | ✅ Good |
| app | 58.8% | ⚠️ Needs improvement |
| strategies | 40.0% | ⚠️ Needs improvement |
| renderer | 30.3% | ⚠️ Needs work |

## Running Tests

### Unit Tests (Fast)
```bash
make test
# or
go test ./... -short
```

### Integration Tests
```bash
make test-integration
# or
go test ./tests/integration/... -tags=integration
```

### E2E Tests
```bash
make test-e2e
# or
go test ./tests/e2e/... -tags=e2e
```

### Coverage Report
```bash
go test ./... -coverprofile=coverage.out
go tool cover -html=coverage.html
open coverage.html
```

## Test Structure

```
tests/
├── unit/               # Fast unit tests with mocks
│   ├── app/           # Application orchestrator tests
│   ├── cache/         # Cache implementation tests
│   ├── config/        # Configuration tests
│   ├── converter/     # HTML/Markdown conversion tests
│   ├── domain/        # Domain models and interfaces
│   ├── fetcher/       # HTTP client tests
│   ├── git/           # Git operations tests
│   ├── llm/           # LLM provider tests
│   ├── manifest/      # Manifest parsing tests
│   ├── output/        # Output writer tests
│   ├── renderer/      # Browser renderer tests
│   ├── state/         # State manager tests
│   └── strategies/    # Strategy pattern tests
├── integration/       # Integration tests with real services
│   ├── fetcher/
│   ├── llm/
│   ├── renderer/
│   └── strategies/
└── e2e/              # End-to-end tests
```

## Testing Patterns

### 1. Table-Driven Tests

Preferred for testing multiple scenarios:

```go
func TestFunction(t *testing.T) {
    tests := []struct {
        name    string
        input   string
        wantErr bool
    }{
        {"valid input", "test", false},
        {"invalid input", "", true},
    }

    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            err := YourFunction(tt.input)
            if tt.wantErr {
                assert.Error(t, err)
            } else {
                assert.NoError(t, err)
            }
        })
    }
}
```

### 2. Mock Testing

Using `go.uber.org/mock` for interfaces:

```go
// Generate mock
//go:generate mockgen -source=domain/cache.go -destination=../../mocks/mock_cache.go

func TestWithMock(t *testing.T) {
    ctrl := gomock.NewController(t)
    defer ctrl.Finish()

    mockCache := mocks.NewMockCache(ctrl)
    mockCache.EXPECT().Get(gomock.Any(), "key").Return([]byte("value"), nil)
}
```

### 3. Test Helpers

Create reusable test helpers:

```go
// tests/testutil/testutil.go
func SetupTestServer(t *testing.T) *httptest.Server {
    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("OK"))
    }))
    t.Cleanup(server.Close)
    return server
}
```

### 4. Fixture Organization

```
tests/
└── fixtures/
    ├── html/           # HTML samples for testing
    │   ├── simple.html
    │   ├── spa.html
    │   └── markdown.html
    └── sitemap/        # Sitemap samples
        ├── small.xml
        └── index.xml
```

## Coverage Targets

- **Minimum**: 80% per package
- **Target**: 90% per package
- **Excellent**: 95%+ per package

Packages below 80% require attention and additional test coverage.

## CI/CD Integration

Tests run automatically on:
- Pull request creation
- Push to main branch
- Scheduled nightly builds

Coverage badges are updated automatically based on test results.

## Best Practices

1. **Write tests first** (TDD when possible)
2. **Keep tests fast** - Use `-short` flag for quick feedback
3. **Use descriptive test names** - `TestFunction_Scenario`
4. **Table-driven for multiple cases** - Better maintainability
5. **Mock external dependencies** - Tests should be deterministic
6. **Clean up resources** - Use `t.Cleanup()` and `defer`
7. **Test error paths** - Not just happy paths
8. **Add benchmarks** - For performance-critical code

## Example: Adding New Tests

```go
// 1. Identify function to test
func MyFunction(input string) (string, error)

// 2. Create test file: tests/unit/mypackage/mypackage_test.go
package mypackage_test

import (
    "testing"
    "github.com/stretchr/testify/assert"
    "github.com/quantmind-br/repodocs-go/internal/mypackage"
)

// 3. Write test
func TestMyFunction(t *testing.T) {
    result, err := mypackage.MyFunction("test")
    assert.NoError(t, err)
    assert.Equal(t, "expected", result)
}

// 4. Run test
// go test ./tests/unit/mypackage/ -v
```

## Coverage Reports

Generate detailed coverage reports:

```bash
# Function coverage
go test ./internal/... -coverprofile=coverage.out
go tool cover -func=coverage.out | grep -v "100.0%"

# HTML report
go tool cover -html=coverage.out -o coverage.html

# By package
go test ./internal/... -coverprofile=coverage.out
go tool cover -func=coverage.out | grep "^github.com"
```

## Troubleshooting

### Tests Timing Out
- Use `-short` flag to skip integration tests
- Check for infinite loops in goroutines
- Add explicit timeouts to contexts

### Browser Tests Failing
- Ensure Chrome/Chromium is installed
- Check `renderer.IsAvailable()` before running
- Use `testing.Short()` to skip when browser unavailable

### Cache Issues
- Clear cache: `rm -rf ~/.repodocs-cache`
- Run with cache disabled: `REPODOCS_CACHE=off ./repodocs`

### Mock Generation
```bash
# Install mockgen
go install go.uber.org/mock/mockgen@latest

# Generate mocks
go generate ./...
```

## Additional Resources

- [Effective Go Testing](https://go.dev/doc/effective_go.html#testing)
- [Table-Driven Tests](https://dave.cheney.net/2019/03/04/table-driven-tests-in-go)
- [Test Coverage](https://blog.golang.org/cover/go14)
- [Go Mock](https://github.com/golang/mock)
</file>
 : Complete codebase content
  - 2026-01-24 14:07:48   : Analysis date
-->

## Role

You are a **Senior Performance Engineer**. Your task is to analyze the codebase and identify performance bottlenecks, optimization opportunities, and efficiency improvements.

---

## Context

**Date:** 2026-01-24 14:07:48

---

## Codebase

└── repodocs-go/
    ├── build/
    ├── cmd/
    │   └── repodocs/
    │       └── main.go [15.2KB]
    ├── configs/
    │   └── config.yaml.template [4.6KB]
    ├── coverage/
    ├── e2e_output/
    ├── examples/
    │   └── manifests/
    │       ├── error-tolerant.yaml [180B]
    │       ├── full-options.yaml [573B]
    │       ├── multi-source.yaml [347B]
    │       └── simple.yaml [43B]
    ├── internal/
    │   ├── app/
    │   │   ├── detector.go [5.0KB]
    │   │   └── orchestrator.go [9.6KB]
    │   ├── cache/
    │   │   ├── badger.go [3.3KB]
    │   │   ├── interface.go [1.0KB]
    │   │   └── keys.go [1.8KB]
    │   ├── config/
    │   │   ├── config.go [4.9KB]
    │   │   ├── defaults.go [3.6KB]
    │   │   └── loader.go [4.9KB]
    │   ├── converter/
    │   │   ├── content_type.go [1.6KB]
    │   │   ├── encoding.go [2.3KB]
    │   │   ├── markdown.go [4.8KB]
    │   │   ├── markdown_reader.go [6.6KB]
    │   │   ├── pipeline.go [6.1KB]
    │   │   ├── plaintext_reader.go [3.3KB]
    │   │   ├── readability.go [6.7KB]
    │   │   └── sanitizer.go [5.5KB]
    │   ├── domain/
    │   │   ├── errors.go [5.9KB]
    │   │   ├── interfaces.go [3.3KB]
    │   │   ├── models.go [8.7KB]
    │   │   └── options.go [392B]
    │   ├── fetcher/
    │   │   ├── client.go [6.2KB]
    │   │   ├── retry.go [3.7KB]
    │   │   ├── stealth.go [4.7KB]
    │   │   └── transport.go [1.6KB]
    │   ├── git/
    │   │   ├── client.go [470B]
    │   │   └── interface.go [257B]
    │   ├── llm/
    │   │   ├── anthropic.go [6.6KB]
    │   │   ├── circuit_breaker.go [3.4KB]
    │   │   ├── google.go [6.1KB]
    │   │   ├── metadata.go [6.7KB]
    │   │   ├── openai.go [5.2KB]
    │   │   ├── provider.go [2.2KB]
    │   │   ├── provider_wrapper.go [3.9KB]
    │   │   ├── ratelimit.go [2.2KB]
    │   │   └── retry.go [4.1KB]
    │   ├── manifest/
    │   │   ├── doc.go [1.2KB]
    │   │   ├── errors.go [768B]
    │   │   ├── loader.go [1.6KB]
    │   │   └── types.go [1.9KB]
    │   ├── output/
    │   │   ├── collector.go [2.5KB]
    │   │   └── writer.go [3.1KB]
    │   ├── renderer/
    │   │   ├── detector.go [3.9KB]
    │   │   ├── pool.go [2.2KB]
    │   │   ├── rod.go [6.8KB]
    │   │   └── stealth.go [1.7KB]
    │   ├── state/
    │   │   ├── errors.go [415B]
    │   │   ├── manager.go [3.3KB]
    │   │   └── models.go [1.6KB]
    │   ├── strategies/
    │   │   ├── git/
    │   │   │   ├── archive.go [3.7KB]
    │   │   │   ├── clone.go [2.0KB]
    │   │   │   ├── doc.go [690B]
    │   │   │   ├── fetcher.go [167B]
    │   │   │   ├── parser.go [3.4KB]
    │   │   │   ├── processor.go [4.0KB]
    │   │   │   ├── strategy.go [5.9KB]
    │   │   │   └── types.go [1.4KB]
    │   │   ├── crawler.go [6.9KB]
    │   │   ├── docsrs.go [8.9KB]
    │   │   ├── docsrs_json.go [3.9KB]
    │   │   ├── docsrs_renderer.go [13.2KB]
    │   │   ├── docsrs_types.go [15.6KB]
    │   │   ├── git_compat.go [3.9KB]
    │   │   ├── github_pages.go [14.2KB]
    │   │   ├── github_pages_discovery.go [8.9KB]
    │   │   ├── llms.go [6.3KB]
    │   │   ├── pkggo.go [4.7KB]
    │   │   ├── sitemap.go [9.2KB]
    │   │   ├── strategy.go [9.8KB]
    │   │   ├── wiki.go [7.3KB]
    │   │   └── wiki_parser.go [7.6KB]
    │   ├── tui/
    │   │   ├── app.go [7.6KB]
    │   │   ├── categories.go [1.6KB]
    │   │   ├── config_adapter.go [9.6KB]
    │   │   ├── forms.go [10.5KB]
    │   │   ├── styles.go [2.0KB]
    │   │   └── validation.go [3.4KB]
    │   └── utils/
    │       ├── fs.go [6.6KB]
    │       ├── logger.go [2.2KB]
    │       ├── progress.go [1.5KB]
    │       ├── url.go [10.6KB]
    │       └── workerpool.go [5.0KB]
    ├── pkg/
    │   └── version/
    │       └── version.go [1.0KB]
    ├── scripts/
    │   └── release.sh [2.3KB]
    ├── tests/
    ├── AGENTS.md [4.1KB]
    ├── CLAUDE.md [1.7KB]
    ├── LICENSE [994B]
    ├── Makefile [2.4KB]
    ├── README.md [10.2KB]
    ├── TESTING.md [6.4KB]
    ├── go.mod [5.9KB]
    ├── go.sum [35.7KB]
    └── opencode.json [338B]

<file path="cmd/repodocs/main.go">
package main

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"os/exec"
	"os/signal"
	"syscall"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/app"
	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/manifest"
	"github.com/quantmind-br/repodocs-go/internal/tui"
	"github.com/quantmind-br/repodocs-go/internal/utils"
	"github.com/quantmind-br/repodocs-go/pkg/version"
	"github.com/spf13/cobra"
	"github.com/spf13/viper"
	"gopkg.in/yaml.v3"
)

var (
	cfgFile      string
	verbose      bool
	manifestPath string
	log          *utils.Logger

	// Dependencies for testing
	osStat       = os.Stat
	execLookPath = exec.LookPath
)

func main() {
	if err := rootCmd.Execute(); err != nil {
		fmt.Fprintln(os.Stderr, err)
		os.Exit(1)
	}
}

var rootCmd = &cobra.Command{
	Use:   "repodocs [url]",
	Short: "Extract documentation from any source",
	Long: `RepoDocs is a CLI tool for extracting documentation from websites,
git repositories, sitemaps, pkg.go.dev, and llms.txt files.

It supports stealth mode for avoiding bot detection and JavaScript rendering
for single-page applications.`,
	Version: version.Short(),
	Args:    cobra.MaximumNArgs(1),
	RunE:    run,
}

func init() {
	cobra.OnInitialize(initConfig)

	// Global flags
	rootCmd.PersistentFlags().StringVar(&cfgFile, "config", "", "config file (default is ~/.repodocs/config.yaml)")
	rootCmd.PersistentFlags().StringP("output", "o", "./docs", "Output directory")
	rootCmd.PersistentFlags().IntP("concurrency", "j", 5, "Number of concurrent workers")
	rootCmd.PersistentFlags().IntP("limit", "l", 0, "Max pages to process (0=unlimited)")
	rootCmd.PersistentFlags().IntP("max-depth", "d", 4, "Max crawl depth")
	rootCmd.PersistentFlags().StringSlice("exclude", nil, "Regex patterns to exclude")
	rootCmd.PersistentFlags().String("filter", "", "Path filter (web: base URL; git: subdirectory)")
	rootCmd.PersistentFlags().Bool("nofolders", false, "Flat output structure")
	rootCmd.PersistentFlags().Bool("force", false, "Overwrite existing files")
	rootCmd.PersistentFlags().BoolVarP(&verbose, "verbose", "v", false, "Verbose output")

	// Cache flags
	rootCmd.PersistentFlags().Bool("no-cache", false, "Disable caching")
	rootCmd.PersistentFlags().Duration("cache-ttl", 24*time.Hour, "Cache TTL")
	rootCmd.PersistentFlags().Bool("refresh-cache", false, "Force cache refresh")

	// Rendering flags
	rootCmd.PersistentFlags().Bool("render-js", false, "Force JS rendering")
	rootCmd.PersistentFlags().Duration("timeout", 30*time.Second, "Request timeout")

	// Output flags
	rootCmd.PersistentFlags().Bool("json-meta", false, "Generate JSON metadata files")
	rootCmd.PersistentFlags().Bool("dry-run", false, "Simulate without writing files")

	// Specific flags
	rootCmd.PersistentFlags().Bool("split", false, "Split output by sections (pkg.go.dev)")
	rootCmd.PersistentFlags().Bool("include-assets", false, "Include referenced images (git)")
	rootCmd.PersistentFlags().String("user-agent", "", "Custom User-Agent")
	rootCmd.PersistentFlags().String("content-selector", "", "CSS selector for main content")
	rootCmd.PersistentFlags().String("exclude-selector", "", "CSS selector for elements to exclude from content")
	rootCmd.PersistentFlags().StringVar(&manifestPath, "manifest", "", "Path to manifest file (YAML/JSON) for batch processing")

	// Sync flags
	rootCmd.PersistentFlags().Bool("sync", false, "Enable incremental sync mode (skip unchanged pages)")
	rootCmd.PersistentFlags().Bool("full-sync", false, "Force full re-processing (ignore state)")
	rootCmd.PersistentFlags().Bool("prune", false, "Remove files for deleted pages")

	// Bind flags to viper
	_ = viper.BindPFlag("output.directory", rootCmd.PersistentFlags().Lookup("output"))
	_ = viper.BindPFlag("concurrency.workers", rootCmd.PersistentFlags().Lookup("concurrency"))
	_ = viper.BindPFlag("concurrency.max_depth", rootCmd.PersistentFlags().Lookup("max-depth"))
	_ = viper.BindPFlag("concurrency.timeout", rootCmd.PersistentFlags().Lookup("timeout"))
	_ = viper.BindPFlag("output.flat", rootCmd.PersistentFlags().Lookup("nofolders"))
	_ = viper.BindPFlag("output.overwrite", rootCmd.PersistentFlags().Lookup("force"))
	_ = viper.BindPFlag("cache.enabled", rootCmd.PersistentFlags().Lookup("no-cache"))
	_ = viper.BindPFlag("cache.ttl", rootCmd.PersistentFlags().Lookup("cache-ttl"))
	_ = viper.BindPFlag("rendering.force_js", rootCmd.PersistentFlags().Lookup("render-js"))
	_ = viper.BindPFlag("output.json_metadata", rootCmd.PersistentFlags().Lookup("json-meta"))
	_ = viper.BindPFlag("stealth.user_agent", rootCmd.PersistentFlags().Lookup("user-agent"))

	// Add subcommands
	rootCmd.AddCommand(doctorCmd)
	rootCmd.AddCommand(versionCmd)
	rootCmd.AddCommand(configCmd)
}

func initConfig() {
	if cfgFile != "" {
		viper.SetConfigFile(cfgFile)
	}
}

func run(cmd *cobra.Command, args []string) error {
	// Initialize logger
	logLevel := "info"
	if verbose {
		logLevel = "debug"
	}
	log = utils.NewLogger(utils.LoggerOptions{
		Level:   logLevel,
		Format:  "pretty",
		Verbose: verbose,
	})

	// Load configuration
	cfg, err := config.Load()
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	if manifestPath != "" {
		if len(args) > 0 {
			return fmt.Errorf("cannot specify both --manifest and URL argument")
		}
		return runManifest(cmd, cfg)
	}

	if len(args) == 0 {
		return cmd.Help()
	}

	url := args[0]

	// Handle output directory:
	// 1. If user explicitly provided -o flag, use that value
	// 2. Otherwise, generate from URL
	if cmd.Flags().Changed("output") {
		// User explicitly set output directory via -o flag
		cfg.Output.Directory, _ = cmd.Flags().GetString("output")
	} else {
		// Use URL-based output directory
		cfg.Output.Directory = utils.GenerateOutputDirFromURL(url)
	}

	// Create context with cancellation
	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// Handle graceful shutdown
	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigCh
		log.Info().Msg("Shutting down gracefully...")
		cancel()
	}()

	limit, _ := cmd.Flags().GetInt("limit")
	dryRun, _ := cmd.Flags().GetBool("dry-run")
	split, _ := cmd.Flags().GetBool("split")
	includeAssets, _ := cmd.Flags().GetBool("include-assets")
	contentSelector, _ := cmd.Flags().GetString("content-selector")
	excludeSelector, _ := cmd.Flags().GetString("exclude-selector")
	excludePatterns, _ := cmd.Flags().GetStringSlice("exclude")
	renderJS, _ := cmd.Flags().GetBool("render-js")
	force, _ := cmd.Flags().GetBool("force")
	filterURL, _ := cmd.Flags().GetString("filter")
	syncEnabled, _ := cmd.Flags().GetBool("sync")
	fullSync, _ := cmd.Flags().GetBool("full-sync")
	prune, _ := cmd.Flags().GetBool("prune")

	orchOpts := app.OrchestratorOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  verbose,
			DryRun:   dryRun,
			Force:    force,
			RenderJS: renderJS,
			Limit:    limit,
			Sync:     syncEnabled,
			FullSync: fullSync,
			Prune:    prune,
		},
		Config:          cfg,
		Split:           split,
		IncludeAssets:   includeAssets,
		ContentSelector: contentSelector,
		ExcludeSelector: excludeSelector,
		ExcludePatterns: excludePatterns,
		FilterURL:       filterURL,
	}

	// Create orchestrator
	orchestrator, err := app.NewOrchestrator(orchOpts)
	if err != nil {
		return fmt.Errorf("failed to create orchestrator: %w", err)
	}
	defer orchestrator.Close()

	// Validate URL
	if err := orchestrator.ValidateURL(url); err != nil {
		return err
	}

	return orchestrator.Run(ctx, url, orchOpts)
}

func runManifest(cmd *cobra.Command, cfg *config.Config) error {
	loader := manifest.NewLoader()
	manifestCfg, err := loader.Load(manifestPath)
	if err != nil {
		return fmt.Errorf("failed to load manifest: %w", err)
	}

	if manifestCfg.Options.Output != "" {
		cfg.Output.Directory = manifestCfg.Options.Output
	}
	if manifestCfg.Options.Concurrency > 0 {
		cfg.Concurrency.Workers = manifestCfg.Options.Concurrency
	}
	if manifestCfg.Options.CacheTTL > 0 {
		cfg.Cache.TTL = manifestCfg.Options.CacheTTL
	}

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	sigCh := make(chan os.Signal, 1)
	signal.Notify(sigCh, syscall.SIGINT, syscall.SIGTERM)

	go func() {
		<-sigCh
		log.Info().Msg("Shutting down gracefully...")
		cancel()
	}()

	limit, _ := cmd.Flags().GetInt("limit")
	dryRun, _ := cmd.Flags().GetBool("dry-run")
	split, _ := cmd.Flags().GetBool("split")
	includeAssets, _ := cmd.Flags().GetBool("include-assets")
	contentSelector, _ := cmd.Flags().GetString("content-selector")
	excludeSelector, _ := cmd.Flags().GetString("exclude-selector")
	excludePatterns, _ := cmd.Flags().GetStringSlice("exclude")
	renderJS, _ := cmd.Flags().GetBool("render-js")
	force, _ := cmd.Flags().GetBool("force")
	filterURL, _ := cmd.Flags().GetString("filter")
	syncEnabled, _ := cmd.Flags().GetBool("sync")
	fullSync, _ := cmd.Flags().GetBool("full-sync")
	prune, _ := cmd.Flags().GetBool("prune")

	orchOpts := app.OrchestratorOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  verbose,
			DryRun:   dryRun,
			Force:    force,
			RenderJS: renderJS,
			Limit:    limit,
			Sync:     syncEnabled,
			FullSync: fullSync,
			Prune:    prune,
		},
		Config:          cfg,
		Split:           split,
		IncludeAssets:   includeAssets,
		ContentSelector: contentSelector,
		ExcludeSelector: excludeSelector,
		ExcludePatterns: excludePatterns,
		FilterURL:       filterURL,
	}

	orchestrator, err := app.NewOrchestrator(orchOpts)
	if err != nil {
		return fmt.Errorf("failed to create orchestrator: %w", err)
	}
	defer orchestrator.Close()

	return orchestrator.RunManifest(ctx, manifestCfg, orchOpts)
}

var doctorCmd = &cobra.Command{
	Use:   "doctor",
	Short: "Check system dependencies",
	Long:  "Verifies that all system dependencies are properly installed and configured.",
	RunE: func(cmd *cobra.Command, args []string) error {
		fmt.Println("Checking system dependencies...")
		allPassed := true

		// Check 1: Internet connection
		fmt.Print("  Internet connection: ")
		if checkInternet() {
			fmt.Println("OK")
		} else {
			fmt.Println("FAILED")
			allPassed = false
		}

		// Check 2: Chrome/Chromium
		fmt.Print("  Chrome/Chromium: ")
		if chromePath := checkChrome(); chromePath != "" {
			fmt.Printf("OK (%s)\n", chromePath)
		} else {
			fmt.Println("NOT FOUND (JS rendering will be unavailable)")
		}

		// Check 3: Write permissions for output dir
		fmt.Print("  Write permissions: ")
		if checkWritePermissions() {
			fmt.Println("OK")
		} else {
			fmt.Println("FAILED")
			allPassed = false
		}

		// Check 4: Config file
		fmt.Print("  Config file: ")
		_, err := config.Load()
		if err != nil {
			fmt.Printf("WARN (%v)\n", err)
		} else {
			fmt.Println("OK")
		}

		// Check 5: Cache directory
		fmt.Print("  Cache directory: ")
		cacheDir := utils.ExpandPath("~/.repodocs/cache")
		if checkCacheDir(cacheDir) {
			fmt.Printf("OK (%s)\n", cacheDir)
		} else {
			fmt.Println("WARN (will be created on first use)")
		}

		fmt.Println()
		if allPassed {
			fmt.Println("All critical checks passed!")
		} else {
			fmt.Println("Some checks failed. Please resolve the issues above.")
		}
		return nil
	},
}

// checkInternet checks if there's an internet connection
func checkInternet() bool {
	ctx, cancel := context.WithTimeout(context.Background(), 5*time.Second)
	defer cancel()

	req, err := http.NewRequestWithContext(ctx, http.MethodHead, "https://www.google.com", nil)
	if err != nil {
		return false
	}

	client := &http.Client{Timeout: 5 * time.Second}
	resp, err := client.Do(req)
	if err != nil {
		return false
	}
	resp.Body.Close()
	return resp.StatusCode < 400
}

// checkChrome checks if Chrome/Chromium is available
func checkChrome() string {
	// Common Chrome/Chromium paths
	paths := []string{
		// Linux
		"/usr/bin/google-chrome",
		"/usr/bin/google-chrome-stable",
		"/usr/bin/chromium",
		"/usr/bin/chromium-browser",
		"/snap/bin/chromium",
		// macOS
		"/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
		"/Applications/Chromium.app/Contents/MacOS/Chromium",
		// Windows (via PATH)
		"chrome.exe",
		"chromium.exe",
	}

	for _, path := range paths {
		if _, err := osStat(path); err == nil {
			return path
		}
	}

	// Try to find via which/where command
	if path, err := execLookPath("google-chrome"); err == nil {
		return path
	}
	if path, err := execLookPath("chromium"); err == nil {
		return path
	}
	if path, err := execLookPath("chromium-browser"); err == nil {
		return path
	}

	return ""
}

// checkWritePermissions checks if we can write to the current directory
func checkWritePermissions() bool {
	tmpFile := ".repodocs_test_write"
	f, err := os.Create(tmpFile)
	if err != nil {
		return false
	}
	f.Close()
	os.Remove(tmpFile)
	return true
}

// checkCacheDir checks if the cache directory exists or can be created
func checkCacheDir(path string) bool {
	info, err := os.Stat(path)
	if err != nil {
		return false
	}
	return info.IsDir()
}

var versionCmd = &cobra.Command{
	Use:   "version",
	Short: "Print version information",
	Run: func(cmd *cobra.Command, args []string) {
		fmt.Println(version.Full())
	},
}

var configCmd = &cobra.Command{
	Use:   "config",
	Short: "Manage RepoDocs configuration",
	Long:  "View, edit, and manage RepoDocs configuration interactively.",
	RunE:  runConfigEdit,
}

var configEditCmd = &cobra.Command{
	Use:   "edit",
	Short: "Edit configuration interactively",
	Long:  "Open an interactive TUI to edit RepoDocs configuration.",
	RunE:  runConfigEdit,
}

var configShowCmd = &cobra.Command{
	Use:   "show",
	Short: "Show current configuration",
	Long:  "Display the current configuration in YAML format.",
	RunE:  runConfigShow,
}

var configInitCmd = &cobra.Command{
	Use:   "init",
	Short: "Initialize configuration file",
	Long:  "Create a default configuration file at ~/.repodocs/config.yaml.",
	RunE:  runConfigInit,
}

var configPathCmd = &cobra.Command{
	Use:   "path",
	Short: "Show configuration file path",
	Long:  "Display the path to the configuration file.",
	Run: func(cmd *cobra.Command, args []string) {
		fmt.Println(config.ConfigFilePath())
	},
}

var accessibleMode bool

func init() {
	configCmd.PersistentFlags().BoolVar(&accessibleMode, "accessible", false, "Enable accessible mode for screen readers")
	configCmd.AddCommand(configEditCmd)
	configCmd.AddCommand(configShowCmd)
	configCmd.AddCommand(configInitCmd)
	configCmd.AddCommand(configPathCmd)
}

func runConfigEdit(cmd *cobra.Command, args []string) error {
	cfg, err := config.Load()
	if err != nil {
		cfg = config.Default()
	}

	accessible := accessibleMode || os.Getenv("ACCESSIBLE") != ""

	return tui.Run(tui.Options{
		Config:     cfg,
		SaveFunc:   config.Save,
		Accessible: accessible,
	})
}

func runConfigShow(cmd *cobra.Command, args []string) error {
	cfg, err := config.Load()
	if err != nil {
		return fmt.Errorf("failed to load config: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	fmt.Println(string(data))
	return nil
}

func runConfigInit(cmd *cobra.Command, args []string) error {
	path := config.ConfigFilePath()

	if _, err := os.Stat(path); err == nil {
		return fmt.Errorf("config file already exists at %s (use --force to overwrite)", path)
	}

	cfg := config.Default()
	if err := config.Save(cfg); err != nil {
		return fmt.Errorf("failed to create config: %w", err)
	}

	fmt.Printf("Created default configuration at %s\n", path)
	return nil
}
</file>
<file path="configs/config.yaml.template">
# repodocs configuration file
# Location: ~/.repodocs/config.yaml
#
# All settings can be overridden via environment variables with REPODOCS_ prefix
# Example: REPODOCS_LLM_API_KEY, REPODOCS_CACHE_ENABLED

# =============================================================================
# LLM Configuration
# =============================================================================
# Supports: OpenAI-compatible (OpenAI, Ollama, vLLM, Groq, Together AI, etc.),
#           Anthropic, and Google Gemini APIs
#
llm:
  # Provider type: openai, anthropic, google
  # provider: openai

  # API key (recommended: use REPODOCS_LLM_API_KEY env var instead)
  # api_key: sk-...

  # Base URL of the API endpoint
  # Examples:
  #   OpenAI:      https://api.openai.com/v1
  #   Ollama:      http://localhost:11434/v1
  #   vLLM:        http://localhost:8000/v1
  #   Groq:        https://api.groq.com/openai/v1
  #   Together AI: https://api.together.xyz/v1
  #   Anthropic:   https://api.anthropic.com
  #   Google:      https://generativelanguage.googleapis.com
  # base_url: http://localhost:11434/v1

  # Model ID (no default - must be specified by user)
  # model: llama3.2

  # Optional parameters (have sensible defaults)
  max_tokens: 4096
  temperature: 0.7
  timeout: 60s
  max_retries: 3

  # Enable LLM-enhanced metadata generation (summary, tags, category)
  enhance_metadata: false

  # Rate limiting configuration for LLM API requests
  rate_limit:
    # Enable rate limiting (recommended for API quotas)
    enabled: true

    # Maximum requests per minute (adjust based on your API tier)
    # OpenAI: 60-10000+ RPM depending on tier
    # Google Gemini: 60-1000+ RPM depending on plan
    # Anthropic: 60-4000 RPM depending on tier
    requests_per_minute: 60

    # Burst size (max concurrent requests before throttling)
    burst_size: 10

    # Retry configuration for transient errors (429, 5xx)
    max_retries: 3
    initial_delay: 1s
    max_delay: 60s
    multiplier: 2.0

    # Circuit breaker configuration (prevents cascading failures)
    circuit_breaker:
      enabled: true
      failure_threshold: 5
      success_threshold_half_open: 1
      reset_timeout: 30s

# =============================================================================
# Output Configuration
# =============================================================================
output:
  # Default output directory (can be overridden with -o flag)
  directory: "./docs"

  # Flat structure (no subdirectories)
  flat: false

  # Generate JSON metadata files
  json_metadata: false

  # Overwrite existing files
  overwrite: false

# =============================================================================
# Concurrency Configuration
# =============================================================================
concurrency:
  # Number of concurrent workers
  workers: 5

  # Request timeout
  timeout: 30s

  # Maximum crawl depth
  max_depth: 4

# =============================================================================
# Cache Configuration
# =============================================================================
cache:
  # Enable caching (recommended)
  enabled: true

  # Cache time-to-live
  ttl: 24h

  # Cache directory (default: ~/.cache/repodocs)
  # directory: ~/.cache/repodocs

# =============================================================================
# JavaScript Rendering Configuration
# =============================================================================
rendering:
  # Force JavaScript rendering for all pages
  force_js: false

  # Timeout for JavaScript execution
  js_timeout: 10s

  # Scroll to end of page (for infinite scroll sites)
  scroll_to_end: false

# =============================================================================
# Stealth Configuration
# =============================================================================
stealth:
  # Custom User-Agent (empty = use default)
  user_agent: ""

  # Random delay between requests (anti-detection)
  random_delay_min: 500ms
  random_delay_max: 2s

# =============================================================================
# Logging Configuration
# =============================================================================
logging:
  # Log level: debug, info, warn, error
  level: info

  # Log format: text, json
  format: text

# =============================================================================
# URL Exclusion Patterns
# =============================================================================
# Glob patterns for URLs to skip during crawling
exclude:
  - "*/login*"
  - "*/logout*"
  - "*/admin/*"
  - "*.pdf"
  - "*.zip"
  - "*.tar.gz"
</file>
<file path="examples/manifests/error-tolerant.yaml">
sources:
  - url: https://docs.example.com
  - url: https://unstable-service.example.com
  - url: https://api.example.com/docs

options:
  continue_on_error: true
  output: ./docs
</file>
<file path="examples/manifests/full-options.yaml">
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    exclude_selector: ".sidebar, .footer, .nav"
    exclude:
      - "*/changelog/*"
      - "*/archive/*"
    max_depth: 4
    render_js: true
    limit: 100

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "*.md"
    exclude:
      - "docs/internal/*"

  - url: https://pkg.go.dev/github.com/org/repo
    strategy: pkggo

options:
  continue_on_error: true
  output: ./knowledge-base
  concurrency: 5
  cache_ttl: 24h
</file>
<file path="examples/manifests/multi-source.yaml">
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    max_depth: 3

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "README.md"

  - url: https://api.example.com/docs
    strategy: sitemap

options:
  output: ./knowledge-base
  concurrency: 10
</file>
<file path="examples/manifests/simple.yaml">
sources:
  - url: https://docs.example.com
</file>
<file path="internal/app/detector.go">
package app

import (
	"net/url"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/strategies"
)

// StrategyType represents the type of extraction strategy
type StrategyType string

const (
	StrategyLLMS        StrategyType = "llms"
	StrategyPkgGo       StrategyType = "pkggo"
	StrategyDocsRS      StrategyType = "docsrs"
	StrategySitemap     StrategyType = "sitemap"
	StrategyWiki        StrategyType = "wiki"
	StrategyGitHubPages StrategyType = "github_pages"
	StrategyGit         StrategyType = "git"
	StrategyCrawler     StrategyType = "crawler"
	StrategyUnknown     StrategyType = "unknown"
)

var validStrategies = map[StrategyType]bool{
	StrategyLLMS:        true,
	StrategyPkgGo:       true,
	StrategyDocsRS:      true,
	StrategySitemap:     true,
	StrategyWiki:        true,
	StrategyGitHubPages: true,
	StrategyGit:         true,
	StrategyCrawler:     true,
}

func IsValidStrategy(s StrategyType) bool {
	return validStrategies[s]
}

// DetectStrategy determines the appropriate strategy based on URL patterns
func DetectStrategy(rawURL string) StrategyType {
	// Trim whitespace
	rawURL = strings.TrimSpace(rawURL)
	if rawURL == "" {
		return StrategyUnknown
	}

	lower := strings.ToLower(rawURL)

	// Check for SSH Git URLs first (git@host:path/repo.git)
	// These don't parse with url.Parse, so handle them before parsing
	if strings.HasPrefix(rawURL, "git@") || strings.HasPrefix(rawURL, "git+ssh://") {
		return StrategyGit
	}

	// Parse URL to strip query and fragment for path-based matching
	parsed, err := url.Parse(rawURL)
	if err != nil {
		// If URL parsing fails, do basic checks on the raw string
		if strings.HasPrefix(lower, "http://") || strings.HasPrefix(lower, "https://") {
			return StrategyCrawler
		}
		return StrategyUnknown
	}

	// Check if the URL has a valid host (for cases like "https://")
	if parsed.Host == "" && (parsed.Scheme == "http" || parsed.Scheme == "https") {
		return StrategyUnknown
	}

	// For path-based matching, use the path without query/fragment
	path := parsed.Path
	lowerPath := strings.ToLower(path)

	// Check for git:// protocol (unsupported)
	if parsed.Scheme == "git" {
		return StrategyUnknown
	}

	// Check for llms.txt first (using path without query/fragment)
	if strings.HasSuffix(lowerPath, "/llms.txt") || strings.HasSuffix(lowerPath, "llms.txt") {
		return StrategyLLMS
	}

	if strings.Contains(lower, "pkg.go.dev") {
		return StrategyPkgGo
	}

	if strings.Contains(lower, "docs.rs") {
		if !strings.Contains(lowerPath, "/src/") && !strings.Contains(lowerPath, "/source/") {
			return StrategyDocsRS
		}
	}

	if strings.HasSuffix(lowerPath, "sitemap.xml") ||
		strings.HasSuffix(lowerPath, "sitemap.xml.gz") ||
		strings.Contains(lowerPath, "sitemap") && strings.HasSuffix(lowerPath, ".xml") {
		return StrategySitemap
	}

	// Check for Wiki (before generic Git) - pass raw URL to support all wiki patterns
	if strategies.IsWikiURL(rawURL) {
		return StrategyWiki
	}

	// Check for GitHub Pages (*.github.io) - after Wiki, before Git
	if strategies.IsGitHubPagesURL(rawURL) {
		return StrategyGitHubPages
	}

	// Check for Git repository
	// Exclude known documentation/pages subdomains
	isDocsSubdomain := strings.Contains(lower, "docs.github.com") ||
		strings.Contains(lower, "pages.github.io") ||
		strings.Contains(lower, "github.io")

	if !isDocsSubdomain && (strings.HasSuffix(lowerPath, ".git") ||
		(strings.Contains(lower, "github.com") && !strings.Contains(lowerPath, "/blob/")) ||
		(strings.Contains(lower, "gitlab.com") && !strings.Contains(lowerPath, "/-/blob/")) ||
		strings.Contains(lower, "bitbucket.org")) {
		return StrategyGit
	}

	// Default to crawler for HTTP URLs
	if parsed.Scheme == "http" || parsed.Scheme == "https" {
		return StrategyCrawler
	}

	return StrategyUnknown
}

func CreateStrategy(strategyType StrategyType, deps *strategies.Dependencies) strategies.Strategy {
	switch strategyType {
	case StrategyLLMS:
		return strategies.NewLLMSStrategy(deps)
	case StrategyPkgGo:
		return strategies.NewPkgGoStrategy(deps)
	case StrategyDocsRS:
		return strategies.NewDocsRSStrategy(deps)
	case StrategySitemap:
		return strategies.NewSitemapStrategy(deps)
	case StrategyWiki:
		return strategies.NewWikiStrategy(deps)
	case StrategyGitHubPages:
		return strategies.NewGitHubPagesStrategy(deps)
	case StrategyGit:
		return strategies.NewGitStrategy(deps)
	case StrategyCrawler:
		return strategies.NewCrawlerStrategy(deps)
	default:
		return nil
	}
}

func GetAllStrategies(deps *strategies.Dependencies) []strategies.Strategy {
	return []strategies.Strategy{
		strategies.NewLLMSStrategy(deps),
		strategies.NewPkgGoStrategy(deps),
		strategies.NewDocsRSStrategy(deps),
		strategies.NewSitemapStrategy(deps),
		strategies.NewWikiStrategy(deps),
		strategies.NewGitHubPagesStrategy(deps),
		strategies.NewGitStrategy(deps),
		strategies.NewCrawlerStrategy(deps),
	}
}

func FindMatchingStrategy(url string, deps *strategies.Dependencies) strategies.Strategy {
	for _, strategy := range GetAllStrategies(deps) {
		if strategy.CanHandle(url) {
			return strategy
		}
	}
	return nil
}
</file>
<file path="internal/app/orchestrator.go">
package app

import (
	"context"
	"fmt"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/manifest"
	"github.com/quantmind-br/repodocs-go/internal/strategies"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// Orchestrator coordinates the documentation extraction process
type Orchestrator struct {
	config          *config.Config
	deps            *strategies.Dependencies
	logger          *utils.Logger
	strategyFactory func(StrategyType, *strategies.Dependencies) strategies.Strategy
}

// OrchestratorOptions contains options for creating an orchestrator
type OrchestratorOptions struct {
	domain.CommonOptions
	Config           *config.Config
	Split            bool
	IncludeAssets    bool
	ContentSelector  string
	ExcludeSelector  string
	ExcludePatterns  []string
	FilterURL        string
	StrategyFactory  func(StrategyType, *strategies.Dependencies) strategies.Strategy
	StrategyOverride string
}

// NewOrchestrator creates a new orchestrator with the given configuration
func NewOrchestrator(opts OrchestratorOptions) (*Orchestrator, error) {
	cfg := opts.Config

	// Validate config
	if cfg == nil {
		return nil, fmt.Errorf("config is required")
	}

	// Create logger
	logLevel := "info"
	logFormat := "pretty"
	if cfg.Logging.Level != "" {
		logLevel = cfg.Logging.Level
	}
	if cfg.Logging.Format != "" {
		logFormat = cfg.Logging.Format
	}
	if opts.Verbose {
		logLevel = "debug"
	}

	logger := utils.NewLogger(utils.LoggerOptions{
		Level:   logLevel,
		Format:  logFormat,
		Verbose: opts.Verbose,
	})

	// Determine cache directory
	cacheDir := cfg.Cache.Directory
	if cacheDir == "" {
		cacheDir = "~/.repodocs/cache"
	}
	cacheDir = utils.ExpandPath(cacheDir)

	// Create dependencies
	deps, err := strategies.NewDependencies(strategies.DependencyOptions{
		CommonOptions: domain.CommonOptions{
			Verbose:  opts.Verbose,
			DryRun:   opts.DryRun,
			Force:    opts.Force || cfg.Output.Overwrite,
			RenderJS: opts.RenderJS,
			Limit:    opts.Limit,
			Sync:     opts.Sync,
			FullSync: opts.FullSync,
			Prune:    opts.Prune,
		},
		Timeout:         cfg.Concurrency.Timeout,
		EnableCache:     cfg.Cache.Enabled,
		CacheTTL:        cfg.Cache.TTL,
		CacheDir:        cacheDir,
		UserAgent:       cfg.Stealth.UserAgent,
		EnableRenderer:  cfg.Rendering.ForceJS || opts.RenderJS,
		RendererTimeout: cfg.Rendering.JSTimeout,
		Concurrency:     cfg.Concurrency.Workers,
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
		OutputDir:       cfg.Output.Directory,
		Flat:            cfg.Output.Flat,
		JSONMetadata:    cfg.Output.JSONMetadata,
		LLMConfig:       &cfg.LLM,
	})
	if err != nil {
		return nil, fmt.Errorf("failed to create dependencies: %w", err)
	}

	// Set default strategy factory if none provided
	strategyFactory := opts.StrategyFactory
	if strategyFactory == nil {
		strategyFactory = func(st StrategyType, d *strategies.Dependencies) strategies.Strategy {
			return CreateStrategy(st, d)
		}
	}

	return &Orchestrator{
		config:          cfg,
		deps:            deps,
		logger:          logger,
		strategyFactory: strategyFactory,
	}, nil
}

// Run executes the documentation extraction for the given URL
func (o *Orchestrator) Run(ctx context.Context, url string, opts OrchestratorOptions) error {
	startTime := time.Now()

	o.logger.Info().
		Str("url", url).
		Str("output", o.config.Output.Directory).
		Int("concurrency", o.config.Concurrency.Workers).
		Msg("Starting documentation extraction")

	var strategyType StrategyType
	if opts.StrategyOverride != "" {
		strategyType = StrategyType(opts.StrategyOverride)
		o.logger.Debug().
			Str("strategy", string(strategyType)).
			Msg("Using strategy override from manifest")

		if !IsValidStrategy(strategyType) {
			return fmt.Errorf("unknown strategy override: %s", opts.StrategyOverride)
		}
	} else {
		strategyType = DetectStrategy(url)
		o.logger.Debug().
			Str("strategy", string(strategyType)).
			Msg("Detected strategy type")

		if strategyType == StrategyUnknown {
			return fmt.Errorf("unable to determine strategy for URL: %s", url)
		}
	}

	// Create strategy using strategy factory (allows injection for testing)
	strategy := o.strategyFactory(strategyType, o.deps)
	if strategy == nil {
		return fmt.Errorf("failed to create strategy for URL: %s", url)
	}

	o.logger.Info().
		Str("strategy", strategy.Name()).
		Msg("Using extraction strategy")

	o.deps.SetSourceURL(url)
	o.deps.SetStrategy(strategy.Name())

	strategyOpts := strategies.Options{
		CommonOptions: domain.CommonOptions{
			Verbose:  opts.Verbose,
			DryRun:   opts.DryRun,
			Force:    opts.Force || o.config.Output.Overwrite,
			RenderJS: opts.RenderJS || o.config.Rendering.ForceJS,
			Limit:    opts.Limit,
		},
		Output:          o.config.Output.Directory,
		Concurrency:     o.config.Concurrency.Workers,
		MaxDepth:        o.config.Concurrency.MaxDepth,
		Exclude:         append(o.config.Exclude, opts.ExcludePatterns...),
		NoFolders:       o.config.Output.Flat,
		Split:           opts.Split,
		IncludeAssets:   opts.IncludeAssets,
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
		FilterURL:       opts.FilterURL,
	}

	if err := strategy.Execute(ctx, url, strategyOpts); err != nil {
		if ctx.Err() != nil {
			o.logger.Warn().Msg("Extraction cancelled")
			return ctx.Err()
		}
		return fmt.Errorf("strategy execution failed: %w", err)
	}

	if err := o.deps.FlushMetadata(); err != nil {
		o.logger.Warn().Err(err).Msg("Failed to flush metadata")
	}

	if opts.Prune {
		pruned, err := o.deps.PruneDeletedFiles(ctx)
		if err != nil {
			o.logger.Warn().Err(err).Msg("Failed to prune deleted files")
		} else if pruned > 0 {
			o.logger.Info().Int("pruned", pruned).Msg("Removed deleted pages")
		}
	}

	if err := o.deps.SaveState(ctx); err != nil {
		o.logger.Warn().Err(err).Msg("Failed to save state")
	}

	duration := time.Since(startTime)
	o.logger.Info().
		Dur("duration", duration).
		Msg("Documentation extraction completed")

	return nil
}

// Close releases all resources held by the orchestrator
func (o *Orchestrator) Close() error {
	if o.deps != nil {
		return o.deps.Close()
	}
	return nil
}

// GetStrategyName returns the detected strategy name for a URL
func (o *Orchestrator) GetStrategyName(url string) string {
	return string(DetectStrategy(url))
}

// ValidateURL checks if the URL can be processed
func (o *Orchestrator) ValidateURL(url string) error {
	strategyType := DetectStrategy(url)
	if strategyType == StrategyUnknown {
		return fmt.Errorf("unsupported URL format: %s", url)
	}
	return nil
}

// ManifestResult represents the result of processing one manifest source
type ManifestResult struct {
	Source   manifest.Source
	Error    error
	Duration time.Duration
}

// RunManifest executes all sources defined in the manifest
func (o *Orchestrator) RunManifest(
	ctx context.Context,
	manifestCfg *manifest.Config,
	baseOpts OrchestratorOptions,
) error {
	startTime := time.Now()
	totalSources := len(manifestCfg.Sources)

	o.logger.Info().
		Int("sources", totalSources).
		Bool("continue_on_error", manifestCfg.Options.ContinueOnError).
		Str("output", manifestCfg.Options.Output).
		Msg("Starting manifest execution")

	var results []ManifestResult
	var firstError error

	for i, source := range manifestCfg.Sources {
		if ctx.Err() != nil {
			o.logger.Warn().Msg("Manifest execution cancelled")
			return ctx.Err()
		}

		sourceStart := time.Now()

		o.logger.Info().
			Int("source", i+1).
			Int("total", totalSources).
			Str("url", source.URL).
			Str("strategy", source.Strategy).
			Msg("Processing source")

		opts := o.buildSourceOptions(source, baseOpts)

		err := o.Run(ctx, source.URL, opts)
		sourceDuration := time.Since(sourceStart)

		result := ManifestResult{
			Source:   source,
			Error:    err,
			Duration: sourceDuration,
		}
		results = append(results, result)

		if err != nil {
			o.logger.Error().
				Err(err).
				Str("url", source.URL).
				Dur("duration", sourceDuration).
				Msg("Source extraction failed")

			if firstError == nil {
				firstError = err
			}

			if !manifestCfg.Options.ContinueOnError {
				o.logger.Warn().Msg("Stopping execution (continue_on_error=false)")
				return fmt.Errorf("source %s failed: %w", source.URL, err)
			}
		} else {
			o.logger.Info().
				Str("url", source.URL).
				Dur("duration", sourceDuration).
				Msg("Source extraction completed")
		}
	}

	duration := time.Since(startTime)
	successCount := 0
	for _, r := range results {
		if r.Error == nil {
			successCount++
		}
	}

	o.logger.Info().
		Dur("total_duration", duration).
		Int("total", totalSources).
		Int("success", successCount).
		Int("failed", totalSources-successCount).
		Msg("Manifest execution completed")

	if firstError != nil {
		return fmt.Errorf("manifest completed with %d/%d failures: %w",
			totalSources-successCount, totalSources, firstError)
	}

	return nil
}

func (o *Orchestrator) buildSourceOptions(source manifest.Source, baseOpts OrchestratorOptions) OrchestratorOptions {
	opts := baseOpts

	if source.Strategy != "" {
		opts.StrategyOverride = source.Strategy
	}

	if source.ContentSelector != "" {
		opts.ContentSelector = source.ContentSelector
	}
	if source.ExcludeSelector != "" {
		opts.ExcludeSelector = source.ExcludeSelector
	}

	if len(source.Exclude) > 0 {
		opts.ExcludePatterns = append(opts.ExcludePatterns, source.Exclude...)
	}

	if source.RenderJS != nil {
		opts.RenderJS = *source.RenderJS
	}

	if source.Limit > 0 {
		opts.Limit = source.Limit
	}

	if source.MaxDepth > 0 {
		o.logger.Debug().
			Int("max_depth", source.MaxDepth).
			Str("url", source.URL).
			Msg("Source max_depth specified but config override not implemented")
	}

	return opts
}
</file>
<file path="internal/cache/badger.go">
package cache

import (
	"context"
	"os"
	"time"

	"github.com/dgraph-io/badger/v4"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// BadgerCache is a cache implementation using BadgerDB
type BadgerCache struct {
	db *badger.DB
}

// NewBadgerCache creates a new BadgerDB cache
func NewBadgerCache(opts Options) (*BadgerCache, error) {
	var badgerOpts badger.Options

	if opts.InMemory {
		badgerOpts = badger.DefaultOptions("").WithInMemory(true)
	} else {
		if opts.Directory == "" {
			homeDir, err := os.UserHomeDir()
			if err != nil {
				return nil, err
			}
			opts.Directory = homeDir + "/.repodocs/cache"
		}

		// Ensure directory exists
		if err := os.MkdirAll(opts.Directory, 0755); err != nil {
			return nil, err
		}

		badgerOpts = badger.DefaultOptions(opts.Directory)
	}

	// Disable logging unless explicitly enabled
	if !opts.Logger {
		badgerOpts = badgerOpts.WithLogger(nil)
	}

	db, err := badger.Open(badgerOpts)
	if err != nil {
		return nil, err
	}

	// Start background garbage collection
	go func() {
		ticker := time.NewTicker(5 * time.Minute)
		defer ticker.Stop()
		for range ticker.C {
			_ = db.RunValueLogGC(0.5)
		}
	}()

	return &BadgerCache{db: db}, nil
}

// Get retrieves a value from cache
func (c *BadgerCache) Get(ctx context.Context, key string) ([]byte, error) {
	// Generate cache key from URL
	cacheKey := GenerateKey(key)

	var value []byte
	err := c.db.View(func(txn *badger.Txn) error {
		item, err := txn.Get([]byte(cacheKey))
		if err != nil {
			if err == badger.ErrKeyNotFound {
				return domain.ErrCacheMiss
			}
			return err
		}

		value, err = item.ValueCopy(nil)
		return err
	})

	if err != nil {
		return nil, err
	}

	return value, nil
}

// Set stores a value in cache with TTL
func (c *BadgerCache) Set(ctx context.Context, key string, value []byte, ttl time.Duration) error {
	// Generate cache key from URL
	cacheKey := GenerateKey(key)

	return c.db.Update(func(txn *badger.Txn) error {
		e := badger.NewEntry([]byte(cacheKey), value)
		if ttl > 0 {
			e = e.WithTTL(ttl)
		}
		return txn.SetEntry(e)
	})
}

// Has checks if a key exists in cache
func (c *BadgerCache) Has(ctx context.Context, key string) bool {
	cacheKey := GenerateKey(key)

	err := c.db.View(func(txn *badger.Txn) error {
		_, err := txn.Get([]byte(cacheKey))
		return err
	})

	return err == nil
}

// Delete removes a key from cache
func (c *BadgerCache) Delete(ctx context.Context, key string) error {
	cacheKey := GenerateKey(key)

	return c.db.Update(func(txn *badger.Txn) error {
		return txn.Delete([]byte(cacheKey))
	})
}

// Close releases cache resources
func (c *BadgerCache) Close() error {
	return c.db.Close()
}

// Clear removes all entries from the cache
func (c *BadgerCache) Clear() error {
	return c.db.DropAll()
}

// Size returns the number of entries in the cache
func (c *BadgerCache) Size() int64 {
	var count int64
	_ = c.db.View(func(txn *badger.Txn) error {
		opts := badger.DefaultIteratorOptions
		opts.PrefetchValues = false
		it := txn.NewIterator(opts)
		defer it.Close()

		for it.Rewind(); it.Valid(); it.Next() {
			count++
		}
		return nil
	})
	return count
}

// Stats returns cache statistics
func (c *BadgerCache) Stats() map[string]interface{} {
	lsm, vlog := c.db.Size()
	return map[string]interface{}{
		"entries":   c.Size(),
		"lsm_size":  lsm,
		"vlog_size": vlog,
	}
}
</file>
<file path="internal/cache/interface.go">
package cache

import (
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Ensure BadgerCache implements domain.Cache
var _ domain.Cache = (*BadgerCache)(nil)

// Entry represents a cached entry with metadata
type Entry struct {
	URL         string    `json:"url"`
	Content     []byte    `json:"content"`
	ContentType string    `json:"content_type"`
	FetchedAt   time.Time `json:"fetched_at"`
	ExpiresAt   time.Time `json:"expires_at"`
}

// IsExpired returns true if the entry has expired
func (e *Entry) IsExpired() bool {
	return time.Now().After(e.ExpiresAt)
}

// TTL returns the remaining time-to-live
func (e *Entry) TTL() time.Duration {
	remaining := time.Until(e.ExpiresAt)
	if remaining < 0 {
		return 0
	}
	return remaining
}

// Options contains cache configuration options
type Options struct {
	Directory string
	InMemory  bool
	Logger    bool
}

// DefaultOptions returns default cache options
func DefaultOptions() Options {
	return Options{
		Directory: "",
		InMemory:  false,
		Logger:    false,
	}
}
</file>
<file path="internal/cache/keys.go">
package cache

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"path"
	"strings"
)

// GenerateKey generates a cache key from a URL
// The key is a SHA256 hash of the normalized URL
func GenerateKey(rawURL string) string {
	normalized := normalizeForKey(rawURL)
	hash := sha256.Sum256([]byte(normalized))
	return hex.EncodeToString(hash[:])
}

// GenerateKeyWithPrefix generates a cache key with a prefix
func GenerateKeyWithPrefix(prefix, rawURL string) string {
	key := GenerateKey(rawURL)
	return prefix + ":" + key
}

// normalizeForKey normalizes a URL for consistent key generation
func normalizeForKey(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return rawURL
	}

	// Normalize scheme
	if u.Scheme == "" {
		u.Scheme = "https"
	}

	// Normalize host
	u.Host = strings.ToLower(u.Host)

	// Remove default ports
	if (u.Scheme == "http" && u.Port() == "80") ||
		(u.Scheme == "https" && u.Port() == "443") {
		u.Host = u.Hostname()
	}

	// Clean path
	if u.Path == "" {
		u.Path = "/"
	} else {
		u.Path = path.Clean(u.Path)
	}

	// Remove trailing slash except for root
	if u.Path != "/" && strings.HasSuffix(u.Path, "/") {
		u.Path = strings.TrimSuffix(u.Path, "/")
	}

	// Remove fragment
	u.Fragment = ""

	return u.String()
}

// KeyPrefix constants for different cache types
const (
	PrefixPage     = "page"
	PrefixSitemap  = "sitemap"
	PrefixGit      = "git"
	PrefixMetadata = "meta"
)

// PageKey generates a cache key for a page
func PageKey(url string) string {
	return GenerateKeyWithPrefix(PrefixPage, url)
}

// SitemapKey generates a cache key for a sitemap
func SitemapKey(url string) string {
	return GenerateKeyWithPrefix(PrefixSitemap, url)
}

// MetadataKey generates a cache key for metadata
func MetadataKey(url string) string {
	return GenerateKeyWithPrefix(PrefixMetadata, url)
}
</file>
<file path="internal/config/config.go">
package config

import "time"

// Config represents the application configuration
type Config struct {
	Output      OutputConfig      `mapstructure:"output" yaml:"output"`
	Concurrency ConcurrencyConfig `mapstructure:"concurrency" yaml:"concurrency"`
	Cache       CacheConfig       `mapstructure:"cache" yaml:"cache"`
	Rendering   RenderingConfig   `mapstructure:"rendering" yaml:"rendering"`
	Stealth     StealthConfig     `mapstructure:"stealth" yaml:"stealth"`
	Exclude     []string          `mapstructure:"exclude" yaml:"exclude"`
	Logging     LoggingConfig     `mapstructure:"logging" yaml:"logging"`
	LLM         LLMConfig         `mapstructure:"llm" yaml:"llm"`
}

// LLMConfig contains LLM provider settings
type LLMConfig struct {
	Provider        string          `mapstructure:"provider" yaml:"provider"`
	APIKey          string          `mapstructure:"api_key" yaml:"api_key"`
	BaseURL         string          `mapstructure:"base_url" yaml:"base_url"`
	Model           string          `mapstructure:"model" yaml:"model"`
	MaxTokens       int             `mapstructure:"max_tokens" yaml:"max_tokens"`
	Temperature     float64         `mapstructure:"temperature" yaml:"temperature"`
	Timeout         time.Duration   `mapstructure:"timeout" yaml:"timeout"`
	MaxRetries      int             `mapstructure:"max_retries" yaml:"max_retries"` // Deprecated: use RateLimit.MaxRetries
	EnhanceMetadata bool            `mapstructure:"enhance_metadata" yaml:"enhance_metadata"`
	RateLimit       RateLimitConfig `mapstructure:"rate_limit" yaml:"rate_limit"`
}

// RateLimitConfig contains rate limiting settings for LLM requests
type RateLimitConfig struct {
	Enabled           bool                 `mapstructure:"enabled" yaml:"enabled"`
	RequestsPerMinute int                  `mapstructure:"requests_per_minute" yaml:"requests_per_minute"`
	BurstSize         int                  `mapstructure:"burst_size" yaml:"burst_size"`
	MaxRetries        int                  `mapstructure:"max_retries" yaml:"max_retries"`
	InitialDelay      time.Duration        `mapstructure:"initial_delay" yaml:"initial_delay"`
	MaxDelay          time.Duration        `mapstructure:"max_delay" yaml:"max_delay"`
	Multiplier        float64              `mapstructure:"multiplier" yaml:"multiplier"`
	CircuitBreaker    CircuitBreakerConfig `mapstructure:"circuit_breaker" yaml:"circuit_breaker"`
}

// CircuitBreakerConfig contains circuit breaker settings
type CircuitBreakerConfig struct {
	Enabled                  bool          `mapstructure:"enabled" yaml:"enabled"`
	FailureThreshold         int           `mapstructure:"failure_threshold" yaml:"failure_threshold"`
	SuccessThresholdHalfOpen int           `mapstructure:"success_threshold_half_open" yaml:"success_threshold_half_open"`
	ResetTimeout             time.Duration `mapstructure:"reset_timeout" yaml:"reset_timeout"`
}

// OutputConfig contains output-related settings
type OutputConfig struct {
	Directory    string `mapstructure:"directory" yaml:"directory"`
	Flat         bool   `mapstructure:"flat" yaml:"flat"`
	JSONMetadata bool   `mapstructure:"json_metadata" yaml:"json_metadata"`
	Overwrite    bool   `mapstructure:"overwrite" yaml:"overwrite"`
}

// ConcurrencyConfig contains concurrency settings
type ConcurrencyConfig struct {
	Workers  int           `mapstructure:"workers" yaml:"workers"`
	Timeout  time.Duration `mapstructure:"timeout" yaml:"timeout"`
	MaxDepth int           `mapstructure:"max_depth" yaml:"max_depth"`
}

// CacheConfig contains cache settings
type CacheConfig struct {
	Enabled   bool          `mapstructure:"enabled" yaml:"enabled"`
	TTL       time.Duration `mapstructure:"ttl" yaml:"ttl"`
	Directory string        `mapstructure:"directory" yaml:"directory"`
}

// RenderingConfig contains JavaScript rendering settings
type RenderingConfig struct {
	ForceJS     bool          `mapstructure:"force_js" yaml:"force_js"`
	JSTimeout   time.Duration `mapstructure:"js_timeout" yaml:"js_timeout"`
	ScrollToEnd bool          `mapstructure:"scroll_to_end" yaml:"scroll_to_end"`
}

// StealthConfig contains stealth mode settings
type StealthConfig struct {
	UserAgent      string        `mapstructure:"user_agent" yaml:"user_agent"`
	RandomDelayMin time.Duration `mapstructure:"random_delay_min" yaml:"random_delay_min"`
	RandomDelayMax time.Duration `mapstructure:"random_delay_max" yaml:"random_delay_max"`
}

// LoggingConfig contains logging settings
type LoggingConfig struct {
	Level  string `mapstructure:"level" yaml:"level"`
	Format string `mapstructure:"format" yaml:"format"`
}

// Validate validates the configuration
func (c *Config) Validate() error {
	if c.Concurrency.Workers < 1 {
		c.Concurrency.Workers = DefaultWorkers
	}
	if c.Concurrency.MaxDepth < 1 {
		c.Concurrency.MaxDepth = DefaultMaxDepth
	}
	if c.Concurrency.Timeout < time.Second {
		c.Concurrency.Timeout = DefaultTimeout
	}
	if c.Cache.TTL < time.Minute {
		c.Cache.TTL = DefaultCacheTTL
	}
	if c.Rendering.JSTimeout < time.Second {
		c.Rendering.JSTimeout = DefaultJSTimeout
	}
	return nil
}
</file>
<file path="internal/config/defaults.go">
package config

import (
	"os"
	"path/filepath"
	"time"
)

// Default values
const (
	// Output defaults
	DefaultOutputDir = "./docs"

	// Concurrency defaults
	DefaultWorkers  = 5
	DefaultTimeout  = 30 * time.Second
	DefaultMaxDepth = 3

	// Cache defaults
	DefaultCacheEnabled = true
	DefaultCacheTTL     = 24 * time.Hour

	// Rendering defaults
	DefaultJSTimeout   = 60 * time.Second
	DefaultScrollToEnd = true

	// Stealth defaults
	DefaultRandomDelayMin = 1 * time.Second
	DefaultRandomDelayMax = 3 * time.Second

	// Logging defaults
	DefaultLogLevel  = "info"
	DefaultLogFormat = "pretty"

	// LLM defaults (only for optional parameters)
	DefaultLLMMaxTokens   = 4096
	DefaultLLMTemperature = 0.7
	DefaultLLMTimeout     = 60 * time.Second
	DefaultLLMMaxRetries  = 3

	// Rate limiting defaults
	DefaultRateLimitEnabled           = true
	DefaultRateLimitRequestsPerMinute = 60
	DefaultRateLimitBurstSize         = 10
	DefaultRateLimitMaxRetries        = 3
	DefaultRateLimitInitialDelay      = 1 * time.Second
	DefaultRateLimitMaxDelay          = 60 * time.Second
	DefaultRateLimitMultiplier        = 2.0

	// Circuit breaker defaults
	DefaultCircuitBreakerEnabled                  = true
	DefaultCircuitBreakerFailureThreshold         = 5
	DefaultCircuitBreakerSuccessThresholdHalfOpen = 1
	DefaultCircuitBreakerResetTimeout             = 30 * time.Second
)

// Default exclude patterns
var DefaultExcludePatterns = []string{
	`.*\.pdf$`,
	`.*/login.*`,
	`.*/logout.*`,
	`.*/admin.*`,
	`.*/sign-in.*`,
	`.*/sign-up.*`,
}

// ConfigDir returns the config directory path
func ConfigDir() string {
	home, err := os.UserHomeDir()
	if err != nil {
		return ".repodocs"
	}
	return filepath.Join(home, ".repodocs")
}

// CacheDir returns the cache directory path
func CacheDir() string {
	return filepath.Join(ConfigDir(), "cache")
}

// ConfigFilePath returns the config file path
func ConfigFilePath() string {
	return filepath.Join(ConfigDir(), "config.yaml")
}

// Default returns the default configuration
func Default() *Config {
	return &Config{
		Output: OutputConfig{
			Directory:    DefaultOutputDir,
			Flat:         false,
			JSONMetadata: false,
			Overwrite:    false,
		},
		Concurrency: ConcurrencyConfig{
			Workers:  DefaultWorkers,
			Timeout:  DefaultTimeout,
			MaxDepth: DefaultMaxDepth,
		},
		Cache: CacheConfig{
			Enabled:   DefaultCacheEnabled,
			TTL:       DefaultCacheTTL,
			Directory: CacheDir(),
		},
		Rendering: RenderingConfig{
			ForceJS:     false,
			JSTimeout:   DefaultJSTimeout,
			ScrollToEnd: DefaultScrollToEnd,
		},
		Stealth: StealthConfig{
			UserAgent:      "",
			RandomDelayMin: DefaultRandomDelayMin,
			RandomDelayMax: DefaultRandomDelayMax,
		},
		Exclude: DefaultExcludePatterns,
		Logging: LoggingConfig{
			Level:  DefaultLogLevel,
			Format: DefaultLogFormat,
		},
		LLM: LLMConfig{
			MaxTokens:   DefaultLLMMaxTokens,
			Temperature: DefaultLLMTemperature,
			Timeout:     DefaultLLMTimeout,
			MaxRetries:  DefaultLLMMaxRetries,
			RateLimit: RateLimitConfig{
				Enabled:           DefaultRateLimitEnabled,
				RequestsPerMinute: DefaultRateLimitRequestsPerMinute,
				BurstSize:         DefaultRateLimitBurstSize,
				MaxRetries:        DefaultRateLimitMaxRetries,
				InitialDelay:      DefaultRateLimitInitialDelay,
				MaxDelay:          DefaultRateLimitMaxDelay,
				Multiplier:        DefaultRateLimitMultiplier,
				CircuitBreaker: CircuitBreakerConfig{
					Enabled:                  DefaultCircuitBreakerEnabled,
					FailureThreshold:         DefaultCircuitBreakerFailureThreshold,
					SuccessThresholdHalfOpen: DefaultCircuitBreakerSuccessThresholdHalfOpen,
					ResetTimeout:             DefaultCircuitBreakerResetTimeout,
				},
			},
		},
	}
}
</file>
<file path="internal/config/loader.go">
package config

import (
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"github.com/spf13/viper"
	"gopkg.in/yaml.v3"
)

// Load loads configuration from file, environment, and defaults
// Uses the global viper instance to access CLI flag bindings
func Load() (*Config, error) {
	// Use global viper instance to get CLI flag bindings
	v := viper.GetViper()

	// Set defaults
	setDefaults(v)

	// Config file settings
	// Search order: current directory first (project-specific), then user config
	v.SetConfigName("config")
	v.SetConfigType("yaml")
	v.AddConfigPath(".")
	v.AddConfigPath(ConfigDir())

	// Read config file (ignore if not found)
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); !ok {
			return nil, err
		}
	}

	// Environment variables (REPODOCS_*)
	v.SetEnvPrefix("REPODOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	// Unmarshal config
	var cfg Config
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, err
	}

	// Validate and apply defaults for invalid values
	if err := cfg.Validate(); err != nil {
		return nil, err
	}

	return &cfg, nil
}

// LoadWithViper loads configuration and returns the viper instance
// This is useful for merging CLI flags later
func LoadWithViper() (*Config, *viper.Viper, error) {
	v := viper.New()

	// Set defaults
	setDefaults(v)

	// Config file settings
	// Search order: current directory first (project-specific), then user config
	v.SetConfigName("config")
	v.SetConfigType("yaml")
	v.AddConfigPath(".")
	v.AddConfigPath(ConfigDir())

	// Read config file (ignore if not found)
	if err := v.ReadInConfig(); err != nil {
		if _, ok := err.(viper.ConfigFileNotFoundError); !ok {
			return nil, nil, err
		}
	}

	// Environment variables (REPODOCS_*)
	v.SetEnvPrefix("REPODOCS")
	v.SetEnvKeyReplacer(strings.NewReplacer(".", "_"))
	v.AutomaticEnv()

	// Unmarshal config
	var cfg Config
	if err := v.Unmarshal(&cfg); err != nil {
		return nil, nil, err
	}

	// Validate
	if err := cfg.Validate(); err != nil {
		return nil, nil, err
	}

	return &cfg, v, nil
}

// setDefaults sets default values in viper
func setDefaults(v *viper.Viper) {
	// Output defaults
	v.SetDefault("output.directory", DefaultOutputDir)
	v.SetDefault("output.flat", false)
	v.SetDefault("output.json_metadata", false)
	v.SetDefault("output.overwrite", false)

	// Concurrency defaults
	v.SetDefault("concurrency.workers", DefaultWorkers)
	v.SetDefault("concurrency.timeout", DefaultTimeout)
	v.SetDefault("concurrency.max_depth", DefaultMaxDepth)

	// Cache defaults
	v.SetDefault("cache.enabled", DefaultCacheEnabled)
	v.SetDefault("cache.ttl", DefaultCacheTTL)
	v.SetDefault("cache.directory", CacheDir())

	// Rendering defaults
	v.SetDefault("rendering.force_js", false)
	v.SetDefault("rendering.js_timeout", DefaultJSTimeout)
	v.SetDefault("rendering.scroll_to_end", DefaultScrollToEnd)

	// Stealth defaults
	v.SetDefault("stealth.user_agent", "")
	v.SetDefault("stealth.random_delay_min", DefaultRandomDelayMin)
	v.SetDefault("stealth.random_delay_max", DefaultRandomDelayMax)

	// Exclude defaults
	v.SetDefault("exclude", DefaultExcludePatterns)

	// Logging defaults
	v.SetDefault("logging.level", DefaultLogLevel)
	v.SetDefault("logging.format", DefaultLogFormat)

	// LLM defaults (all keys must be registered for env var binding)
	v.SetDefault("llm.provider", "")
	v.SetDefault("llm.api_key", "")
	v.SetDefault("llm.base_url", "")
	v.SetDefault("llm.model", "")
	v.SetDefault("llm.max_tokens", DefaultLLMMaxTokens)
	v.SetDefault("llm.temperature", DefaultLLMTemperature)
	v.SetDefault("llm.timeout", DefaultLLMTimeout)
	v.SetDefault("llm.max_retries", DefaultLLMMaxRetries)
	v.SetDefault("llm.enhance_metadata", false)
}

// EnsureConfigDir creates the config directory if it doesn't exist
func EnsureConfigDir() error {
	dir := ConfigDir()
	return os.MkdirAll(dir, 0755)
}

// EnsureCacheDir creates the cache directory if it doesn't exist
func EnsureCacheDir() error {
	dir := CacheDir()
	return os.MkdirAll(dir, 0755)
}

// Save writes the configuration to the config file at ~/.repodocs/config.yaml
func Save(cfg *Config) error {
	if err := EnsureConfigDir(); err != nil {
		return fmt.Errorf("failed to create config directory: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	path := ConfigFilePath()
	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write config file: %w", err)
	}

	return nil
}

// SaveTo writes the configuration to a specific path
func SaveTo(cfg *Config, path string) error {
	dir := filepath.Dir(path)
	if err := os.MkdirAll(dir, 0755); err != nil {
		return fmt.Errorf("failed to create directory: %w", err)
	}

	data, err := yaml.Marshal(cfg)
	if err != nil {
		return fmt.Errorf("failed to marshal config: %w", err)
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return fmt.Errorf("failed to write config file: %w", err)
	}

	return nil
}
</file>
<file path="internal/converter/content_type.go">
package converter

import "strings"

func IsMarkdownContent(contentType, url string) bool {
	ct := strings.ToLower(contentType)
	if strings.Contains(ct, "text/markdown") ||
		strings.Contains(ct, "text/x-markdown") ||
		strings.Contains(ct, "application/markdown") {
		return true
	}

	lowerURL := strings.ToLower(url)

	if idx := strings.Index(lowerURL, "?"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	if idx := strings.Index(lowerURL, "#"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	if strings.HasSuffix(lowerURL, ".md") ||
		strings.HasSuffix(lowerURL, ".mdx") ||
		strings.HasSuffix(lowerURL, ".markdown") ||
		strings.HasSuffix(lowerURL, ".mdown") {
		return true
	}

	return false
}

// IsPlainTextContent checks if the content is plain text.
// Returns true for text/plain content type or .txt URL extension.
// Query strings and fragments are stripped before checking the extension.
func IsPlainTextContent(contentType, url string) bool {
	ct := strings.ToLower(contentType)
	if strings.Contains(ct, "text/plain") {
		return true
	}

	lowerURL := strings.ToLower(url)

	if idx := strings.Index(lowerURL, "?"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}
	if idx := strings.Index(lowerURL, "#"); idx != -1 {
		lowerURL = lowerURL[:idx]
	}

	return strings.HasSuffix(lowerURL, ".txt")
}

// IsHTMLContent checks if the content type indicates HTML content.
// Returns true for empty content type (assumes HTML for backward compatibility).
func IsHTMLContent(contentType string) bool {
	if contentType == "" {
		return true
	}
	ct := strings.ToLower(contentType)
	return strings.Contains(ct, "text/html") ||
		strings.Contains(ct, "application/xhtml")
}
</file>
<file path="internal/converter/encoding.go">
package converter

import (
	"bytes"
	"io"
	"strings"

	"golang.org/x/net/html/charset"
	"golang.org/x/text/encoding"
	"golang.org/x/text/encoding/htmlindex"
	"golang.org/x/text/transform"
)

// DetectEncoding detects the character encoding of HTML content
func DetectEncoding(content []byte) string {
	// Try to detect from content-type meta tag or charset attribute
	contentStr := string(content[:min(1024, len(content))])

	// Look for charset in meta tag
	if enc := extractCharsetFromMeta(contentStr); enc != "" {
		return enc
	}

	// Use golang.org/x/net/html/charset for detection
	_, name, _ := charset.DetermineEncoding(content, "")
	if name != "" && name != "windows-1252" {
		return name
	}

	// Default to UTF-8
	return "utf-8"
}

// extractCharsetFromMeta extracts charset from meta tag
func extractCharsetFromMeta(html string) string {
	html = strings.ToLower(html)

	// Look for <meta charset="...">
	if idx := strings.Index(html, "charset="); idx != -1 {
		start := idx + 8
		end := start

		// Skip quote if present
		if start < len(html) && (html[start] == '"' || html[start] == '\'') {
			start++
		}

		// Find end of charset value
		for end = start; end < len(html); end++ {
			c := html[end]
			if c == '"' || c == '\'' || c == ';' || c == '>' || c == ' ' {
				break
			}
		}

		if end > start {
			return strings.TrimSpace(html[start:end])
		}
	}

	return ""
}

// ConvertToUTF8 converts content from detected encoding to UTF-8
func ConvertToUTF8(content []byte) ([]byte, error) {
	enc := DetectEncoding(content)

	// Already UTF-8
	if enc == "utf-8" || enc == "utf8" {
		return content, nil
	}

	// Get encoder for the detected charset
	e, err := htmlindex.Get(enc)
	if err != nil {
		// Unknown encoding, return as-is
		return content, nil
	}

	// Decode to UTF-8
	reader := transform.NewReader(bytes.NewReader(content), e.NewDecoder())
	return io.ReadAll(reader)
}

// IsUTF8 checks if content is valid UTF-8
func IsUTF8(content []byte) bool {
	enc := DetectEncoding(content)
	return enc == "utf-8" || enc == "utf8"
}

// GetEncoder returns the encoding for a charset name
func GetEncoder(charsetName string) (encoding.Encoding, error) {
	return htmlindex.Get(charsetName)
}

// min returns the minimum of two integers
func min(a, b int) int {
	if a < b {
		return a
	}
	return b
}
</file>
<file path="internal/converter/markdown.go">
package converter

import (
	"fmt"
	"regexp"
	"strings"

	md "github.com/JohannesKaufmann/html-to-markdown/v2"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"gopkg.in/yaml.v3"
)

// Pre-compiled regex patterns for markdown stripping
var (
	linkRegex                = regexp.MustCompile(`\[([^\]]+)\]\([^)]+\)`)
	imageRegex               = regexp.MustCompile(`!\[([^\]]*)\]\([^)]+\)`)
	boldAsterisksRegex       = regexp.MustCompile(`\*\*([^*]+)\*\*`)
	italicAsterisksRegex     = regexp.MustCompile(`\*([^*]+)\*`)
	boldUnderscoresRegex     = regexp.MustCompile(`__([^_]+)__`)
	italicUnderscoresRegex   = regexp.MustCompile(`_([^_]+)_`)
	headersRegex             = regexp.MustCompile(`(?m)^#{1,6}\s+`)
	stripHorizontalRuleRegex = regexp.MustCompile(`(?m)^[\-*_]{3,}$`)
	blockquoteRegex          = regexp.MustCompile(`(?m)^>\s+`)
	unorderedListRegex       = regexp.MustCompile(`(?m)^[\s]*[\-*+]\s+`)
	orderedListRegex         = regexp.MustCompile(`(?m)^[\s]*\d+\.\s+`)
	fencedCodeBlockRegex     = regexp.MustCompile("(?s)```[^`]*```")
	indentedCodeBlockRegex   = regexp.MustCompile("(?m)^(    |\t).*$")
)

// MarkdownConverter converts HTML to Markdown
type MarkdownConverter struct {
	domain string
}

// MarkdownOptions contains options for Markdown conversion
type MarkdownOptions struct {
	Domain          string
	CodeBlockStyle  string // "fenced" or "indented"
	HeadingStyle    string // "atx" or "setext"
	BulletListStyle string // "-", "*", or "+"
}

// DefaultMarkdownOptions returns default Markdown options
func DefaultMarkdownOptions() MarkdownOptions {
	return MarkdownOptions{
		CodeBlockStyle:  "fenced",
		HeadingStyle:    "atx",
		BulletListStyle: "-",
	}
}

// NewMarkdownConverter creates a new Markdown converter
func NewMarkdownConverter(opts MarkdownOptions) *MarkdownConverter {
	return &MarkdownConverter{
		domain: opts.Domain,
	}
}

// Convert converts HTML to Markdown
func (c *MarkdownConverter) Convert(html string) (string, error) {
	// html-to-markdown v2 uses ConvertString directly
	markdown, err := md.ConvertString(html)
	if err != nil {
		return "", fmt.Errorf("failed to convert HTML to Markdown: %w", err)
	}

	// Clean up the markdown
	markdown = c.cleanMarkdown(markdown)

	return markdown, nil
}

// cleanMarkdown cleans up the converted markdown
func (c *MarkdownConverter) cleanMarkdown(markdown string) string {
	// Remove excessive blank lines (more than 2 consecutive)
	for strings.Contains(markdown, "\n\n\n\n") {
		markdown = strings.ReplaceAll(markdown, "\n\n\n\n", "\n\n\n")
	}

	// Trim leading/trailing whitespace
	markdown = strings.TrimSpace(markdown)

	return markdown
}

// GenerateFrontmatter generates YAML frontmatter for a document
func GenerateFrontmatter(doc *domain.Document) (string, error) {
	fm := doc.ToFrontmatter()
	data, err := yaml.Marshal(fm)
	if err != nil {
		return "", err
	}

	return fmt.Sprintf("---\n%s---\n\n", string(data)), nil
}

// AddFrontmatter adds YAML frontmatter to markdown content
func AddFrontmatter(markdown string, doc *domain.Document) (string, error) {
	frontmatter, err := GenerateFrontmatter(doc)
	if err != nil {
		return "", err
	}

	return frontmatter + markdown, nil
}

// CountWords counts words in text
func CountWords(text string) int {
	words := strings.Fields(text)
	return len(words)
}

// CountChars counts characters in text
func CountChars(text string) int {
	return len(text)
}

// StripMarkdown removes markdown formatting to get plain text
func StripMarkdown(markdown string) string {
	// Remove code blocks
	markdown = removeCodeBlocks(markdown)

	// Remove links but keep text: [text](url) -> text
	markdown = linkRegex.ReplaceAllString(markdown, "$1")

	// Remove images: ![alt](url) -> alt
	markdown = imageRegex.ReplaceAllString(markdown, "$1")

	// Remove emphasis: **bold** -> bold, *italic* -> italic
	markdown = boldAsterisksRegex.ReplaceAllString(markdown, "$1")
	markdown = italicAsterisksRegex.ReplaceAllString(markdown, "$1")
	markdown = boldUnderscoresRegex.ReplaceAllString(markdown, "$1")
	markdown = italicUnderscoresRegex.ReplaceAllString(markdown, "$1")

	// Remove headers: # Header -> Header
	markdown = headersRegex.ReplaceAllString(markdown, "")

	// Remove horizontal rules
	markdown = stripHorizontalRuleRegex.ReplaceAllString(markdown, "")

	// Remove blockquotes
	markdown = blockquoteRegex.ReplaceAllString(markdown, "")

	// Remove list markers
	markdown = unorderedListRegex.ReplaceAllString(markdown, "")
	markdown = orderedListRegex.ReplaceAllString(markdown, "")

	return strings.TrimSpace(markdown)
}

// removeCodeBlocks removes fenced code blocks
func removeCodeBlocks(markdown string) string {
	// Remove fenced code blocks
	markdown = fencedCodeBlockRegex.ReplaceAllString(markdown, "")

	// Remove indented code blocks (lines starting with 4 spaces or tab)
	markdown = indentedCodeBlockRegex.ReplaceAllString(markdown, "")

	return markdown
}
</file>
<file path="internal/converter/markdown_reader.go">
package converter

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"gopkg.in/yaml.v3"
)

// MarkdownReader reads and extracts metadata from markdown content
// without using HTML parsing (avoids the 512 node limit issue).
type MarkdownReader struct{}

// NewMarkdownReader creates a new markdown reader.
func NewMarkdownReader() *MarkdownReader {
	return &MarkdownReader{}
}

// Frontmatter represents YAML frontmatter commonly found in markdown files.
type Frontmatter struct {
	Title       string   `yaml:"title"`
	Description string   `yaml:"description"`
	Summary     string   `yaml:"summary"`
	Author      string   `yaml:"author"`
	Date        string   `yaml:"date"`
	Tags        []string `yaml:"tags"`
	Category    string   `yaml:"category"`
}

// Read processes markdown content and returns a Document.
func (r *MarkdownReader) Read(content, sourceURL string) (*domain.Document, error) {
	frontmatter, body := r.parseFrontmatter(content)
	title := r.extractTitle(frontmatter, body)
	description := r.extractDescription(frontmatter, body)
	headers := r.extractHeaders(body)
	links := r.extractLinks(body, sourceURL)

	plainText := StripMarkdown(body)
	wordCount := CountWords(plainText)
	charCount := CountChars(plainText)
	contentHash := r.calculateHash(body)

	return &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        body,
		HTMLContent:    "",
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        headers,
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}, nil
}

func (r *MarkdownReader) parseFrontmatter(content string) (*Frontmatter, string) {
	content = strings.TrimSpace(content)

	if !strings.HasPrefix(content, "---") {
		return nil, content
	}

	rest := content[3:]

	if strings.HasPrefix(rest, "\n") {
		rest = rest[1:]
	} else if strings.HasPrefix(rest, "\r\n") {
		rest = rest[2:]
	}

	lines := strings.Split(rest, "\n")
	yamlLines := []string{}
	closingIdx := -1

	for i, line := range lines {
		trimmed := strings.TrimRight(line, "\r")
		if trimmed == "---" {
			closingIdx = i
			break
		}
		yamlLines = append(yamlLines, line)
	}

	if closingIdx == -1 {
		return nil, content
	}

	yamlContent := strings.Join(yamlLines, "\n")
	var fm Frontmatter
	if err := yaml.Unmarshal([]byte(yamlContent), &fm); err != nil {
		return nil, content
	}

	bodyLines := lines[closingIdx+1:]
	body := strings.TrimSpace(strings.Join(bodyLines, "\n"))

	return &fm, body
}

func (r *MarkdownReader) extractTitle(fm *Frontmatter, body string) string {
	if fm != nil && fm.Title != "" {
		return fm.Title
	}

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		if strings.HasPrefix(trimmed, "# ") {
			title := strings.TrimSpace(strings.TrimPrefix(trimmed, "# "))
			title = strings.TrimRight(title, "#")
			return strings.TrimSpace(title)
		}
	}

	return ""
}

var numberedListRegex = regexp.MustCompile(`^\d+\.\s`)
var horizontalRuleRegex = regexp.MustCompile(`^[-*_]{3,}$`)

func (r *MarkdownReader) extractDescription(fm *Frontmatter, body string) string {
	if fm != nil {
		if fm.Description != "" {
			return fm.Description
		}
		if fm.Summary != "" {
			return fm.Summary
		}
	}

	inCodeBlock := false
	lines := strings.Split(body, "\n")
	var paragraphLines []string

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		if strings.HasPrefix(trimmed, "#") {
			continue
		}

		if strings.HasPrefix(trimmed, "- ") ||
			strings.HasPrefix(trimmed, "* ") ||
			strings.HasPrefix(trimmed, "+ ") ||
			numberedListRegex.MatchString(trimmed) {
			continue
		}

		if trimmed == "" {
			if len(paragraphLines) > 0 {
				break
			}
			continue
		}

		if horizontalRuleRegex.MatchString(trimmed) {
			continue
		}

		paragraphLines = append(paragraphLines, trimmed)
	}

	if len(paragraphLines) > 0 {
		desc := strings.Join(paragraphLines, " ")
		if len(desc) > 300 {
			desc = desc[:297] + "..."
		}
		return desc
	}

	return ""
}

var headingRegex = regexp.MustCompile(`^(#{1,6})\s+(.+)$`)

func (r *MarkdownReader) extractHeaders(body string) map[string][]string {
	headers := make(map[string][]string)

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		matches := headingRegex.FindStringSubmatch(trimmed)
		if len(matches) == 3 {
			level := len(matches[1])
			text := strings.TrimSpace(matches[2])

			text = strings.TrimRight(text, "#")
			text = strings.TrimSpace(text)

			if text != "" {
				key := "h" + string(rune('0'+level))
				headers[key] = append(headers[key], text)
			}
		}
	}

	return headers
}

var markdownLinkRegex = regexp.MustCompile(`\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)`)

func (r *MarkdownReader) extractLinks(body, baseURL string) []string {
	var links []string
	seen := make(map[string]bool)

	base, _ := url.Parse(baseURL)

	inCodeBlock := false
	lines := strings.Split(body, "\n")

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if strings.HasPrefix(trimmed, "```") || strings.HasPrefix(trimmed, "~~~") {
			inCodeBlock = !inCodeBlock
			continue
		}

		if inCodeBlock {
			continue
		}

		matches := markdownLinkRegex.FindAllStringSubmatch(line, -1)
		for _, match := range matches {
			if len(match) >= 3 {
				href := strings.TrimSpace(match[2])

				if href == "" ||
					strings.HasPrefix(href, "#") ||
					strings.HasPrefix(href, "javascript:") ||
					strings.HasPrefix(href, "mailto:") ||
					strings.HasPrefix(href, "tel:") {
					continue
				}

				if base != nil && !strings.HasPrefix(href, "http://") && !strings.HasPrefix(href, "https://") {
					if refURL, err := url.Parse(href); err == nil {
						href = base.ResolveReference(refURL).String()
					}
				}

				if !seen[href] {
					seen[href] = true
					links = append(links, href)
				}
			}
		}
	}

	return links
}

func (r *MarkdownReader) calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}
</file>
<file path="internal/converter/pipeline.go">
package converter

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"errors"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Pipeline orchestrates the HTML to Markdown conversion process
type Pipeline struct {
	sanitizer       *Sanitizer
	extractor       *ExtractContent
	mdConverter     *MarkdownConverter
	excludeSelector string
}

// PipelineOptions contains options for the conversion pipeline
type PipelineOptions struct {
	BaseURL         string
	ContentSelector string
	ExcludeSelector string
}

// NewPipeline creates a new conversion pipeline
func NewPipeline(opts PipelineOptions) *Pipeline {
	sanitizer := NewSanitizer(SanitizerOptions{
		BaseURL:          opts.BaseURL,
		RemoveNavigation: true,
		RemoveComments:   true,
	})

	extractor := NewExtractContent(opts.ContentSelector)

	mdConverter := NewMarkdownConverter(MarkdownOptions{
		Domain:          opts.BaseURL,
		CodeBlockStyle:  "fenced",
		HeadingStyle:    "atx",
		BulletListStyle: "-",
	})

	return &Pipeline{
		sanitizer:       sanitizer,
		extractor:       extractor,
		mdConverter:     mdConverter,
		excludeSelector: opts.ExcludeSelector,
	}
}

// Convert processes HTML content and returns a Document
func (p *Pipeline) Convert(ctx context.Context, html string, sourceURL string) (*domain.Document, error) {
	// Step 1: Convert encoding to UTF-8
	htmlBytes, err := ConvertToUTF8([]byte(html))
	if err != nil {
		return nil, err
	}
	html = string(htmlBytes)

	// Step 2: Parse original HTML once
	origDoc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return nil, err
	}

	// Step 3: Extract main content
	var contentHTML string
	var title string
	var contentSel *goquery.Selection
	usedSelector := false

	if p.extractor.selector != "" {
		contentHTML, title, err = p.extractor.ExtractFromDocument(origDoc, sourceURL)
		if err != nil {
			if errors.Is(err, ErrSelectorNotFound) {
				contentHTML, title, err = p.extractor.extractWithReadability(html, sourceURL)
			} else {
				return nil, err
			}
		} else {
			usedSelector = true
			contentSel = origDoc.Find(p.extractor.selector)
		}
	} else {
		contentHTML, title, err = p.extractor.extractWithReadability(html, sourceURL)
	}
	if err != nil {
		return nil, err
	}

	// Step 3.5: Apply exclusion selector (remove unwanted elements)
	if p.excludeSelector != "" {
		if usedSelector {
			contentSel = p.removeExcludedFromSelection(contentSel)
		} else {
			contentHTML = p.removeExcluded(contentHTML)
		}
	}

	if usedSelector && contentSel == nil {
		return nil, ErrSelectorNotFound
	}

	// Step 4: Sanitize HTML
	var sanitizedHTML string
	var headers map[string][]string
	var links []string

	description := ExtractDescription(origDoc)

	if usedSelector {
		sanitizedSel, selErr := p.sanitizer.SanitizeSelection(contentSel)
		if selErr != nil {
			return nil, selErr
		}

		headers = extractHeadersFromSelection(sanitizedSel)
		links = extractLinksFromSelection(sanitizedSel, sourceURL)

		sanitizedHTML, err = selectionHTML(sanitizedSel)
		if err != nil {
			return nil, err
		}
	} else {
		contentDoc, docErr := goquery.NewDocumentFromReader(strings.NewReader(contentHTML))
		if docErr != nil {
			return nil, docErr
		}

		sanitizedDoc, docErr := p.sanitizer.SanitizeDocument(contentDoc)
		if docErr != nil {
			return nil, docErr
		}

		headers = ExtractHeadersFromDoc(sanitizedDoc)
		links = ExtractLinksFromDoc(sanitizedDoc, sourceURL)

		sanitizedHTML, err = sanitizedDoc.Html()
		if err != nil {
			return nil, err
		}
	}

	// Step 5: Convert to Markdown
	markdown, err := p.mdConverter.Convert(sanitizedHTML)
	if err != nil {
		return nil, err
	}

	// Step 6: Calculate statistics
	plainText := StripMarkdown(markdown)
	wordCount := CountWords(plainText)
	charCount := CountChars(plainText)
	contentHash := calculateHash(markdown)

	// Step 7: Build document
	document := &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        markdown,
		HTMLContent:    html,
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        headers,
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}

	return document, nil
}

// calculateHash calculates SHA256 hash of content
func calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}

// ConvertHTML is a convenience function for simple HTML to Markdown conversion
func ConvertHTML(html, sourceURL string) (*domain.Document, error) {
	pipeline := NewPipeline(PipelineOptions{
		BaseURL: sourceURL,
	})
	return pipeline.Convert(context.Background(), html, sourceURL)
}

// ConvertHTMLWithSelector converts HTML with a specific content selector
func ConvertHTMLWithSelector(html, sourceURL, selector string) (*domain.Document, error) {
	pipeline := NewPipeline(PipelineOptions{
		BaseURL:         sourceURL,
		ContentSelector: selector,
	})
	return pipeline.Convert(context.Background(), html, sourceURL)
}

// removeExcluded removes elements matching the exclude selector from HTML content
func (p *Pipeline) removeExcluded(html string) string {
	if p.excludeSelector == "" {
		return html
	}

	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return html
	}

	_ = p.removeExcludedFromSelection(doc.Selection)

	result, err := doc.Find("body").Html()
	if err != nil {
		// If body extraction fails, try getting the whole document
		result, err = doc.Html()
		if err != nil {
			return html
		}
	}

	return result
}

func (p *Pipeline) removeExcludedFromSelection(sel *goquery.Selection) *goquery.Selection {
	if p.excludeSelector == "" || sel == nil {
		return sel
	}

	findWithRoot(sel, p.excludeSelector).Remove()
	return sel
}

func selectionHTML(sel *goquery.Selection) (string, error) {
	if sel == nil {
		return "", nil
	}

	var combined strings.Builder
	found := false
	sel.Each(func(_ int, node *goquery.Selection) {
		if html, err := node.Html(); err == nil {
			combined.WriteString(html)
			found = true
		}
	})

	if !found {
		return sel.Html()
	}

	return combined.String(), nil
}
</file>
<file path="internal/converter/plaintext_reader.go">
package converter

import (
	"crypto/sha256"
	"encoding/hex"
	"net/url"
	"path"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type PlainTextReader struct{}

func NewPlainTextReader() *PlainTextReader {
	return &PlainTextReader{}
}

func (r *PlainTextReader) Read(content, sourceURL string) (*domain.Document, error) {
	content = strings.TrimSpace(content)

	title := r.extractTitle(content, sourceURL)
	description := r.extractDescription(content)
	links := r.extractLinks(content, sourceURL)

	wordCount := CountWords(content)
	charCount := CountChars(content)
	contentHash := r.calculateHash(content)

	return &domain.Document{
		URL:            sourceURL,
		Title:          title,
		Description:    description,
		Content:        content,
		HTMLContent:    "",
		FetchedAt:      time.Now(),
		ContentHash:    contentHash,
		WordCount:      wordCount,
		CharCount:      charCount,
		Links:          links,
		Headers:        make(map[string][]string),
		RenderedWithJS: false,
		SourceStrategy: "",
		CacheHit:       false,
	}, nil
}

func (r *PlainTextReader) extractTitle(content, sourceURL string) string {
	lines := strings.Split(content, "\n")
	for _, line := range lines {
		trimmed := strings.TrimSpace(line)
		if trimmed == "" {
			continue
		}

		if strings.HasPrefix(trimmed, "# ") {
			title := strings.TrimPrefix(trimmed, "# ")
			return strings.TrimSpace(title)
		}

		if len(trimmed) > 100 {
			return trimmed[:97] + "..."
		}
		return trimmed
	}

	parsed, err := url.Parse(sourceURL)
	if err == nil {
		filename := path.Base(parsed.Path)
		if filename != "" && filename != "." && filename != "/" {
			return strings.TrimSuffix(filename, ".txt")
		}
	}

	return ""
}

func (r *PlainTextReader) extractDescription(content string) string {
	lines := strings.Split(content, "\n")
	var descLines []string
	started := false

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if !started {
			if trimmed == "" || strings.HasPrefix(trimmed, "#") {
				continue
			}
			started = true
		}

		if trimmed == "" && started {
			break
		}

		descLines = append(descLines, trimmed)
	}

	if len(descLines) > 0 {
		desc := strings.Join(descLines, " ")
		if len(desc) > 300 {
			return desc[:297] + "..."
		}
		return desc
	}

	return ""
}

var plainTextLinkRegex = regexp.MustCompile(`\[([^\]]*)\]\(([^)\s]+)(?:\s+"[^"]*")?\)`)

func (r *PlainTextReader) extractLinks(content, baseURL string) []string {
	var links []string
	seen := make(map[string]bool)
	base, _ := url.Parse(baseURL)

	matches := plainTextLinkRegex.FindAllStringSubmatch(content, -1)
	for _, match := range matches {
		if len(match) >= 3 {
			href := strings.TrimSpace(match[2])

			if href == "" ||
				strings.HasPrefix(href, "#") ||
				strings.HasPrefix(href, "javascript:") ||
				strings.HasPrefix(href, "mailto:") ||
				strings.HasPrefix(href, "tel:") {
				continue
			}

			if base != nil && !strings.HasPrefix(href, "http://") && !strings.HasPrefix(href, "https://") {
				if refURL, err := url.Parse(href); err == nil {
					href = base.ResolveReference(refURL).String()
				}
			}

			if !seen[href] {
				seen[href] = true
				links = append(links, href)
			}
		}
	}

	return links
}

func (r *PlainTextReader) calculateHash(content string) string {
	hash := sha256.Sum256([]byte(content))
	return hex.EncodeToString(hash[:])
}
</file>
<file path="internal/converter/readability.go">
package converter

import (
	"errors"
	"fmt"
	"net/url"
	"strings"

	"github.com/PuerkitoBio/goquery"
	"github.com/go-shiori/go-readability"
	"github.com/rs/zerolog/log"
)

// ErrSelectorNotFound indicates no elements matched the selector.
var ErrSelectorNotFound = errors.New("selector not found")

// ExtractContent extracts the main content from HTML
type ExtractContent struct {
	selector string
}

// ExtractOptions contains options for content extraction
type ExtractOptions struct {
	Selector string // CSS selector for main content
	URL      string // Source URL for resolving relative links
}

// NewExtractContent creates a new content extractor
func NewExtractContent(selector string) *ExtractContent {
	return &ExtractContent{selector: selector}
}

// Extract extracts main content from HTML
func (e *ExtractContent) Extract(html, sourceURL string) (string, string, error) {
	// If a selector is provided, use it directly
	if e.selector != "" {
		doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
		if err != nil {
			return "", "", err
		}

		content, title, err := e.ExtractFromDocument(doc, sourceURL)
		if err == nil {
			return content, title, nil
		}
		if errors.Is(err, ErrSelectorNotFound) {
			return e.extractWithReadability(html, sourceURL)
		}

		return "", "", err
	}

	// Otherwise, use readability algorithm
	return e.extractWithReadability(html, sourceURL)
}

// ExtractFromDocument extracts content using a pre-parsed document.
func (e *ExtractContent) ExtractFromDocument(doc *goquery.Document, sourceURL string) (string, string, error) {
	if e.selector == "" {
		return "", "", fmt.Errorf("extract from document requires selector")
	}

	if doc == nil {
		return "", "", fmt.Errorf("extract from document requires document")
	}

	content := doc.Find(e.selector)
	matchCount := content.Length()

	log.Debug().
		Str("selector", e.selector).
		Int("matches", matchCount).
		Str("url", sourceURL).
		Msg("Content selector applied")

	if matchCount == 0 {
		log.Debug().
			Str("selector", e.selector).
			Str("url", sourceURL).
			Msg("Selector not found, falling back to Readability algorithm")
		return "", "", ErrSelectorNotFound
	}

	title := extractTitle(doc)

	var combined strings.Builder
	content.Each(func(_ int, sel *goquery.Selection) {
		if h, err := sel.Html(); err == nil {
			combined.WriteString(h)
		}
	})

	return combined.String(), title, nil
}

// extractWithSelector extracts content using a CSS selector
func (e *ExtractContent) extractWithSelector(html, sourceURL string) (string, string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return "", "", err
	}

	return e.ExtractFromDocument(doc, sourceURL)
}

// extractWithReadability extracts content using the readability algorithm
func (e *ExtractContent) extractWithReadability(html, sourceURL string) (string, string, error) {
	parsedURL, err := url.Parse(sourceURL)
	if err != nil {
		parsedURL = &url.URL{Scheme: "https", Host: "example.com"}
	}

	article, err := readability.FromReader(strings.NewReader(html), parsedURL)
	if err != nil {
		// If readability fails, try to extract the body
		return e.extractBody(html)
	}

	return article.Content, article.Title, nil
}

// extractBody extracts the body content as a fallback
func (e *ExtractContent) extractBody(html string) (string, string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return html, "", nil
	}

	title := extractTitle(doc)

	// Get body content
	body := doc.Find("body")
	if body.Length() == 0 {
		return html, title, nil
	}

	bodyHTML, err := body.Html()
	if err != nil {
		return html, title, nil
	}

	return bodyHTML, title, nil
}

// extractTitle extracts the page title
func extractTitle(doc *goquery.Document) string {
	// Try <title> tag
	title := strings.TrimSpace(doc.Find("title").First().Text())
	if title != "" {
		return title
	}

	// Try <h1> tag
	h1 := strings.TrimSpace(doc.Find("h1").First().Text())
	if h1 != "" {
		return h1
	}

	// Try og:title meta tag
	ogTitle, exists := doc.Find("meta[property='og:title']").Attr("content")
	if exists && ogTitle != "" {
		return ogTitle
	}

	return ""
}

// ExtractDescription extracts the page description
func ExtractDescription(doc *goquery.Document) string {
	// Try meta description
	desc, exists := doc.Find("meta[name='description']").Attr("content")
	if exists && desc != "" {
		return strings.TrimSpace(desc)
	}

	// Try og:description
	ogDesc, exists := doc.Find("meta[property='og:description']").Attr("content")
	if exists && ogDesc != "" {
		return strings.TrimSpace(ogDesc)
	}

	return ""
}

// ExtractHeaders extracts all headers from HTML
func ExtractHeaders(html string) map[string][]string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return map[string][]string{}
	}

	return ExtractHeadersFromDoc(doc)
}

// ExtractHeadersFromDoc extracts headers from a parsed document.
func ExtractHeadersFromDoc(doc *goquery.Document) map[string][]string {
	return extractHeadersFromSelection(doc.Selection)
}

func extractHeadersFromSelection(sel *goquery.Selection) map[string][]string {
	headers := make(map[string][]string)

	for i := 1; i <= 6; i++ {
		tag := string('h') + string('0'+byte(i)) // h1, h2, ..., h6
		findWithRoot(sel, tag).Each(func(_ int, node *goquery.Selection) {
			text := strings.TrimSpace(node.Text())
			if text != "" {
				headers[tag] = append(headers[tag], text)
			}
		})
	}

	return headers
}

// ExtractLinks extracts all links from HTML
func ExtractLinks(html, baseURL string) []string {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return []string{}
	}

	return ExtractLinksFromDoc(doc, baseURL)
}

// ExtractLinksFromDoc extracts links from a parsed document.
func ExtractLinksFromDoc(doc *goquery.Document, baseURL string) []string {
	return extractLinksFromSelection(doc.Selection, baseURL)
}

func extractLinksFromSelection(sel *goquery.Selection, baseURL string) []string {
	var links []string

	base, _ := url.Parse(baseURL)

	findWithRoot(sel, "a[href]").Each(func(_ int, node *goquery.Selection) {
		href, exists := node.Attr("href")
		if !exists || href == "" {
			return
		}

		// Skip anchors, javascript, mailto
		if strings.HasPrefix(href, "#") ||
			strings.HasPrefix(href, "javascript:") ||
			strings.HasPrefix(href, "mailto:") ||
			strings.HasPrefix(href, "tel:") {
			return
		}

		// Resolve relative URLs
		if base != nil && !strings.HasPrefix(href, "http") {
			refURL, err := url.Parse(href)
			if err == nil {
				href = base.ResolveReference(refURL).String()
			}
		}

		links = append(links, href)
	})

	return links
}

func findWithRoot(sel *goquery.Selection, query string) *goquery.Selection {
	return sel.Filter(query).AddSelection(sel.Find(query))
}
</file>
<file path="internal/converter/sanitizer.go">
package converter

import (
	"net/url"
	"regexp"
	"strings"

	"github.com/PuerkitoBio/goquery"
)

// TagsToRemove are HTML tags that should be completely removed
var TagsToRemove = []string{
	"script",
	"style",
	"noscript",
	"iframe",
	"object",
	"embed",
	"applet",
	"form",
	"input",
	"button",
	"select",
	"textarea",
	"footer",
	"header",
	"aside",
	"advertisement",
	"banner",
}

// ClassesToRemove are CSS classes that indicate non-content elements
var ClassesToRemove = []string{
	"sidebar",
	"navigation",
	"nav",
	"menu",
	"footer",
	"header",
	"banner",
	"advertisement",
	"ad",
	"social",
	"share",
	"comment",
	"comments",
	"related",
	"recommended",
}

// IDsToRemove are element IDs that indicate non-content elements
var IDsToRemove = []string{
	"sidebar",
	"navigation",
	"nav",
	"menu",
	"footer",
	"header",
	"banner",
	"advertisement",
	"comments",
}

// Sanitizer cleans HTML content for conversion
type Sanitizer struct {
	baseURL          string
	removeNavigation bool
	removeComments   bool
}

// SanitizerOptions contains options for the sanitizer
type SanitizerOptions struct {
	BaseURL          string
	RemoveNavigation bool
	RemoveComments   bool
}

// NewSanitizer creates a new sanitizer
func NewSanitizer(opts SanitizerOptions) *Sanitizer {
	return &Sanitizer{
		baseURL:          opts.BaseURL,
		removeNavigation: opts.RemoveNavigation,
		removeComments:   opts.RemoveComments,
	}
}

// Sanitize cleans HTML content
func (s *Sanitizer) Sanitize(html string) (string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return "", err
	}

	cleanDoc, err := s.SanitizeDocument(doc)
	if err != nil {
		return "", err
	}

	result, err := cleanDoc.Html()
	if err != nil {
		return "", err
	}

	return result, nil
}

// SanitizeDocument cleans a pre-parsed document in place.
func (s *Sanitizer) SanitizeDocument(doc *goquery.Document) (*goquery.Document, error) {
	if doc == nil {
		return nil, nil
	}

	s.sanitizeSelection(doc.Selection)
	return doc, nil
}

// SanitizeSelection cleans a selection in place.
func (s *Sanitizer) SanitizeSelection(sel *goquery.Selection) (*goquery.Selection, error) {
	if sel == nil {
		return nil, nil
	}

	s.sanitizeSelection(sel)
	return sel, nil
}

func (s *Sanitizer) sanitizeSelection(sel *goquery.Selection) {
	// Remove unwanted tags
	for _, tag := range TagsToRemove {
		findWithRoot(sel, tag).Remove()
	}

	// Remove elements by class
	if s.removeNavigation {
		for _, class := range ClassesToRemove {
			findWithRoot(sel, "."+class).Remove()
			findWithRoot(sel, "[class*='"+class+"']").Remove()
		}

		// Remove elements by ID
		for _, id := range IDsToRemove {
			findWithRoot(sel, "#"+id).Remove()
		}

		findWithRoot(sel, "nav").Remove()
	}

	// Remove hidden elements
	findWithRoot(sel, "[style*='display:none']").Remove()
	findWithRoot(sel, "[style*='display: none']").Remove()
	findWithRoot(sel, "[hidden]").Remove()

	// Normalize URLs if base URL is provided
	if s.baseURL != "" {
		s.normalizeURLsFromSelection(sel)
	}

	// Remove empty paragraphs and divs
	s.removeEmptyElementsFromSelection(sel)
}

// normalizeURLs converts relative URLs to absolute URLs
func (s *Sanitizer) normalizeURLs(doc *goquery.Document) {
	s.normalizeURLsFromSelection(doc.Selection)
}

func (s *Sanitizer) normalizeURLsFromSelection(sel *goquery.Selection) {
	base, err := url.Parse(s.baseURL)
	if err != nil {
		return
	}

	// Normalize href attributes
	findWithRoot(sel, "a[href]").Each(func(_ int, node *goquery.Selection) {
		if href, exists := node.Attr("href"); exists {
			if absoluteURL := resolveURL(base, href); absoluteURL != "" {
				node.SetAttr("href", absoluteURL)
			}
		}
	})

	// Normalize src attributes
	findWithRoot(sel, "[src]").Each(func(_ int, node *goquery.Selection) {
		if src, exists := node.Attr("src"); exists {
			if absoluteURL := resolveURL(base, src); absoluteURL != "" {
				node.SetAttr("src", absoluteURL)
			}
		}
	})

	// Normalize srcset attributes
	findWithRoot(sel, "[srcset]").Each(func(_ int, node *goquery.Selection) {
		if srcset, exists := node.Attr("srcset"); exists {
			node.SetAttr("srcset", normalizeSrcset(base, srcset))
		}
	})
}

// resolveURL resolves a relative URL against a base URL
func resolveURL(base *url.URL, ref string) string {
	// Skip empty, fragment, javascript, mailto, and data URLs
	if ref == "" || strings.HasPrefix(ref, "#") ||
		strings.HasPrefix(ref, "javascript:") ||
		strings.HasPrefix(ref, "mailto:") ||
		strings.HasPrefix(ref, "data:") {
		return ref
	}

	refURL, err := url.Parse(ref)
	if err != nil {
		return ref
	}

	return base.ResolveReference(refURL).String()
}

// normalizeSrcset normalizes URLs in srcset attribute
func normalizeSrcset(base *url.URL, srcset string) string {
	parts := strings.Split(srcset, ",")
	for i, part := range parts {
		part = strings.TrimSpace(part)
		tokens := strings.Fields(part)
		if len(tokens) > 0 {
			tokens[0] = resolveURL(base, tokens[0])
			parts[i] = strings.Join(tokens, " ")
		}
	}
	return strings.Join(parts, ", ")
}

// removeEmptyElements removes empty block elements
func (s *Sanitizer) removeEmptyElements(doc *goquery.Document) {
	s.removeEmptyElementsFromSelection(doc.Selection)
}

func (s *Sanitizer) removeEmptyElementsFromSelection(sel *goquery.Selection) {
	emptyTags := []string{"p", "div", "span", "section", "article"}
	whitespaceRegex := regexp.MustCompile(`^\s*$`)

	for _, tag := range emptyTags {
		findWithRoot(sel, tag).Each(func(_ int, node *goquery.Selection) {
			text := strings.TrimSpace(node.Text())
			if whitespaceRegex.MatchString(text) && node.Children().Length() == 0 {
				node.Remove()
			}
		})
	}
}
</file>
<file path="internal/domain/errors.go">
package domain

import (
	"errors"
	"fmt"
)

// Sentinel errors
var (
	// ErrNotFound indicates a resource was not found
	ErrNotFound = errors.New("not found")

	// ErrCacheMiss indicates a cache miss
	ErrCacheMiss = errors.New("cache miss")

	// ErrCacheExpired indicates the cached entry has expired
	ErrCacheExpired = errors.New("cache entry expired")

	// ErrRateLimited indicates rate limiting was encountered
	ErrRateLimited = errors.New("rate limited")

	// ErrBlocked indicates the request was blocked (e.g., by Cloudflare)
	ErrBlocked = errors.New("request blocked")

	// ErrTimeout indicates a timeout occurred
	ErrTimeout = errors.New("timeout")

	// ErrInvalidURL indicates an invalid URL was provided
	ErrInvalidURL = errors.New("invalid URL")

	// ErrNoStrategy indicates no strategy can handle the URL
	ErrNoStrategy = errors.New("no strategy found for URL")

	// ErrRenderFailed indicates JavaScript rendering failed
	ErrRenderFailed = errors.New("render failed")

	// ErrConversionFailed indicates HTML to Markdown conversion failed
	ErrConversionFailed = errors.New("conversion failed")

	// ErrWriteFailed indicates writing output failed
	ErrWriteFailed = errors.New("write failed")

	// ErrBrowserNotFound indicates Chrome/Chromium was not found
	ErrBrowserNotFound = errors.New("browser not found")
)

// FetchError represents an error during fetching
type FetchError struct {
	URL        string
	StatusCode int
	Err        error
}

func (e *FetchError) Error() string {
	if e.StatusCode > 0 {
		return fmt.Sprintf("fetch error for %s: status %d: %v", e.URL, e.StatusCode, e.Err)
	}
	return fmt.Sprintf("fetch error for %s: %v", e.URL, e.Err)
}

func (e *FetchError) Unwrap() error {
	return e.Err
}

// NewFetchError creates a new FetchError
func NewFetchError(url string, statusCode int, err error) *FetchError {
	return &FetchError{
		URL:        url,
		StatusCode: statusCode,
		Err:        err,
	}
}

// RetryableError indicates an error that can be retried
type RetryableError struct {
	Err        error
	RetryAfter int // Seconds to wait before retry, 0 if unknown
}

func (e *RetryableError) Error() string {
	if e.RetryAfter > 0 {
		return fmt.Sprintf("retryable error (retry after %ds): %v", e.RetryAfter, e.Err)
	}
	return fmt.Sprintf("retryable error: %v", e.Err)
}

func (e *RetryableError) Unwrap() error {
	return e.Err
}

// IsRetryable checks if an error should be retried
func IsRetryable(err error) bool {
	var retryable *RetryableError
	if errors.As(err, &retryable) {
		return true
	}

	var fetchErr *FetchError
	if errors.As(err, &fetchErr) {
		// Retry on specific status codes
		switch fetchErr.StatusCode {
		case 429, 503, 502, 504:
			return true
		}
		// Retry on Cloudflare errors
		if fetchErr.StatusCode >= 520 && fetchErr.StatusCode <= 530 {
			return true
		}
	}

	return errors.Is(err, ErrRateLimited) || errors.Is(err, ErrTimeout)
}

// ValidationError represents a validation error
type ValidationError struct {
	Field   string
	Message string
}

func (e *ValidationError) Error() string {
	return fmt.Sprintf("validation error for %s: %s", e.Field, e.Message)
}

// NewValidationError creates a new ValidationError
func NewValidationError(field, message string) *ValidationError {
	return &ValidationError{
		Field:   field,
		Message: message,
	}
}

// StrategyError represents an error in strategy execution
type StrategyError struct {
	Strategy string
	URL      string
	Err      error
}

func (e *StrategyError) Error() string {
	return fmt.Sprintf("strategy %s failed for %s: %v", e.Strategy, e.URL, e.Err)
}

func (e *StrategyError) Unwrap() error {
	return e.Err
}

// NewStrategyError creates a new StrategyError
func NewStrategyError(strategy, url string, err error) *StrategyError {
	return &StrategyError{
		Strategy: strategy,
		URL:      url,
		Err:      err,
	}
}

// =============================================================================
// LLM Errors
// =============================================================================

// LLM sentinel errors
var (
	// ErrLLMNotConfigured indicates LLM provider is not configured
	ErrLLMNotConfigured = errors.New("LLM provider not configured")

	// ErrLLMMissingAPIKey indicates API key is required but not provided
	ErrLLMMissingAPIKey = errors.New("LLM API key is required")

	// ErrLLMMissingBaseURL indicates base URL is required but not provided
	ErrLLMMissingBaseURL = errors.New("LLM base URL is required")

	// ErrLLMMissingModel indicates model is required but not provided
	ErrLLMMissingModel = errors.New("LLM model is required")

	// ErrLLMInvalidProvider indicates an invalid provider type
	ErrLLMInvalidProvider = errors.New("invalid LLM provider")

	// ErrLLMRequestFailed indicates the LLM request failed
	ErrLLMRequestFailed = errors.New("LLM request failed")

	// ErrLLMRateLimited indicates rate limit was exceeded
	ErrLLMRateLimited = errors.New("LLM rate limit exceeded")

	// ErrLLMAuthFailed indicates authentication failed
	ErrLLMAuthFailed = errors.New("LLM authentication failed")

	// ErrLLMContextTooLong indicates context length was exceeded
	ErrLLMContextTooLong = errors.New("LLM context length exceeded")

	// ErrLLMCircuitOpen indicates the circuit breaker is open
	ErrLLMCircuitOpen = errors.New("LLM circuit breaker is open")

	// ErrLLMMaxRetriesExceeded indicates all retry attempts failed
	ErrLLMMaxRetriesExceeded = errors.New("LLM max retries exceeded")
)

// LLMError represents an LLM-specific error
type LLMError struct {
	Provider   string
	StatusCode int
	Message    string
	Err        error
}

func (e *LLMError) Error() string {
	if e.StatusCode > 0 {
		return fmt.Sprintf("%s error (HTTP %d): %s", e.Provider, e.StatusCode, e.Message)
	}
	return fmt.Sprintf("%s error: %s", e.Provider, e.Message)
}

func (e *LLMError) Unwrap() error {
	return e.Err
}

// NewLLMError creates a new LLMError
func NewLLMError(provider string, statusCode int, message string, err error) *LLMError {
	return &LLMError{
		Provider:   provider,
		StatusCode: statusCode,
		Message:    message,
		Err:        err,
	}
}
</file>
<file path="internal/domain/interfaces.go">
package domain

import (
	"context"
	"net/http"
	"time"
)

// Strategy defines the interface for documentation extraction strategies
type Strategy interface {
	// Name returns the strategy name
	Name() string
	// CanHandle returns true if this strategy can handle the given URL
	CanHandle(url string) bool
	// Execute runs the extraction strategy
	Execute(ctx context.Context, url string, opts StrategyOptions) error
}

// StrategyOptions contains options for strategy execution
type StrategyOptions struct {
	CommonOptions
	Output          string
	Concurrency     int
	MaxDepth        int
	Exclude         []string
	NoFolders       bool
	Split           bool
	IncludeAssets   bool
	ContentSelector string
}

// Fetcher defines the interface for HTTP fetching with stealth capabilities
type Fetcher interface {
	// Get fetches content from a URL
	Get(ctx context.Context, url string) (*Response, error)
	// GetWithHeaders fetches content with custom headers
	GetWithHeaders(ctx context.Context, url string, headers map[string]string) (*Response, error)
	// GetCookies returns cookies for a URL (for sharing with renderer)
	GetCookies(url string) []*http.Cookie
	// Transport returns an http.RoundTripper for integration with other HTTP clients (e.g., colly)
	Transport() http.RoundTripper
	// Close releases resources
	Close() error
}

// Response represents an HTTP response
type Response struct {
	StatusCode  int
	Body        []byte
	Headers     http.Header
	ContentType string
	URL         string
	FromCache   bool
}

// Renderer defines the interface for JavaScript rendering
type Renderer interface {
	// Render fetches and renders a page with JavaScript
	Render(ctx context.Context, url string, opts RenderOptions) (string, error)
	// Close releases browser resources
	Close() error
}

// RenderOptions contains options for page rendering
type RenderOptions struct {
	Timeout     time.Duration
	WaitFor     string        // CSS selector to wait for
	WaitStable  time.Duration // Wait for network idle
	ScrollToEnd bool          // Scroll to load lazy content
	Cookies     []*http.Cookie
}

// Cache defines the interface for content caching
type Cache interface {
	// Get retrieves a value from cache
	Get(ctx context.Context, key string) ([]byte, error)
	// Set stores a value in cache with TTL
	Set(ctx context.Context, key string, value []byte, ttl time.Duration) error
	// Has checks if a key exists in cache
	Has(ctx context.Context, key string) bool
	// Delete removes a key from cache
	Delete(ctx context.Context, key string) error
	// Close releases cache resources
	Close() error
}

// Converter defines the interface for HTML to Markdown conversion
type Converter interface {
	// Convert transforms HTML content to a Document
	Convert(ctx context.Context, html string, sourceURL string) (*Document, error)
}

// Writer defines the interface for output writing
type Writer interface {
	// Write saves a document to the output directory
	Write(ctx context.Context, doc *Document) error
}

// LLMProvider defines the interface for LLM interactions
type LLMProvider interface {
	// Name returns the provider name (openai, anthropic, google)
	Name() string
	// Complete sends a request and returns the response
	Complete(ctx context.Context, req *LLMRequest) (*LLMResponse, error)
	// Close releases resources
	Close() error
}
</file>
<file path="internal/domain/models.go">
package domain

import "time"

// Document represents a processed documentation page
type Document struct {
	URL            string              `json:"url"`
	Title          string              `json:"title"`
	Description    string              `json:"description,omitempty"`
	Content        string              `json:"-"` // Markdown content (not in JSON)
	HTMLContent    string              `json:"-"` // Original HTML (not in JSON)
	FetchedAt      time.Time           `json:"fetched_at"`
	ContentHash    string              `json:"content_hash"`
	WordCount      int                 `json:"word_count"`
	CharCount      int                 `json:"char_count"`
	Links          []string            `json:"links,omitempty"`
	Headers        map[string][]string `json:"headers,omitempty"` // h1, h2, h3...
	RenderedWithJS bool                `json:"rendered_with_js"`
	SourceStrategy string              `json:"source_strategy"`
	CacheHit       bool                `json:"cache_hit"`
	RelativePath   string              `json:"-"` // Relative path for Git-sourced files (used for output structure)

	// LLM-enhanced metadata fields
	Summary  string   `json:"summary,omitempty"`  // AI-generated summary
	Tags     []string `json:"tags,omitempty"`     // AI-generated tags
	Category string   `json:"category,omitempty"` // AI-generated category
}

// Page represents a raw fetched page before conversion
type Page struct {
	URL         string
	Content     []byte
	ContentType string
	StatusCode  int
	FetchedAt   time.Time
	FromCache   bool
	RenderedJS  bool
}

// CacheEntry represents a cached page entry
type CacheEntry struct {
	URL         string    `json:"url"`
	Content     []byte    `json:"content"`
	ContentType string    `json:"content_type"`
	FetchedAt   time.Time `json:"fetched_at"`
	ExpiresAt   time.Time `json:"expires_at"`
}

// SitemapURL represents a URL entry in a sitemap
type SitemapURL struct {
	Loc        string    `xml:"loc"`
	LastMod    time.Time `xml:"-"`
	LastModStr string    `xml:"lastmod"`
	ChangeFreq string    `xml:"changefreq"`
	Priority   float64   `xml:"priority"`
}

// Sitemap represents a parsed sitemap
type Sitemap struct {
	URLs      []SitemapURL
	Sitemaps  []string // For sitemap index files
	IsIndex   bool
	SourceURL string
}

// LLMSLink represents a link parsed from llms.txt
type LLMSLink struct {
	Title string
	URL   string
}

// Deprecated: Metadata is replaced by SimpleMetadata for JSON output.
// Use SimpleMetadata for cleaner, LLM-evaluation-friendly metadata.
type Metadata struct {
	URL            string              `json:"url"`
	Title          string              `json:"title"`
	Description    string              `json:"description,omitempty"`
	FetchedAt      time.Time           `json:"fetched_at"`
	ContentHash    string              `json:"content_hash"`
	WordCount      int                 `json:"word_count"`
	CharCount      int                 `json:"char_count"`
	Links          []string            `json:"links,omitempty"`
	Headers        map[string][]string `json:"headers,omitempty"`
	RenderedWithJS bool                `json:"rendered_with_js"`
	SourceStrategy string              `json:"source_strategy"`
	CacheHit       bool                `json:"cache_hit"`
	Summary        string              `json:"summary,omitempty"`
	Tags           []string            `json:"tags,omitempty"`
	Category       string              `json:"category,omitempty"`
}

// ToMetadata converts a Document to Metadata
func (d *Document) ToMetadata() *Metadata {
	return &Metadata{
		URL:            d.URL,
		Title:          d.Title,
		Description:    d.Description,
		FetchedAt:      d.FetchedAt,
		ContentHash:    d.ContentHash,
		WordCount:      d.WordCount,
		CharCount:      d.CharCount,
		Links:          d.Links,
		Headers:        d.Headers,
		RenderedWithJS: d.RenderedWithJS,
		SourceStrategy: d.SourceStrategy,
		CacheHit:       d.CacheHit,
		Summary:        d.Summary,
		Tags:           d.Tags,
		Category:       d.Category,
	}
}

// Frontmatter represents YAML frontmatter for markdown files
type Frontmatter struct {
	Title      string    `yaml:"title"`
	URL        string    `yaml:"url"`
	Source     string    `yaml:"source"`
	FetchedAt  time.Time `yaml:"fetched_at"`
	RenderedJS bool      `yaml:"rendered_js"`
	WordCount  int       `yaml:"word_count"`
	Summary    string    `yaml:"summary,omitempty"`
	Tags       []string  `yaml:"tags,omitempty"`
	Category   string    `yaml:"category,omitempty"`
}

// ToFrontmatter converts a Document to Frontmatter
func (d *Document) ToFrontmatter() *Frontmatter {
	return &Frontmatter{
		Title:      d.Title,
		URL:        d.URL,
		Source:     d.SourceStrategy,
		FetchedAt:  d.FetchedAt,
		RenderedJS: d.RenderedWithJS,
		WordCount:  d.WordCount,
		Summary:    d.Summary,
		Tags:       d.Tags,
		Category:   d.Category,
	}
}

// Deprecated: MetadataIndex is replaced by SimpleMetadataIndex for JSON output.
type MetadataIndex struct {
	GeneratedAt    time.Time          `json:"generated_at"`
	SourceURL      string             `json:"source_url"`
	Strategy       string             `json:"strategy"`
	TotalDocuments int                `json:"total_documents"`
	TotalWordCount int                `json:"total_word_count"`
	TotalCharCount int                `json:"total_char_count"`
	Documents      []DocumentMetadata `json:"documents"`
}

// Deprecated: DocumentMetadata is replaced by SimpleDocumentMetadata for JSON output.
type DocumentMetadata struct {
	FilePath string `json:"file_path"`
	*Metadata
}

// ToDocumentMetadata creates a DocumentMetadata from a Document
func (d *Document) ToDocumentMetadata(filePath string) *DocumentMetadata {
	return &DocumentMetadata{
		FilePath: filePath,
		Metadata: d.ToMetadata(),
	}
}

// =============================================================================
// Simple Metadata Types (Simplified JSON output for LLM evaluation)
// =============================================================================

// SimpleMetadata represents simplified document metadata for JSON output
// This is a cleaner structure optimized for LLM evaluation, containing only
// essential fields without technical metadata like content_hash, word_count, etc.
type SimpleMetadata struct {
	Title       string    `json:"title"`
	URL         string    `json:"url"`
	Source      string    `json:"source"`
	FetchedAt   time.Time `json:"fetched_at"`
	Description string    `json:"description,omitempty"`
	Summary     string    `json:"summary,omitempty"`
	Tags        []string  `json:"tags,omitempty"`
	Category    string    `json:"category,omitempty"`
}

// SimpleDocumentMetadata adds file_path to SimpleMetadata for document indexing
type SimpleDocumentMetadata struct {
	FilePath string `json:"file_path"`
	*SimpleMetadata
}

// SimpleMetadataIndex represents the consolidated JSON output with simplified metadata
type SimpleMetadataIndex struct {
	GeneratedAt    time.Time                `json:"generated_at"`
	SourceURL      string                   `json:"source_url"`
	Strategy       string                   `json:"strategy"`
	TotalDocuments int                      `json:"total_documents"`
	Documents      []SimpleDocumentMetadata `json:"documents"`
}

// ToSimpleMetadata converts a Document to SimpleMetadata
func (d *Document) ToSimpleMetadata() *SimpleMetadata {
	return &SimpleMetadata{
		Title:       d.Title,
		URL:         d.URL,
		Source:      d.SourceStrategy,
		FetchedAt:   d.FetchedAt,
		Description: d.Description,
		Summary:     d.Summary,
		Tags:        d.Tags,
		Category:    d.Category,
	}
}

// ToSimpleDocumentMetadata creates a SimpleDocumentMetadata from a Document
func (d *Document) ToSimpleDocumentMetadata(filePath string) *SimpleDocumentMetadata {
	return &SimpleDocumentMetadata{
		FilePath:       filePath,
		SimpleMetadata: d.ToSimpleMetadata(),
	}
}

// =============================================================================
// LLM Types
// =============================================================================

// MessageRole represents the role in a conversation
type MessageRole string

const (
	// RoleSystem represents a system message
	RoleSystem MessageRole = "system"
	// RoleUser represents a user message
	RoleUser MessageRole = "user"
	// RoleAssistant represents an assistant message
	RoleAssistant MessageRole = "assistant"
)

// LLMMessage represents a message in the conversation
type LLMMessage struct {
	Role    MessageRole
	Content string
}

// LLMRequest represents a completion request
type LLMRequest struct {
	Messages    []LLMMessage
	MaxTokens   int      // 0 = use provider default
	Temperature *float64 // nil = use provider default
}

// LLMResponse represents the LLM response
type LLMResponse struct {
	Content      string
	Model        string
	FinishReason string
	Usage        LLMUsage
}

// LLMUsage contains token usage statistics
type LLMUsage struct {
	PromptTokens     int
	CompletionTokens int
	TotalTokens      int
}
</file>
<file path="internal/domain/options.go">
package domain

// CommonOptions contains shared configuration options for strategies and orchestration.
type CommonOptions struct {
	Verbose  bool
	DryRun   bool
	Force    bool
	RenderJS bool
	Limit    int
	Sync     bool
	FullSync bool
	Prune    bool
}

// DefaultCommonOptions returns CommonOptions with default values.
func DefaultCommonOptions() CommonOptions {
	return CommonOptions{}
}
</file>
<file path="internal/fetcher/client.go">
package fetcher

import (
	"context"
	"fmt"
	"io"
	"net/http"
	"net/url"
	"time"

	fhttp "github.com/bogdanfinn/fhttp"
	tls_client "github.com/bogdanfinn/tls-client"
	"github.com/bogdanfinn/tls-client/profiles"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Client is a stealth HTTP client using tls-client
type Client struct {
	tlsClient    tls_client.HttpClient
	userAgent    string
	retrier      *Retrier
	cache        domain.Cache
	cacheEnabled bool
	cacheTTL     time.Duration
}

// ClientOptions contains options for creating a Client
type ClientOptions struct {
	Timeout     time.Duration
	MaxRetries  int
	EnableCache bool
	CacheTTL    time.Duration
	Cache       domain.Cache
	UserAgent   string
	ProxyURL    string
}

// DefaultClientOptions returns default client options
func DefaultClientOptions() ClientOptions {
	return ClientOptions{
		Timeout:     30 * time.Second,
		MaxRetries:  3,
		EnableCache: true,
		CacheTTL:    24 * time.Hour,
		UserAgent:   "",
		ProxyURL:    "",
	}
}

// NewClient creates a new stealth HTTP client
func NewClient(opts ClientOptions) (*Client, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = 30 * time.Second
	}

	// TLS client options
	tlsOpts := []tls_client.HttpClientOption{
		tls_client.WithTimeoutSeconds(int(opts.Timeout.Seconds())),
		tls_client.WithClientProfile(profiles.Chrome_131),
		tls_client.WithRandomTLSExtensionOrder(),
		tls_client.WithNotFollowRedirects(),
	}

	if opts.ProxyURL != "" {
		tlsOpts = append(tlsOpts, tls_client.WithProxyUrl(opts.ProxyURL))
	}

	tlsClient, err := tls_client.NewHttpClient(tls_client.NewNoopLogger(), tlsOpts...)
	if err != nil {
		return nil, fmt.Errorf("failed to create tls client: %w", err)
	}

	// Create retrier
	retrier := NewRetrier(RetrierOptions{
		MaxRetries:      opts.MaxRetries,
		InitialInterval: 1 * time.Second,
		MaxInterval:     30 * time.Second,
		Multiplier:      2.0,
	})

	return &Client{
		tlsClient:    tlsClient,
		userAgent:    opts.UserAgent,
		retrier:      retrier,
		cache:        opts.Cache,
		cacheEnabled: opts.EnableCache,
		cacheTTL:     opts.CacheTTL,
	}, nil
}

// Get fetches content from a URL
func (c *Client) Get(ctx context.Context, url string) (*domain.Response, error) {
	return c.GetWithHeaders(ctx, url, nil)
}

// GetWithHeaders fetches content with custom headers
func (c *Client) GetWithHeaders(ctx context.Context, url string, extraHeaders map[string]string) (*domain.Response, error) {
	// Check cache first
	if c.cacheEnabled && c.cache != nil {
		cached, err := c.getFromCache(ctx, url)
		if err == nil && cached != nil {
			return cached, nil
		}
	}

	// Perform request with retry
	var resp *domain.Response
	err := c.retrier.Retry(ctx, func() error {
		var err error
		resp, err = c.doRequest(ctx, url, extraHeaders)
		return err
	})

	if err != nil {
		return nil, err
	}

	// Cache the response
	if c.cacheEnabled && c.cache != nil && resp != nil {
		_ = c.saveToCache(ctx, url, resp)
	}

	return resp, nil
}

// doRequest performs the actual HTTP request
func (c *Client) doRequest(ctx context.Context, targetURL string, extraHeaders map[string]string) (*domain.Response, error) {
	// Create request using fhttp (tls-client's http package)
	req, err := fhttp.NewRequest(fhttp.MethodGet, targetURL, nil)
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	// Apply stealth headers
	headers := StealthHeaders(c.userAgent)
	for k, v := range headers {
		req.Header.Set(k, v)
	}

	// Apply extra headers
	for k, v := range extraHeaders {
		req.Header.Set(k, v)
	}

	// Perform request
	resp, err := c.tlsClient.Do(req)
	if err != nil {
		return nil, &domain.FetchError{
			URL: targetURL,
			Err: fmt.Errorf("request failed: %w", err),
		}
	}
	defer resp.Body.Close()

	// Check for error status codes
	if resp.StatusCode >= 400 {
		if ShouldRetryStatus(resp.StatusCode) {
			return nil, &domain.RetryableError{
				Err:        &domain.FetchError{URL: targetURL, StatusCode: resp.StatusCode, Err: fmt.Errorf("HTTP %d", resp.StatusCode)},
				RetryAfter: int(ParseRetryAfter(resp.Header.Get("Retry-After")).Seconds()),
			}
		}
		return nil, &domain.FetchError{
			URL:        targetURL,
			StatusCode: resp.StatusCode,
			Err:        fmt.Errorf("HTTP %d", resp.StatusCode),
		}
	}

	// Read body
	body, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response body: %w", err)
	}

	// Convert fhttp.Header to http.Header
	httpHeaders := make(http.Header)
	for k, v := range resp.Header {
		httpHeaders[k] = v
	}

	return &domain.Response{
		StatusCode:  resp.StatusCode,
		Body:        body,
		Headers:     httpHeaders,
		ContentType: resp.Header.Get("Content-Type"),
		URL:         targetURL,
		FromCache:   false,
	}, nil
}

// GetCookies returns cookies for a URL (for sharing with renderer)
func (c *Client) GetCookies(rawURL string) []*http.Cookie {
	parsedURL, err := url.Parse(rawURL)
	if err != nil {
		return nil
	}
	cookies := c.tlsClient.GetCookies(parsedURL)
	result := make([]*http.Cookie, len(cookies))
	for i, cookie := range cookies {
		result[i] = &http.Cookie{
			Name:     cookie.Name,
			Value:    cookie.Value,
			Path:     cookie.Path,
			Domain:   cookie.Domain,
			Expires:  cookie.Expires,
			Secure:   cookie.Secure,
			HttpOnly: cookie.HttpOnly,
		}
	}
	return result
}

// Close releases client resources
func (c *Client) Close() error {
	// TLS client doesn't have a Close method, but we keep this for interface compliance
	return nil
}

// getFromCache retrieves a response from cache
func (c *Client) getFromCache(ctx context.Context, url string) (*domain.Response, error) {
	if c.cache == nil {
		return nil, domain.ErrCacheMiss
	}

	data, err := c.cache.Get(ctx, url)
	if err != nil {
		return nil, err
	}

	return &domain.Response{
		StatusCode:  200,
		Body:        data,
		ContentType: "text/html",
		URL:         url,
		FromCache:   true,
	}, nil
}

// saveToCache saves a response to cache
func (c *Client) saveToCache(ctx context.Context, url string, resp *domain.Response) error {
	if c.cache == nil {
		return nil
	}
	return c.cache.Set(ctx, url, resp.Body, c.cacheTTL)
}

// SetCache sets the cache implementation
func (c *Client) SetCache(cache domain.Cache) {
	c.cache = cache
}

// SetCacheEnabled enables or disables caching
func (c *Client) SetCacheEnabled(enabled bool) {
	c.cacheEnabled = enabled
}
</file>
<file path="internal/fetcher/retry.go">
package fetcher

import (
	"context"
	"time"

	"github.com/cenkalti/backoff/v4"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Retrier handles retry logic with exponential backoff
type Retrier struct {
	maxRetries      int
	initialInterval time.Duration
	maxInterval     time.Duration
	multiplier      float64
}

// RetrierOptions contains options for creating a Retrier
type RetrierOptions struct {
	MaxRetries      int
	InitialInterval time.Duration
	MaxInterval     time.Duration
	Multiplier      float64
}

// DefaultRetrierOptions returns default retrier options
func DefaultRetrierOptions() RetrierOptions {
	return RetrierOptions{
		MaxRetries:      3,
		InitialInterval: 1 * time.Second,
		MaxInterval:     30 * time.Second,
		Multiplier:      2.0,
	}
}

// NewRetrier creates a new Retrier with the given options
func NewRetrier(opts RetrierOptions) *Retrier {
	if opts.MaxRetries <= 0 {
		opts.MaxRetries = 3
	}
	if opts.InitialInterval <= 0 {
		opts.InitialInterval = 1 * time.Second
	}
	if opts.MaxInterval <= 0 {
		opts.MaxInterval = 30 * time.Second
	}
	if opts.Multiplier <= 0 {
		opts.Multiplier = 2.0
	}

	return &Retrier{
		maxRetries:      opts.MaxRetries,
		initialInterval: opts.InitialInterval,
		maxInterval:     opts.MaxInterval,
		multiplier:      opts.Multiplier,
	}
}

// newBackoff creates a new exponential backoff
func (r *Retrier) newBackoff() backoff.BackOff {
	b := backoff.NewExponentialBackOff()
	b.InitialInterval = r.initialInterval
	b.MaxInterval = r.maxInterval
	b.Multiplier = r.multiplier
	b.RandomizationFactor = 0.5
	b.Reset()

	return backoff.WithMaxRetries(b, uint64(r.maxRetries))
}

// Retry executes an operation with exponential backoff
func (r *Retrier) Retry(ctx context.Context, operation func() error) error {
	b := r.newBackoff()
	b = backoff.WithContext(b, ctx)

	return backoff.Retry(func() error {
		err := operation()
		if err == nil {
			return nil
		}

		// Check if error is retryable
		if !domain.IsRetryable(err) {
			return backoff.Permanent(err)
		}

		return err
	}, b)
}

// RetryWithValue executes an operation with exponential backoff and returns a value
func RetryWithValue[T any](ctx context.Context, r *Retrier, operation func() (T, error)) (T, error) {
	var result T
	var lastErr error

	b := r.newBackoff()
	b = backoff.WithContext(b, ctx)

	err := backoff.Retry(func() error {
		var err error
		result, err = operation()
		if err == nil {
			return nil
		}

		lastErr = err

		// Check if error is retryable
		if !domain.IsRetryable(err) {
			return backoff.Permanent(err)
		}

		return err
	}, b)

	if err != nil {
		return result, lastErr
	}

	return result, nil
}

// ShouldRetryStatus returns true if the HTTP status code should be retried
func ShouldRetryStatus(statusCode int) bool {
	switch statusCode {
	case 429: // Too Many Requests
		return true
	case 502: // Bad Gateway
		return true
	case 503: // Service Unavailable
		return true
	case 504: // Gateway Timeout
		return true
	}

	// Cloudflare errors (520-530)
	if statusCode >= 520 && statusCode <= 530 {
		return true
	}

	return false
}

// ParseRetryAfter parses the Retry-After header value
func ParseRetryAfter(retryAfter string) time.Duration {
	if retryAfter == "" {
		return 0
	}

	// Try to parse as seconds
	var seconds int
	if _, err := parseRetryAfterInt(retryAfter, &seconds); err == nil && seconds > 0 {
		return time.Duration(seconds) * time.Second
	}

	// Try to parse as HTTP date (simplified)
	// Full parsing would use time.Parse with HTTP date format
	return 0
}

// parseRetryAfterInt is a helper to parse retry-after as int
func parseRetryAfterInt(s string, result *int) (int, error) {
	n := 0
	for _, c := range s {
		if c < '0' || c > '9' {
			break
		}
		n = n*10 + int(c-'0')
	}
	*result = n
	return n, nil
}
</file>
<file path="internal/fetcher/stealth.go">
package fetcher

import (
	"math/rand"
	"time"
)

// UserAgents is a pool of real Chrome/Firefox/Safari user agents
var UserAgents = []string{
	// Chrome on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36",
	// Chrome on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	// Chrome on Linux
	"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36",
	"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36",
	// Firefox on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Firefox on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Firefox on Linux
	"Mozilla/5.0 (X11; Linux x86_64; rv:132.0) Gecko/20100101 Firefox/132.0",
	"Mozilla/5.0 (X11; Linux x86_64; rv:131.0) Gecko/20100101 Firefox/131.0",
	// Safari on macOS
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15",
	"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.5 Safari/605.1.15",
	// Edge on Windows
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/131.0.0.0 Safari/537.36 Edg/131.0.0.0",
	"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/130.0.0.0 Safari/537.36 Edg/130.0.0.0",
}

// AcceptLanguages are common Accept-Language header values
var AcceptLanguages = []string{
	"en-US,en;q=0.9",
	"en-GB,en;q=0.9,en-US;q=0.8",
	"en-US,en;q=0.9,es;q=0.8",
	"en-US,en;q=0.9,de;q=0.8",
	"en-US,en;q=0.9,fr;q=0.8",
	"en,en-US;q=0.9",
}

// SecChUaPlatforms are Sec-CH-UA-Platform header values
var SecChUaPlatforms = []string{
	`"Windows"`,
	`"macOS"`,
	`"Linux"`,
}

// init seeds the random number generator
func init() {
	rand.Seed(time.Now().UnixNano())
}

// RandomUserAgent returns a random user agent from the pool
func RandomUserAgent() string {
	return UserAgents[rand.Intn(len(UserAgents))]
}

// RandomAcceptLanguage returns a random Accept-Language header value
func RandomAcceptLanguage() string {
	return AcceptLanguages[rand.Intn(len(AcceptLanguages))]
}

// RandomSecChUaPlatform returns a random Sec-CH-UA-Platform header value
func RandomSecChUaPlatform() string {
	return SecChUaPlatforms[rand.Intn(len(SecChUaPlatforms))]
}

// StealthHeaders returns a map of stealth headers for HTTP requests
func StealthHeaders(userAgent string) map[string]string {
	if userAgent == "" {
		userAgent = RandomUserAgent()
	}

	headers := map[string]string{
		"User-Agent":                userAgent,
		"Accept":                    "text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8",
		"Accept-Language":           RandomAcceptLanguage(),
		"Accept-Encoding":           "gzip, deflate, br",
		"Cache-Control":             "no-cache",
		"Pragma":                    "no-cache",
		"Sec-Fetch-Dest":            "document",
		"Sec-Fetch-Mode":            "navigate",
		"Sec-Fetch-Site":            "none",
		"Sec-Fetch-User":            "?1",
		"Upgrade-Insecure-Requests": "1",
	}

	// Add Chrome-specific headers if using Chrome UA
	if isChrome(userAgent) {
		headers["Sec-CH-UA"] = `"Google Chrome";v="131", "Chromium";v="131", "Not_A Brand";v="24"`
		headers["Sec-CH-UA-Mobile"] = "?0"
		headers["Sec-CH-UA-Platform"] = RandomSecChUaPlatform()
	}

	return headers
}

// isChrome checks if the user agent is Chrome
func isChrome(userAgent string) bool {
	return len(userAgent) > 0 && (contains(userAgent, "Chrome") || contains(userAgent, "Chromium"))
}

// contains is a simple string contains check
func contains(s, substr string) bool {
	for i := 0; i <= len(s)-len(substr); i++ {
		if s[i:i+len(substr)] == substr {
			return true
		}
	}
	return false
}

// RandomDelay returns a random delay between min and max duration
func RandomDelay(min, max time.Duration) time.Duration {
	if min >= max {
		return min
	}
	delta := max - min
	return min + time.Duration(rand.Int63n(int64(delta)))
}
</file>
<file path="internal/fetcher/transport.go">
package fetcher

import (
	"bytes"
	"io"
	"net/http"
)

// StealthTransport is an http.RoundTripper that uses the stealth client
// This allows integration with Colly and other HTTP client libraries
type StealthTransport struct {
	client *Client
}

// NewStealthTransport creates a new StealthTransport
func NewStealthTransport(client *Client) *StealthTransport {
	return &StealthTransport{client: client}
}

// RoundTrip implements http.RoundTripper
func (t *StealthTransport) RoundTrip(req *http.Request) (*http.Response, error) {
	// Extract headers from request
	extraHeaders := make(map[string]string)
	for k, v := range req.Header {
		if len(v) > 0 {
			extraHeaders[k] = v[0]
		}
	}

	// Use the stealth client to make the request
	resp, err := t.client.GetWithHeaders(req.Context(), req.URL.String(), extraHeaders)
	if err != nil {
		return nil, err
	}

	// Convert domain.Response to http.Response
	// IMPORTANT: We must strip Content-Encoding header because we are returning
	// the already decompressed body. If we leave it, the caller (e.g. Colly)
	// will try to decompress it again and fail with "gzip: invalid header".
	resp.Headers.Del("Content-Encoding")

	return &http.Response{
		Status: http.StatusText(resp.StatusCode),

		StatusCode:    resp.StatusCode,
		Proto:         "HTTP/1.1",
		ProtoMajor:    1,
		ProtoMinor:    1,
		Header:        resp.Headers,
		Body:          io.NopCloser(bytes.NewReader(resp.Body)),
		ContentLength: int64(len(resp.Body)),
		Request:       req,
	}, nil
}

// Transport returns the StealthTransport as http.RoundTripper
func (c *Client) Transport() http.RoundTripper {
	return NewStealthTransport(c)
}
</file>
<file path="internal/git/client.go">
package git

import (
	"context"

	"github.com/go-git/go-git/v5"
)

// RealClient implements Client using go-git
type RealClient struct{}

// NewClient creates a new RealClient
func NewClient() *RealClient {
	return &RealClient{}
}

// PlainCloneContext calls git.PlainCloneContext
func (c *RealClient) PlainCloneContext(ctx context.Context, path string, isBare bool, o *git.CloneOptions) (*git.Repository, error) {
	return git.PlainCloneContext(ctx, path, isBare, o)
}
</file>
<file path="internal/git/interface.go">
package git

import (
	"context"

	"github.com/go-git/go-git/v5"
)

// Client defines the interface for Git operations
type Client interface {
	PlainCloneContext(ctx context.Context, path string, isBare bool, o *git.CloneOptions) (*git.Repository, error)
}
</file>
<file path="internal/llm/anthropic.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

const anthropicVersion = "2023-06-01"

type anthropicRequest struct {
	Model     string             `json:"model"`
	MaxTokens int                `json:"max_tokens"`
	Messages  []anthropicMessage `json:"messages"`
	System    string             `json:"system,omitempty"`
}

type anthropicMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type anthropicResponse struct {
	ID      string `json:"id"`
	Type    string `json:"type"`
	Role    string `json:"role"`
	Model   string `json:"model"`
	Content []struct {
		Type string `json:"type"`
		Text string `json:"text"`
	} `json:"content"`
	StopReason string `json:"stop_reason"`
	Usage      struct {
		InputTokens  int `json:"input_tokens"`
		OutputTokens int `json:"output_tokens"`
	} `json:"usage"`
	Error *struct {
		Type    string `json:"type"`
		Message string `json:"message"`
	} `json:"error,omitempty"`
}

type AnthropicProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewAnthropicProvider(cfg ProviderConfig, httpClient *http.Client) (*AnthropicProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	maxTokens := cfg.MaxTokens
	if maxTokens == 0 {
		maxTokens = 4096
	}

	return &AnthropicProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   maxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *AnthropicProvider) Name() string {
	return "anthropic"
}

func (p *AnthropicProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	var systemPrompt string
	messages := make([]anthropicMessage, 0, len(req.Messages))

	for _, msg := range req.Messages {
		if msg.Role == domain.RoleSystem {
			systemPrompt = msg.Content
		} else {
			messages = append(messages, anthropicMessage{
				Role:    string(msg.Role),
				Content: msg.Content,
			})
		}
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	anthropicReq := anthropicRequest{
		Model:     p.model,
		MaxTokens: maxTokens,
		Messages:  messages,
		System:    systemPrompt,
	}

	body, err := json.Marshal(anthropicReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := p.baseURL + "/v1/messages"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-api-key", p.apiKey)
	httpReq.Header.Set("anthropic-version", anthropicVersion)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "anthropic",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	// Check for non-OK status codes first
	if resp.StatusCode != http.StatusOK {
		// Try to parse error response
		var anthropicResp anthropicResponse
		if parseErr := json.Unmarshal(respBody, &anthropicResp); parseErr == nil && anthropicResp.Error != nil {
			// Successfully parsed error response
			// Check if this is a rate limit error based on status code or error type
			if resp.StatusCode == http.StatusTooManyRequests || anthropicResp.Error.Type == "rate_limit_error" {
				return nil, &domain.LLMError{
					Provider:   "anthropic",
					StatusCode: resp.StatusCode,
					Message:    anthropicResp.Error.Message,
					Err:        domain.ErrLLMRateLimited,
				}
			}
			// Check if this is an authentication error
			if resp.StatusCode == http.StatusUnauthorized || anthropicResp.Error.Type == "authentication_error" {
				return nil, &domain.LLMError{
					Provider:   "anthropic",
					StatusCode: resp.StatusCode,
					Message:    anthropicResp.Error.Message,
					Err:        domain.ErrLLMAuthFailed,
				}
			}
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMRequestFailed,
			}
		}
		// Failed to parse error response or no error field, use handleHTTPError
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	var anthropicResp anthropicResponse
	if err := json.Unmarshal(respBody, &anthropicResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	// Check if response contains an error field even with status 200
	if anthropicResp.Error != nil {
		// Check if this is a rate limit error based on error type
		if anthropicResp.Error.Type == "rate_limit_error" {
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMRateLimited,
			}
		}
		// Check if this is an authentication error
		if anthropicResp.Error.Type == "authentication_error" {
			return nil, &domain.LLMError{
				Provider:   "anthropic",
				StatusCode: resp.StatusCode,
				Message:    anthropicResp.Error.Message,
				Err:        domain.ErrLLMAuthFailed,
			}
		}
		return nil, &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: resp.StatusCode,
			Message:    anthropicResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	var sb strings.Builder
	for _, block := range anthropicResp.Content {
		if block.Type == "text" {
			sb.WriteString(block.Text)
		}
	}
	content := sb.String()

	return &domain.LLMResponse{
		Content:      content,
		Model:        anthropicResp.Model,
		FinishReason: anthropicResp.StopReason,
		Usage: domain.LLMUsage{
			PromptTokens:     anthropicResp.Usage.InputTokens,
			CompletionTokens: anthropicResp.Usage.OutputTokens,
			TotalTokens:      anthropicResp.Usage.InputTokens + anthropicResp.Usage.OutputTokens,
		},
	}, nil
}

func (p *AnthropicProvider) Close() error {
	return nil
}

func (p *AnthropicProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "anthropic",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/circuit_breaker.go">
package llm

import (
	"sync"
	"time"
)

// CircuitState represents the state of a circuit breaker
type CircuitState int

const (
	StateClosed CircuitState = iota
	StateOpen
	StateHalfOpen
)

func (s CircuitState) String() string {
	switch s {
	case StateClosed:
		return "closed"
	case StateOpen:
		return "open"
	case StateHalfOpen:
		return "half-open"
	default:
		return "unknown"
	}
}

// CircuitBreaker protects against cascading failures
type CircuitBreaker interface {
	Allow() bool
	RecordSuccess()
	RecordFailure()
	State() CircuitState
}

// CircuitBreakerConfig holds circuit breaker configuration
type CircuitBreakerConfig struct {
	FailureThreshold         int
	SuccessThresholdHalfOpen int
	ResetTimeout             time.Duration
}

// DefaultCircuitBreakerConfig returns sensible defaults
func DefaultCircuitBreakerConfig() CircuitBreakerConfig {
	return CircuitBreakerConfig{
		FailureThreshold:         5,
		SuccessThresholdHalfOpen: 1,
		ResetTimeout:             30 * time.Second,
	}
}

type circuitBreaker struct {
	config          CircuitBreakerConfig
	state           CircuitState
	failures        int
	successes       int
	lastStateChange time.Time
	mu              sync.RWMutex
}

// NewCircuitBreaker creates a new circuit breaker
func NewCircuitBreaker(config CircuitBreakerConfig) CircuitBreaker {
	if config.FailureThreshold <= 0 {
		config.FailureThreshold = 5
	}
	if config.SuccessThresholdHalfOpen <= 0 {
		config.SuccessThresholdHalfOpen = 1
	}
	if config.ResetTimeout <= 0 {
		config.ResetTimeout = 30 * time.Second
	}

	return &circuitBreaker{
		config:          config,
		state:           StateClosed,
		lastStateChange: time.Now(),
	}
}

// Allow checks if a request is allowed to proceed
func (cb *circuitBreaker) Allow() bool {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	switch cb.state {
	case StateClosed:
		return true
	case StateOpen:
		if time.Since(cb.lastStateChange) >= cb.config.ResetTimeout {
			cb.transitionTo(StateHalfOpen)
			return true
		}
		return false
	case StateHalfOpen:
		return true
	default:
		return false
	}
}

// RecordSuccess records a successful operation
func (cb *circuitBreaker) RecordSuccess() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	cb.failures = 0

	switch cb.state {
	case StateHalfOpen:
		cb.successes++
		if cb.successes >= cb.config.SuccessThresholdHalfOpen {
			cb.transitionTo(StateClosed)
		}
	case StateClosed:
		cb.successes++
	}
}

// RecordFailure records a failed operation
func (cb *circuitBreaker) RecordFailure() {
	cb.mu.Lock()
	defer cb.mu.Unlock()

	cb.successes = 0

	switch cb.state {
	case StateClosed:
		cb.failures++
		if cb.failures >= cb.config.FailureThreshold {
			cb.transitionTo(StateOpen)
		}
	case StateHalfOpen:
		cb.transitionTo(StateOpen)
	}
}

// State returns the current state
func (cb *circuitBreaker) State() CircuitState {
	cb.mu.RLock()
	defer cb.mu.RUnlock()
	return cb.state
}

func (cb *circuitBreaker) transitionTo(newState CircuitState) {
	cb.state = newState
	cb.lastStateChange = time.Now()
	cb.failures = 0
	cb.successes = 0
}

// NoOpCircuitBreaker always allows requests
type NoOpCircuitBreaker struct{}

// Allow always returns true
func (n *NoOpCircuitBreaker) Allow() bool {
	return true
}

// RecordSuccess does nothing
func (n *NoOpCircuitBreaker) RecordSuccess() {}

// RecordFailure does nothing
func (n *NoOpCircuitBreaker) RecordFailure() {}

// State always returns StateClosed
func (n *NoOpCircuitBreaker) State() CircuitState {
	return StateClosed
}
</file>
<file path="internal/llm/google.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type googleRequest struct {
	Contents          []googleContent  `json:"contents"`
	SystemInstruction *googleContent   `json:"systemInstruction,omitempty"`
	GenerationConfig  *googleGenConfig `json:"generationConfig,omitempty"`
}

type googleContent struct {
	Role  string       `json:"role,omitempty"`
	Parts []googlePart `json:"parts"`
}

type googlePart struct {
	Text string `json:"text"`
}

type googleGenConfig struct {
	MaxOutputTokens int     `json:"maxOutputTokens,omitempty"`
	Temperature     float64 `json:"temperature,omitempty"`
}

type googleResponse struct {
	Candidates []struct {
		Content struct {
			Role  string `json:"role"`
			Parts []struct {
				Text string `json:"text"`
			} `json:"parts"`
		} `json:"content"`
		FinishReason string `json:"finishReason"`
	} `json:"candidates"`
	UsageMetadata struct {
		PromptTokenCount     int `json:"promptTokenCount"`
		CandidatesTokenCount int `json:"candidatesTokenCount"`
		TotalTokenCount      int `json:"totalTokenCount"`
	} `json:"usageMetadata"`
	Error *struct {
		Code    int    `json:"code"`
		Message string `json:"message"`
		Status  string `json:"status"`
	} `json:"error,omitempty"`
}

type GoogleProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewGoogleProvider(cfg ProviderConfig, httpClient *http.Client) (*GoogleProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	return &GoogleProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   cfg.MaxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *GoogleProvider) Name() string {
	return "google"
}

func (p *GoogleProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	var systemInstruction *googleContent
	contents := make([]googleContent, 0, len(req.Messages))

	for _, msg := range req.Messages {
		switch msg.Role {
		case domain.RoleSystem:
			systemInstruction = &googleContent{
				Parts: []googlePart{{Text: msg.Content}},
			}
		case domain.RoleUser:
			contents = append(contents, googleContent{
				Role:  "user",
				Parts: []googlePart{{Text: msg.Content}},
			})
		case domain.RoleAssistant:
			contents = append(contents, googleContent{
				Role:  "model",
				Parts: []googlePart{{Text: msg.Content}},
			})
		}
	}

	googleReq := googleRequest{
		Contents:          contents,
		SystemInstruction: systemInstruction,
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	temp := p.temperature
	if req.Temperature != nil {
		temp = *req.Temperature
	}

	if maxTokens > 0 || temp > 0 {
		googleReq.GenerationConfig = &googleGenConfig{
			MaxOutputTokens: maxTokens,
			Temperature:     temp,
		}
	}

	body, err := json.Marshal(googleReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := fmt.Sprintf("%s/v1beta/models/%s:generateContent", p.baseURL, p.model)
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("x-goog-api-key", p.apiKey)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "google",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	if resp.StatusCode != http.StatusOK {
		var googleResp googleResponse
		if json.Unmarshal(respBody, &googleResp) == nil && googleResp.Error != nil {
			if resp.StatusCode == http.StatusTooManyRequests || googleResp.Error.Status == "RESOURCE_EXHAUSTED" {
				return nil, &domain.LLMError{
					Provider:   "google",
					StatusCode: googleResp.Error.Code,
					Message:    googleResp.Error.Message,
					Err:        domain.ErrLLMRateLimited,
				}
			}
			return nil, &domain.LLMError{
				Provider:   "google",
				StatusCode: googleResp.Error.Code,
				Message:    googleResp.Error.Message,
				Err:        domain.ErrLLMRequestFailed,
			}
		}
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	var googleResp googleResponse
	if err := json.Unmarshal(respBody, &googleResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if googleResp.Error != nil {
		return nil, &domain.LLMError{
			Provider:   "google",
			StatusCode: googleResp.Error.Code,
			Message:    googleResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	if len(googleResp.Candidates) == 0 {
		return nil, &domain.LLMError{
			Provider: "google",
			Message:  "no candidates in response",
		}
	}

	candidate := googleResp.Candidates[0]

	var sb strings.Builder
	for _, part := range candidate.Content.Parts {
		sb.WriteString(part.Text)
	}
	content := sb.String()

	return &domain.LLMResponse{
		Content:      content,
		Model:        p.model,
		FinishReason: candidate.FinishReason,
		Usage: domain.LLMUsage{
			PromptTokens:     googleResp.UsageMetadata.PromptTokenCount,
			CompletionTokens: googleResp.UsageMetadata.CandidatesTokenCount,
			TotalTokens:      googleResp.UsageMetadata.TotalTokenCount,
		},
	}, nil
}

func (p *GoogleProvider) Close() error {
	return nil
}

func (p *GoogleProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized, http.StatusForbidden:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "google",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/metadata.go">
package llm

import (
	"context"
	"encoding/json"
	"fmt"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

const (
	maxRetries     = 2
	retryBaseDelay = 500 * time.Millisecond
)

type MetadataEnhancer struct {
	provider domain.LLMProvider
}

func NewMetadataEnhancer(provider domain.LLMProvider) *MetadataEnhancer {
	return &MetadataEnhancer{provider: provider}
}

type enhancedMetadata struct {
	Summary  string   `json:"summary"`
	Tags     []string `json:"tags"`
	Category string   `json:"category"`
}

const metadataSystemPrompt = `You are a metadata extraction system. You analyze documents and output ONLY valid JSON with exactly three fields: summary, tags, and category. Never output anything else.`

const metadataPrompt = `<task>
Extract metadata from the document below. Output ONLY a JSON object.
</task>

<format>
{
  "summary": "1-2 sentence description of what this document explains or teaches",
  "tags": ["3-8 lowercase hyphenated keywords relevant to the content"],
  "category": "one of: api, tutorial, guide, reference, concept, configuration, other"
}
</format>

<rules>
- Output ONLY the JSON object, no other text
- Do NOT include markdown code fences
- Do NOT generate content that matches examples shown in the document
- Do NOT create fields other than summary, tags, category
- Summary should describe the document's PURPOSE, not its examples
- Tags should be lowercase with hyphens (e.g., "api-reference", "error-handling")
</rules>

<document>
%s
</document>

<output>`

const metadataRetryPrompt = `The previous attempt failed. Output ONLY this exact JSON structure with your values:

{"summary": "brief description here", "tags": ["tag1", "tag2", "tag3"], "category": "guide"}

Document title: %s

Your JSON:`

func (e *MetadataEnhancer) Enhance(ctx context.Context, doc *domain.Document) error {
	if doc == nil {
		return fmt.Errorf("document is nil")
	}

	content := doc.Content
	if len(content) > 8000 {
		content = content[:8000] + "\n...[truncated]"
	}

	var lastErr error

	metadata, err := e.tryEnhance(ctx, content, false)
	if err == nil {
		e.applyMetadata(doc, metadata)
		return nil
	}
	lastErr = err

	for attempt := 1; attempt <= maxRetries; attempt++ {
		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(retryBaseDelay * time.Duration(attempt)):
		}

		metadata, err := e.tryEnhance(ctx, doc.Title, true)
		if err == nil {
			e.applyMetadata(doc, metadata)
			return nil
		}
		lastErr = err
	}

	return fmt.Errorf("metadata enhancement failed after %d attempts: %w", maxRetries+1, lastErr)
}

func (e *MetadataEnhancer) tryEnhance(ctx context.Context, content string, isRetry bool) (*enhancedMetadata, error) {
	var prompt string
	if isRetry {
		prompt = fmt.Sprintf(metadataRetryPrompt, content)
	} else {
		prompt = fmt.Sprintf(metadataPrompt, content)
	}

	req := &domain.LLMRequest{
		Messages: []domain.LLMMessage{
			{Role: domain.RoleSystem, Content: metadataSystemPrompt},
			{Role: domain.RoleUser, Content: prompt},
		},
		MaxTokens: 1024, // reduced from 32000 - metadata output is small
	}

	resp, err := e.provider.Complete(ctx, req)
	if err != nil {
		return nil, fmt.Errorf("LLM completion failed: %w", err)
	}

	jsonStr := extractJSON(resp.Content)
	if jsonStr == "" {
		return nil, fmt.Errorf("no valid JSON structure found in response: %s", truncateForError(resp.Content))
	}

	var metadata enhancedMetadata
	if err := json.Unmarshal([]byte(jsonStr), &metadata); err != nil {
		return nil, fmt.Errorf("JSON unmarshal failed: %w (extracted: %s)", err, truncateForError(jsonStr))
	}

	return &metadata, nil
}

func (e *MetadataEnhancer) applyMetadata(doc *domain.Document, metadata *enhancedMetadata) {
	doc.Summary = metadata.Summary
	doc.Tags = metadata.Tags
	doc.Category = metadata.Category
}

func extractJSON(text string) string {
	text = strings.TrimSpace(text)

	if candidate := tryExtractAndValidate(text); candidate != "" {
		return candidate
	}

	stripped := stripMarkdownCodeBlocks(text)
	if candidate := tryExtractAndValidate(stripped); candidate != "" {
		return candidate
	}

	if jsonObj := findJSONObjectByBraceMatching(stripped); jsonObj != "" {
		if candidate := tryExtractAndValidate(jsonObj); candidate != "" {
			return candidate
		}
	}

	if jsonObj := findJSONObjectByBraceMatching(text); jsonObj != "" {
		if candidate := tryExtractAndValidate(jsonObj); candidate != "" {
			return candidate
		}
	}

	return ""
}

func tryExtractAndValidate(text string) string {
	if !strings.HasPrefix(text, "{") {
		return ""
	}

	if !json.Valid([]byte(text)) {
		return ""
	}

	var check map[string]interface{}
	if err := json.Unmarshal([]byte(text), &check); err != nil {
		return ""
	}

	if _, ok := check["summary"]; !ok {
		return ""
	}
	if _, ok := check["tags"]; !ok {
		return ""
	}
	if _, ok := check["category"]; !ok {
		return ""
	}

	if _, ok := check["summary"].(string); !ok {
		return ""
	}
	if _, ok := check["tags"].([]interface{}); !ok {
		return ""
	}
	if _, ok := check["category"].(string); !ok {
		return ""
	}

	return text
}

// codeBlockRegex: (?s)^\x60{3}\s*(?:json|JSON)?\s*\n?(.*?)\n?\x60{3}$
// Matches markdown fences with optional json/JSON (and optional space), captures inner content
var codeBlockRegex = regexp.MustCompile(`(?s)^\x60\x60\x60\s*(?:json|JSON)?\s*\n?(.*?)\n?\x60\x60\x60$`)

func stripMarkdownCodeBlocks(text string) string {
	text = strings.TrimSpace(text)

	if matches := codeBlockRegex.FindStringSubmatch(text); len(matches) == 2 {
		return strings.TrimSpace(matches[1])
	}

	for _, prefix := range []string{"```json", "```JSON", "``` json", "```"} {
		text = strings.TrimPrefix(text, prefix)
	}
	text = strings.TrimSuffix(text, "```")
	text = strings.TrimSpace(text)

	if strings.HasPrefix(text, "\n") {
		text = strings.TrimPrefix(text, "\n")
	}

	return strings.TrimSpace(text)
}

func findJSONObjectByBraceMatching(text string) string {
	start := strings.Index(text, "{")
	if start == -1 {
		return ""
	}

	depth := 0
	inString := false
	escaped := false

	for i := start; i < len(text); i++ {
		c := text[i]

		if escaped {
			escaped = false
			continue
		}

		if c == '\\' && inString {
			escaped = true
			continue
		}

		if c == '"' {
			inString = !inString
			continue
		}

		if inString {
			continue
		}

		switch c {
		case '{':
			depth++
		case '}':
			depth--
			if depth == 0 {
				candidate := text[start : i+1]
				if json.Valid([]byte(candidate)) {
					return candidate
				}
			}
		}
	}

	return ""
}

func truncateForError(s string) string {
	const maxLen = 200
	if len(s) > maxLen {
		return s[:maxLen] + "..."
	}
	return s
}

func (e *MetadataEnhancer) EnhanceAll(ctx context.Context, docs []*domain.Document) error {
	for _, doc := range docs {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			if err := e.Enhance(ctx, doc); err != nil {
				return err
			}
		}
	}
	return nil
}
</file>
<file path="internal/llm/openai.go">
package llm

import (
	"bytes"
	"context"
	"encoding/json"
	"fmt"
	"io"
	"net/http"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type openAIRequest struct {
	Model       string          `json:"model"`
	Messages    []openAIMessage `json:"messages"`
	MaxTokens   int             `json:"max_tokens,omitempty"`
	Temperature float64         `json:"temperature,omitempty"`
}

type openAIMessage struct {
	Role    string `json:"role"`
	Content string `json:"content"`
}

type openAIResponse struct {
	ID      string `json:"id"`
	Model   string `json:"model"`
	Choices []struct {
		Index   int `json:"index"`
		Message struct {
			Role    string `json:"role"`
			Content string `json:"content"`
		} `json:"message"`
		FinishReason string `json:"finish_reason"`
	} `json:"choices"`
	Usage struct {
		PromptTokens     int `json:"prompt_tokens"`
		CompletionTokens int `json:"completion_tokens"`
		TotalTokens      int `json:"total_tokens"`
	} `json:"usage"`
	Error *struct {
		Message string `json:"message"`
		Type    string `json:"type"`
		Code    string `json:"code"`
	} `json:"error,omitempty"`
}

type OpenAIProvider struct {
	httpClient  *http.Client
	apiKey      string
	baseURL     string
	model       string
	maxTokens   int
	temperature float64
}

func NewOpenAIProvider(cfg ProviderConfig, httpClient *http.Client) (*OpenAIProvider, error) {
	baseURL := strings.TrimSuffix(cfg.BaseURL, "/")

	return &OpenAIProvider{
		httpClient:  httpClient,
		apiKey:      cfg.APIKey,
		baseURL:     baseURL,
		model:       cfg.Model,
		maxTokens:   cfg.MaxTokens,
		temperature: cfg.Temperature,
	}, nil
}

func (p *OpenAIProvider) Name() string {
	return "openai"
}

func (p *OpenAIProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	messages := make([]openAIMessage, len(req.Messages))
	for i, msg := range req.Messages {
		messages[i] = openAIMessage{
			Role:    string(msg.Role),
			Content: msg.Content,
		}
	}

	maxTokens := req.MaxTokens
	if maxTokens == 0 {
		maxTokens = p.maxTokens
	}

	temp := p.temperature
	if req.Temperature != nil {
		temp = *req.Temperature
	}

	openAIReq := openAIRequest{
		Model:       p.model,
		Messages:    messages,
		MaxTokens:   maxTokens,
		Temperature: temp,
	}

	body, err := json.Marshal(openAIReq)
	if err != nil {
		return nil, fmt.Errorf("failed to marshal request: %w", err)
	}

	url := p.baseURL + "/chat/completions"
	httpReq, err := http.NewRequestWithContext(ctx, "POST", url, bytes.NewReader(body))
	if err != nil {
		return nil, fmt.Errorf("failed to create request: %w", err)
	}

	httpReq.Header.Set("Content-Type", "application/json")
	httpReq.Header.Set("Authorization", "Bearer "+p.apiKey)

	resp, err := p.httpClient.Do(httpReq)
	if err != nil {
		return nil, &domain.LLMError{
			Provider: "openai",
			Message:  fmt.Sprintf("request failed: %v", err),
			Err:      err,
		}
	}
	defer resp.Body.Close()

	respBody, err := io.ReadAll(resp.Body)
	if err != nil {
		return nil, fmt.Errorf("failed to read response: %w", err)
	}

	var openAIResp openAIResponse
	if err := json.Unmarshal(respBody, &openAIResp); err != nil {
		return nil, fmt.Errorf("failed to parse response: %w", err)
	}

	if openAIResp.Error != nil {
		// Check if this is a rate limit error based on status code
		if resp.StatusCode == http.StatusTooManyRequests {
			return nil, &domain.LLMError{
				Provider:   "openai",
				StatusCode: resp.StatusCode,
				Message:    openAIResp.Error.Message,
				Err:        domain.ErrLLMRateLimited,
			}
		}
		// Check if this is an authentication error
		if resp.StatusCode == http.StatusUnauthorized {
			return nil, &domain.LLMError{
				Provider:   "openai",
				StatusCode: resp.StatusCode,
				Message:    openAIResp.Error.Message,
				Err:        domain.ErrLLMAuthFailed,
			}
		}
		return nil, &domain.LLMError{
			Provider:   "openai",
			StatusCode: resp.StatusCode,
			Message:    openAIResp.Error.Message,
			Err:        domain.ErrLLMRequestFailed,
		}
	}

	if resp.StatusCode != http.StatusOK {
		return nil, p.handleHTTPError(resp.StatusCode, respBody)
	}

	if len(openAIResp.Choices) == 0 {
		return nil, &domain.LLMError{
			Provider: "openai",
			Message:  "no choices in response",
		}
	}

	choice := openAIResp.Choices[0]

	return &domain.LLMResponse{
		Content:      choice.Message.Content,
		Model:        openAIResp.Model,
		FinishReason: choice.FinishReason,
		Usage: domain.LLMUsage{
			PromptTokens:     openAIResp.Usage.PromptTokens,
			CompletionTokens: openAIResp.Usage.CompletionTokens,
			TotalTokens:      openAIResp.Usage.TotalTokens,
		},
	}, nil
}

func (p *OpenAIProvider) Close() error {
	return nil
}

func (p *OpenAIProvider) handleHTTPError(statusCode int, body []byte) error {
	switch statusCode {
	case http.StatusUnauthorized:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    "authentication failed",
			Err:        domain.ErrLLMAuthFailed,
		}
	case http.StatusTooManyRequests:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    "rate limit exceeded",
			Err:        domain.ErrLLMRateLimited,
		}
	default:
		return &domain.LLMError{
			Provider:   "openai",
			StatusCode: statusCode,
			Message:    string(body),
		}
	}
}
</file>
<file path="internal/llm/provider.go">
package llm

import (
	"fmt"
	"net/http"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Default base URLs for each provider
const (
	DefaultOpenAIBaseURL    = "https://api.openai.com/v1"
	DefaultAnthropicBaseURL = "https://api.anthropic.com/v1"
	DefaultGoogleBaseURL    = "https://generativelanguage.googleapis.com"
)

type ProviderConfig struct {
	Provider    string
	APIKey      string
	BaseURL     string
	Model       string
	MaxTokens   int
	Temperature float64
	Timeout     time.Duration
	MaxRetries  int
	HTTPClient  *http.Client
}

// DefaultBaseURL returns the default base URL for a given provider.
// Returns empty string if provider is unknown.
func DefaultBaseURL(provider string) string {
	switch provider {
	case "openai":
		return DefaultOpenAIBaseURL
	case "anthropic":
		return DefaultAnthropicBaseURL
	case "google":
		return DefaultGoogleBaseURL
	default:
		return ""
	}
}

func NewProviderFromConfig(cfg *config.LLMConfig) (domain.LLMProvider, error) {
	if cfg.Provider == "" {
		return nil, domain.ErrLLMNotConfigured
	}
	if cfg.APIKey == "" {
		return nil, domain.ErrLLMMissingAPIKey
	}
	if cfg.Model == "" {
		return nil, domain.ErrLLMMissingModel
	}

	baseURL := cfg.BaseURL
	if baseURL == "" {
		baseURL = DefaultBaseURL(cfg.Provider)
		if baseURL == "" {
			return nil, domain.ErrLLMMissingBaseURL
		}
	}

	pcfg := ProviderConfig{
		Provider:    cfg.Provider,
		APIKey:      cfg.APIKey,
		BaseURL:     baseURL,
		Model:       cfg.Model,
		MaxTokens:   cfg.MaxTokens,
		Temperature: cfg.Temperature,
		Timeout:     cfg.Timeout,
		MaxRetries:  cfg.MaxRetries,
	}

	return NewProvider(pcfg)
}

func NewProvider(cfg ProviderConfig) (domain.LLMProvider, error) {
	timeout := cfg.Timeout
	if timeout == 0 {
		timeout = 60 * time.Second
	}

	httpClient := cfg.HTTPClient
	if httpClient == nil {
		httpClient = &http.Client{Timeout: timeout}
	}

	switch cfg.Provider {
	case "openai":
		return NewOpenAIProvider(cfg, httpClient)
	case "anthropic":
		return NewAnthropicProvider(cfg, httpClient)
	case "google":
		return NewGoogleProvider(cfg, httpClient)
	default:
		return nil, fmt.Errorf("%w: %s", domain.ErrLLMInvalidProvider, cfg.Provider)
	}
}
</file>
<file path="internal/llm/provider_wrapper.go">
package llm

import (
	"context"
	"fmt"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// RateLimitedProviderConfig holds configuration for the wrapper
type RateLimitedProviderConfig struct {
	RequestsPerMinute        int
	BurstSize                int
	MaxRetries               int
	InitialDelay             time.Duration
	MaxDelay                 time.Duration
	Multiplier               float64
	JitterFactor             float64
	CircuitBreakerEnabled    bool
	FailureThreshold         int
	SuccessThresholdHalfOpen int
	ResetTimeout             time.Duration
}

// DefaultRateLimitedProviderConfig returns sensible defaults
func DefaultRateLimitedProviderConfig() RateLimitedProviderConfig {
	return RateLimitedProviderConfig{
		RequestsPerMinute:        60,
		BurstSize:                10,
		MaxRetries:               3,
		InitialDelay:             time.Second,
		MaxDelay:                 60 * time.Second,
		Multiplier:               2.0,
		JitterFactor:             0.1,
		CircuitBreakerEnabled:    true,
		FailureThreshold:         5,
		SuccessThresholdHalfOpen: 1,
		ResetTimeout:             30 * time.Second,
	}
}

// RateLimitedProvider wraps an LLMProvider with rate limiting, retry, and circuit breaker
type RateLimitedProvider struct {
	provider       domain.LLMProvider
	rateLimiter    RateLimiter
	retrier        *Retrier
	circuitBreaker CircuitBreaker
	logger         *utils.Logger
}

// NewRateLimitedProvider creates a new rate-limited provider wrapper
func NewRateLimitedProvider(
	provider domain.LLMProvider,
	config RateLimitedProviderConfig,
	logger *utils.Logger,
) *RateLimitedProvider {
	var rateLimiter RateLimiter
	if config.RequestsPerMinute > 0 {
		rateLimiter = NewTokenBucket(config.RequestsPerMinute, config.BurstSize)
	} else {
		rateLimiter = &NoOpRateLimiter{}
	}

	retrier := NewRetrier(RetryConfig{
		MaxRetries:      config.MaxRetries,
		InitialInterval: config.InitialDelay,
		MaxInterval:     config.MaxDelay,
		Multiplier:      config.Multiplier,
		JitterFactor:    config.JitterFactor,
	}, logger)

	var circuitBreaker CircuitBreaker
	if config.CircuitBreakerEnabled {
		circuitBreaker = NewCircuitBreaker(CircuitBreakerConfig{
			FailureThreshold:         config.FailureThreshold,
			SuccessThresholdHalfOpen: config.SuccessThresholdHalfOpen,
			ResetTimeout:             config.ResetTimeout,
		})
	} else {
		circuitBreaker = &NoOpCircuitBreaker{}
	}

	return &RateLimitedProvider{
		provider:       provider,
		rateLimiter:    rateLimiter,
		retrier:        retrier,
		circuitBreaker: circuitBreaker,
		logger:         logger,
	}
}

// Name returns the wrapped provider's name
func (p *RateLimitedProvider) Name() string {
	return p.provider.Name()
}

// Complete executes the request with rate limiting, retry, and circuit breaker
func (p *RateLimitedProvider) Complete(ctx context.Context, req *domain.LLMRequest) (*domain.LLMResponse, error) {
	if p.logger != nil {
		p.logger.Debug().
			Float64("tokens_available", p.rateLimiter.Available()).
			Msg("Waiting for rate limit token")
	}

	if err := p.rateLimiter.Wait(ctx); err != nil {
		return nil, fmt.Errorf("rate limit wait cancelled: %w", err)
	}

	if !p.circuitBreaker.Allow() {
		if p.logger != nil {
			p.logger.Warn().
				Str("state", p.circuitBreaker.State().String()).
				Msg("Circuit breaker is open, rejecting request")
		}
		return nil, domain.ErrLLMCircuitOpen
	}

	var response *domain.LLMResponse
	err := p.retrier.Execute(ctx, func() error {
		var err error
		response, err = p.provider.Complete(ctx, req)
		return err
	})

	if err != nil {
		p.circuitBreaker.RecordFailure()
		if p.logger != nil {
			p.logger.Error().
				Err(err).
				Str("circuit_state", p.circuitBreaker.State().String()).
				Msg("LLM request failed")
		}
		return nil, err
	}

	p.circuitBreaker.RecordSuccess()
	return response, nil
}

// Close closes the wrapped provider
func (p *RateLimitedProvider) Close() error {
	return p.provider.Close()
}
</file>
<file path="internal/llm/ratelimit.go">
package llm

import (
	"context"
	"sync"
	"time"
)

// RateLimiter controls the rate of operations
type RateLimiter interface {
	Wait(ctx context.Context) error
	TryAcquire() bool
	Available() float64
}

// TokenBucket implements a token bucket rate limiter
type TokenBucket struct {
	tokens     float64
	capacity   float64
	refillRate float64
	lastRefill time.Time
	mu         sync.Mutex
}

// NewTokenBucket creates a new token bucket rate limiter
func NewTokenBucket(requestsPerMinute int, burstSize int) *TokenBucket {
	if requestsPerMinute <= 0 {
		requestsPerMinute = 60
	}
	if burstSize <= 0 {
		burstSize = 1
	}

	return &TokenBucket{
		tokens:     float64(burstSize),
		capacity:   float64(burstSize),
		refillRate: float64(requestsPerMinute) / 60.0,
		lastRefill: time.Now(),
	}
}

func (tb *TokenBucket) refill() {
	now := time.Now()
	elapsed := now.Sub(tb.lastRefill).Seconds()
	tb.tokens += elapsed * tb.refillRate
	if tb.tokens > tb.capacity {
		tb.tokens = tb.capacity
	}
	tb.lastRefill = now
}

// Wait blocks until a token is available or context is cancelled
func (tb *TokenBucket) Wait(ctx context.Context) error {
	for {
		tb.mu.Lock()
		tb.refill()
		if tb.tokens >= 1.0 {
			tb.tokens--
			tb.mu.Unlock()
			return nil
		}

		tokensNeeded := 1.0 - tb.tokens
		waitDuration := time.Duration(tokensNeeded / tb.refillRate * float64(time.Second))
		tb.mu.Unlock()

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(waitDuration):
			continue
		}
	}
}

// TryAcquire attempts to acquire a token without blocking
func (tb *TokenBucket) TryAcquire() bool {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	if tb.tokens >= 1.0 {
		tb.tokens--
		return true
	}
	return false
}

// Available returns the current number of available tokens
func (tb *TokenBucket) Available() float64 {
	tb.mu.Lock()
	defer tb.mu.Unlock()

	tb.refill()
	return tb.tokens
}

// NoOpRateLimiter is a rate limiter that doesn't limit
type NoOpRateLimiter struct{}

// Wait always returns immediately
func (n *NoOpRateLimiter) Wait(_ context.Context) error {
	return nil
}

// TryAcquire always returns true
func (n *NoOpRateLimiter) TryAcquire() bool {
	return true
}

// Available always returns 1
func (n *NoOpRateLimiter) Available() float64 {
	return 1.0
}
</file>
<file path="internal/llm/retry.go">
package llm

import (
	"context"
	"errors"
	"fmt"
	"math"
	"math/rand"
	"net/http"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// RetryConfig holds retry configuration
type RetryConfig struct {
	MaxRetries      int
	InitialInterval time.Duration
	MaxInterval     time.Duration
	Multiplier      float64
	JitterFactor    float64
}

// DefaultRetryConfig returns sensible retry defaults
func DefaultRetryConfig() RetryConfig {
	return RetryConfig{
		MaxRetries:      3,
		InitialInterval: 1 * time.Second,
		MaxInterval:     60 * time.Second,
		Multiplier:      2.0,
		JitterFactor:    0.1,
	}
}

// Retrier executes operations with retry logic
type Retrier struct {
	config RetryConfig
	logger *utils.Logger
}

// NewRetrier creates a new Retrier
func NewRetrier(config RetryConfig, logger *utils.Logger) *Retrier {
	if config.MaxRetries < 0 {
		config.MaxRetries = 0
	}
	if config.InitialInterval <= 0 {
		config.InitialInterval = time.Second
	}
	if config.MaxInterval <= 0 {
		config.MaxInterval = 60 * time.Second
	}
	if config.Multiplier <= 0 {
		config.Multiplier = 2.0
	}
	if config.JitterFactor < 0 {
		config.JitterFactor = 0
	}

	return &Retrier{
		config: config,
		logger: logger,
	}
}

// Execute runs the operation with retry logic
func (r *Retrier) Execute(ctx context.Context, operation func() error) error {
	var lastErr error

	for attempt := 0; attempt <= r.config.MaxRetries; attempt++ {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		lastErr = operation()
		if lastErr == nil {
			if attempt > 0 && r.logger != nil {
				r.logger.Info().Int("attempts", attempt+1).Msg("LLM request succeeded after retries")
			}
			return nil
		}

		if !IsRetryableError(lastErr) {
			return lastErr
		}

		if attempt >= r.config.MaxRetries {
			break
		}

		backoff := r.calculateBackoff(attempt)
		if r.logger != nil {
			r.logger.Warn().
				Int("attempt", attempt+1).
				Int("max_retries", r.config.MaxRetries).
				Dur("backoff", backoff).
				Err(lastErr).
				Msg("Retrying LLM request after error")
		}

		select {
		case <-ctx.Done():
			return ctx.Err()
		case <-time.After(backoff):
			continue
		}
	}

	return fmt.Errorf("%w: %v", domain.ErrLLMMaxRetriesExceeded, lastErr)
}

func (r *Retrier) calculateBackoff(attempt int) time.Duration {
	backoff := float64(r.config.InitialInterval) * math.Pow(r.config.Multiplier, float64(attempt))

	if r.config.JitterFactor > 0 {
		jitter := backoff * r.config.JitterFactor * (rand.Float64()*2 - 1)
		backoff += jitter
	}

	if backoff > float64(r.config.MaxInterval) {
		backoff = float64(r.config.MaxInterval)
	}

	if backoff < 0 {
		backoff = 0
	}

	return time.Duration(backoff)
}

// IsRetryableError checks if an error should be retried
func IsRetryableError(err error) bool {
	if err == nil {
		return false
	}

	if errors.Is(err, context.Canceled) || errors.Is(err, context.DeadlineExceeded) {
		return false
	}

	if errors.Is(err, domain.ErrLLMRateLimited) {
		return true
	}

	var llmErr *domain.LLMError
	if errors.As(err, &llmErr) {
		return ShouldRetryStatusCode(llmErr.StatusCode)
	}

	var fetchErr *domain.FetchError
	if errors.As(err, &fetchErr) {
		return ShouldRetryStatusCode(fetchErr.StatusCode)
	}

	return false
}

// ShouldRetryStatusCode checks if an HTTP status code is retryable
func ShouldRetryStatusCode(statusCode int) bool {
	switch statusCode {
	case http.StatusTooManyRequests,
		http.StatusInternalServerError,
		http.StatusBadGateway,
		http.StatusServiceUnavailable,
		http.StatusGatewayTimeout:
		return true
	default:
		return false
	}
}

// ShouldRetry is kept for backward compatibility
func ShouldRetry(statusCode int) bool {
	return ShouldRetryStatusCode(statusCode)
}

// CalculateBackoff is kept for backward compatibility
func CalculateBackoff(attempt int, cfg RetryConfig) time.Duration {
	backoff := float64(cfg.InitialInterval) * math.Pow(cfg.Multiplier, float64(attempt))

	jitter := backoff * 0.1 * (rand.Float64()*2 - 1)
	backoff += jitter

	if backoff > float64(cfg.MaxInterval) {
		backoff = float64(cfg.MaxInterval)
	}

	return time.Duration(backoff)
}
</file>
<file path="internal/manifest/doc.go">
// Package manifest provides types and utilities for loading and validating
// RepoDocs manifest files. A manifest defines multiple documentation sources
// with per-source configurations, enabling batch processing.
//
// # Manifest Format
//
// Manifests can be written in YAML or JSON format:
//
//	sources:
//	  - url: https://docs.example.com
//	    strategy: crawler
//	    content_selector: "article.main"
//	  - url: https://github.com/org/repo
//	    strategy: git
//	    include: ["docs/**/*.md"]
//	options:
//	  continue_on_error: true
//	  output: ./knowledge-base
//
// # Usage
//
// Load a manifest file:
//
//	loader := manifest.NewLoader()
//	cfg, err := loader.Load("sources.yaml")
//	if err != nil {
//	    log.Fatal(err)
//	}
//
//	for _, source := range cfg.Sources {
//	    // Process each source
//	}
//
// # Error Handling
//
// The package defines sentinel errors for common failure cases:
//   - ErrNoSources: manifest has no sources defined
//   - ErrEmptyURL: source is missing required URL field
//   - ErrInvalidFormat: file is not valid YAML/JSON
//   - ErrFileNotFound: manifest file does not exist
//   - ErrUnsupportedExt: unsupported file extension
package manifest
</file>
<file path="internal/manifest/errors.go">
package manifest

import "errors"

// Sentinel errors for the manifest package
var (
	// ErrNoSources indicates the manifest has no sources defined
	ErrNoSources = errors.New("manifest must contain at least one source")

	// ErrEmptyURL indicates a source is missing the required URL field
	ErrEmptyURL = errors.New("source URL cannot be empty")

	// ErrInvalidFormat indicates the manifest file is not valid YAML or JSON
	ErrInvalidFormat = errors.New("manifest must be valid YAML or JSON")

	// ErrFileNotFound indicates the manifest file does not exist
	ErrFileNotFound = errors.New("manifest file not found")

	// ErrUnsupportedExt indicates an unsupported file extension
	ErrUnsupportedExt = errors.New("unsupported file extension (use .yaml, .yml, or .json)")
)
</file>
<file path="internal/manifest/loader.go">
package manifest

import (
	"encoding/json"
	"fmt"
	"os"
	"path/filepath"
	"strings"

	"gopkg.in/yaml.v3"
)

// Loader loads and validates manifest files
type Loader struct{}

// NewLoader creates a new manifest loader
func NewLoader() *Loader {
	return &Loader{}
}

// Load reads and parses a manifest file from the given path
func (l *Loader) Load(path string) (*Config, error) {
	if _, err := os.Stat(path); os.IsNotExist(err) {
		return nil, fmt.Errorf("%w: %s", ErrFileNotFound, path)
	}

	data, err := os.ReadFile(path)
	if err != nil {
		return nil, fmt.Errorf("failed to read manifest file: %w", err)
	}

	return l.LoadFromBytes(data, filepath.Ext(path))
}

// LoadFromBytes parses manifest configuration from raw bytes
func (l *Loader) LoadFromBytes(data []byte, ext string) (*Config, error) {
	ext = strings.ToLower(ext)

	var cfg Config
	switch ext {
	case ".yaml", ".yml":
		if err := yaml.Unmarshal(data, &cfg); err != nil {
			return nil, fmt.Errorf("%w: %v", ErrInvalidFormat, err)
		}
	case ".json":
		if err := json.Unmarshal(data, &cfg); err != nil {
			return nil, fmt.Errorf("%w: %v", ErrInvalidFormat, err)
		}
	default:
		return nil, fmt.Errorf("%w: %s", ErrUnsupportedExt, ext)
	}

	l.applyDefaults(&cfg)

	if err := cfg.Validate(); err != nil {
		return nil, err
	}

	return &cfg, nil
}

func (l *Loader) applyDefaults(cfg *Config) {
	defaults := DefaultOptions()

	if cfg.Options.Output == "" {
		cfg.Options.Output = defaults.Output
	}
	if cfg.Options.Concurrency == 0 {
		cfg.Options.Concurrency = defaults.Concurrency
	}
	if cfg.Options.CacheTTL == 0 {
		cfg.Options.CacheTTL = defaults.CacheTTL
	}
}
</file>
<file path="internal/manifest/types.go">
package manifest

import (
	"fmt"
	"time"
)

// Config represents the complete manifest configuration
type Config struct {
	Sources []Source `yaml:"sources" json:"sources"`
	Options Options  `yaml:"options" json:"options"`
}

// Source represents an individual documentation source
type Source struct {
	URL             string   `yaml:"url" json:"url"`
	Strategy        string   `yaml:"strategy,omitempty" json:"strategy,omitempty"`
	ContentSelector string   `yaml:"content_selector,omitempty" json:"content_selector,omitempty"`
	ExcludeSelector string   `yaml:"exclude_selector,omitempty" json:"exclude_selector,omitempty"`
	Exclude         []string `yaml:"exclude,omitempty" json:"exclude,omitempty"`
	Include         []string `yaml:"include,omitempty" json:"include,omitempty"`
	MaxDepth        int      `yaml:"max_depth,omitempty" json:"max_depth,omitempty"`
	RenderJS        *bool    `yaml:"render_js,omitempty" json:"render_js,omitempty"`
	Limit           int      `yaml:"limit,omitempty" json:"limit,omitempty"`
}

// Options represents global manifest options
type Options struct {
	ContinueOnError bool          `yaml:"continue_on_error" json:"continue_on_error"`
	Output          string        `yaml:"output,omitempty" json:"output,omitempty"`
	Concurrency     int           `yaml:"concurrency,omitempty" json:"concurrency,omitempty"`
	CacheTTL        time.Duration `yaml:"cache_ttl,omitempty" json:"cache_ttl,omitempty"`
}

// Validate validates the manifest configuration
func (c *Config) Validate() error {
	if len(c.Sources) == 0 {
		return ErrNoSources
	}
	for i, src := range c.Sources {
		if src.URL == "" {
			return fmt.Errorf("source %d: %w", i, ErrEmptyURL)
		}
	}
	return nil
}

// DefaultOptions returns options with sensible defaults
func DefaultOptions() Options {
	return Options{
		ContinueOnError: false,
		Output:          "./docs",
		Concurrency:     5,
		CacheTTL:        24 * time.Hour,
	}
}
</file>
<file path="internal/output/collector.go">
package output

import (
	"encoding/json"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
)

type MetadataCollector struct {
	mu        sync.RWMutex
	documents []*domain.SimpleDocumentMetadata
	sourceURL string
	strategy  string
	baseDir   string
	filename  string
	enabled   bool
}

type CollectorOptions struct {
	BaseDir   string
	Filename  string
	SourceURL string
	Strategy  string
	Enabled   bool
}

func NewMetadataCollector(opts CollectorOptions) *MetadataCollector {
	filename := opts.Filename
	if filename == "" {
		filename = "metadata.json"
	}
	return &MetadataCollector{
		documents: make([]*domain.SimpleDocumentMetadata, 0),
		sourceURL: opts.SourceURL,
		strategy:  opts.Strategy,
		baseDir:   opts.BaseDir,
		filename:  filename,
		enabled:   opts.Enabled,
	}
}

func (c *MetadataCollector) Add(doc *domain.Document, filePath string) {
	if !c.enabled || doc == nil {
		return
	}

	c.mu.Lock()
	defer c.mu.Unlock()

	relPath, err := filepath.Rel(c.baseDir, filePath)
	if err != nil {
		relPath = filePath
	}
	relPath = filepath.ToSlash(relPath)

	metadata := doc.ToSimpleDocumentMetadata(relPath)
	// Use the collector's strategy as the source, overriding the document's SourceStrategy
	metadata.Source = c.strategy
	c.documents = append(c.documents, metadata)
}

func (c *MetadataCollector) Flush() error {
	if !c.enabled || len(c.documents) == 0 {
		return nil
	}

	c.mu.RLock()
	defer c.mu.RUnlock()

	index := c.buildIndex()

	data, err := json.MarshalIndent(index, "", "  ")
	if err != nil {
		return err
	}

	outputPath := filepath.Join(c.baseDir, c.filename)
	return os.WriteFile(outputPath, data, 0644)
}

func (c *MetadataCollector) buildIndex() *domain.SimpleMetadataIndex {
	docs := make([]domain.SimpleDocumentMetadata, len(c.documents))

	for i, doc := range c.documents {
		docs[i] = *doc
	}

	return &domain.SimpleMetadataIndex{
		GeneratedAt:    time.Now(),
		SourceURL:      c.sourceURL,
		Strategy:       c.strategy,
		TotalDocuments: len(c.documents),
		Documents:      docs,
	}
}

func (c *MetadataCollector) Count() int {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return len(c.documents)
}

func (c *MetadataCollector) GetIndex() *domain.SimpleMetadataIndex {
	c.mu.RLock()
	defer c.mu.RUnlock()
	return c.buildIndex()
}

func (c *MetadataCollector) IsEnabled() bool {
	return c.enabled
}

func (c *MetadataCollector) SetStrategy(name string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.strategy = name
}

func (c *MetadataCollector) SetSourceURL(url string) {
	c.mu.Lock()
	defer c.mu.Unlock()
	c.sourceURL = url
}
</file>
<file path="internal/output/writer.go">
package output

import (
	"context"
	"os"
	"path/filepath"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type Writer struct {
	baseDir      string
	flat         bool
	jsonMetadata bool
	force        bool
	dryRun       bool
	collector    *MetadataCollector
}

type WriterOptions struct {
	BaseDir      string
	Flat         bool
	JSONMetadata bool
	Force        bool
	DryRun       bool
	Collector    *MetadataCollector
}

func NewWriter(opts WriterOptions) *Writer {
	if opts.BaseDir == "" {
		opts.BaseDir = "./docs"
	}

	return &Writer{
		baseDir:      opts.BaseDir,
		flat:         opts.Flat,
		jsonMetadata: opts.JSONMetadata,
		force:        opts.Force,
		dryRun:       opts.DryRun,
		collector:    opts.Collector,
	}
}

// Write saves a document to the output directory
func (w *Writer) Write(ctx context.Context, doc *domain.Document) error {
	// Generate path
	var path string
	if doc.RelativePath != "" {
		// For Git-sourced files, use the relative path directly
		path = utils.GeneratePathFromRelative(w.baseDir, doc.RelativePath, w.flat)
	} else {
		// For other sources, generate path from URL
		path = utils.GeneratePath(w.baseDir, doc.URL, w.flat)
	}

	// Check if file exists
	if !w.force {
		if _, err := os.Stat(path); err == nil {
			// File exists, skip
			return nil
		}
	}

	// Dry run - just return
	if w.dryRun {
		return nil
	}

	// Ensure directory exists
	if err := utils.EnsureDir(path); err != nil {
		return err
	}

	// Add frontmatter
	content, err := converter.AddFrontmatter(doc.Content, doc)
	if err != nil {
		return err
	}

	if err := os.WriteFile(path, []byte(content), 0644); err != nil {
		return err
	}

	if w.jsonMetadata && w.collector != nil {
		w.collector.Add(doc, path)
	}

	return nil
}

func (w *Writer) FlushMetadata() error {
	if w.collector != nil {
		return w.collector.Flush()
	}
	return nil
}

func (w *Writer) WriteMultiple(ctx context.Context, docs []*domain.Document) error {
	for _, doc := range docs {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
			if err := w.Write(ctx, doc); err != nil {
				return err
			}
		}
	}
	return nil
}

// GetPath returns the output path for a URL
func (w *Writer) GetPath(url string) string {
	return utils.GeneratePath(w.baseDir, url, w.flat)
}

// Exists checks if a document already exists
func (w *Writer) Exists(url string) bool {
	path := w.GetPath(url)
	_, err := os.Stat(path)
	return err == nil
}

// EnsureBaseDir creates the base directory if it doesn't exist
func (w *Writer) EnsureBaseDir() error {
	return os.MkdirAll(w.baseDir, 0755)
}

// Clean removes the output directory
func (w *Writer) Clean() error {
	return os.RemoveAll(w.baseDir)
}

// Stats returns statistics about the output directory
func (w *Writer) Stats() (int, int64, error) {
	var count int
	var size int64

	err := filepath.Walk(w.baseDir, func(path string, info os.FileInfo, err error) error {
		if err != nil {
			return err
		}
		if !info.IsDir() && filepath.Ext(path) == ".md" {
			count++
			size += info.Size()
		}
		return nil
	})

	return count, size, err
}
</file>
<file path="internal/renderer/detector.go">
package renderer

import (
	"regexp"
	"strings"
)

// SPA detection patterns
var (
	// React patterns
	reactPatterns = []string{
		`<div id="root"></div>`,
		`<div id="root"/>`,
		`<div id="app"></div>`,
		`<div id="app"/>`,
		`data-reactroot`,
		`__REACT_DEVTOOLS_GLOBAL_HOOK__`,
	}

	// Vue patterns
	vuePatterns = []string{
		`<div id="app"></div>`,
		`<div id="app"/>`,
		`__VUE__`,
		`v-cloak`,
		`Vue.createApp`,
	}

	// Next.js patterns
	nextPatterns = []string{
		`<div id="__next"></div>`,
		`<div id="__next"/>`,
		`__NEXT_DATA__`,
		`_next/static`,
	}

	// Nuxt patterns
	nuxtPatterns = []string{
		`__NUXT__`,
		`window.__NUXT__`,
		`<div id="__nuxt">`,
	}

	// Angular patterns
	angularPatterns = []string{
		`ng-version`,
		`ng-app`,
		`ng-controller`,
		`<app-root>`,
	}

	// Svelte patterns
	sveltePatterns = []string{
		`__svelte`,
		`svelte-`,
	}

	// Generic SPA indicators
	spaIndicators = []string{
		`window.__INITIAL_STATE__`,
		`window.__STATE__`,
		`window.__PRELOADED_STATE__`,
	}
)

// contentMinLength is the minimum content length to consider a page as rendered
const contentMinLength = 500

// scriptTagRegex matches script tags
var scriptTagRegex = regexp.MustCompile(`<script[^>]*>[\s\S]*?</script>`)

// htmlTagRegex matches HTML tags
var htmlTagRegex = regexp.MustCompile(`<[^>]+>`)

// NeedsJSRendering detects if a page needs JavaScript rendering
func NeedsJSRendering(html string) bool {
	// Check for SPA framework patterns
	if hasSPAPattern(html) {
		return true
	}

	// Check content length without scripts
	contentWithoutScripts := scriptTagRegex.ReplaceAllString(html, "")
	textContent := htmlTagRegex.ReplaceAllString(contentWithoutScripts, "")
	textContent = strings.TrimSpace(textContent)

	// If there's very little content but many scripts, likely a SPA
	if len(textContent) < contentMinLength {
		scriptCount := strings.Count(strings.ToLower(html), "<script")
		if scriptCount > 3 {
			return true
		}
	}

	return false
}

// hasSPAPattern checks if the HTML contains any SPA framework patterns
func hasSPAPattern(html string) bool {
	htmlLower := strings.ToLower(html)

	allPatterns := append([]string{}, reactPatterns...)
	allPatterns = append(allPatterns, vuePatterns...)
	allPatterns = append(allPatterns, nextPatterns...)
	allPatterns = append(allPatterns, nuxtPatterns...)
	allPatterns = append(allPatterns, angularPatterns...)
	allPatterns = append(allPatterns, sveltePatterns...)
	allPatterns = append(allPatterns, spaIndicators...)

	for _, pattern := range allPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return true
		}
	}

	return false
}

// DetectFramework attempts to detect which SPA framework is being used
func DetectFramework(html string) string {
	htmlLower := strings.ToLower(html)

	for _, pattern := range nextPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Next.js"
		}
	}

	for _, pattern := range nuxtPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Nuxt"
		}
	}

	for _, pattern := range reactPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "React"
		}
	}

	for _, pattern := range vuePatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Vue"
		}
	}

	for _, pattern := range angularPatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Angular"
		}
	}

	for _, pattern := range sveltePatterns {
		if strings.Contains(htmlLower, strings.ToLower(pattern)) {
			return "Svelte"
		}
	}

	return "Unknown"
}

// HasDynamicContent checks for indicators of dynamic content loading
func HasDynamicContent(html string) bool {
	indicators := []string{
		"loading...",
		"loading…",
		"please wait",
		"spinner",
		"skeleton",
		"lazy-load",
		"lazyload",
		"infinite-scroll",
	}

	htmlLower := strings.ToLower(html)
	for _, indicator := range indicators {
		if strings.Contains(htmlLower, indicator) {
			return true
		}
	}

	return false
}
</file>
<file path="internal/renderer/pool.go">
package renderer

import (
	"context"
	"sync"

	"github.com/go-rod/rod"
)

// TabPool manages a pool of browser tabs for concurrent rendering
type TabPool struct {
	browser    *rod.Browser
	maxTabs    int
	activeTabs chan *rod.Page
	mu         sync.Mutex
	closed     bool
}

// NewTabPool creates a new tab pool
func NewTabPool(browser *rod.Browser, maxTabs int) (*TabPool, error) {
	if maxTabs <= 0 {
		maxTabs = 5
	}

	pool := &TabPool{
		browser:    browser,
		maxTabs:    maxTabs,
		activeTabs: make(chan *rod.Page, maxTabs),
	}

	// Pre-create tabs
	for i := 0; i < maxTabs; i++ {
		page, err := StealthPage(browser)
		if err != nil {
			pool.Close()
			return nil, err
		}
		pool.activeTabs <- page
	}

	return pool, nil
}

// Acquire gets a page from the pool, blocking if none available
func (p *TabPool) Acquire(ctx context.Context) (*rod.Page, error) {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		return nil, ErrPoolClosed
	}
	p.mu.Unlock()

	select {
	case page := <-p.activeTabs:
		return page, nil
	case <-ctx.Done():
		return nil, ctx.Err()
	}
}

// Release returns a page to the pool after cleaning up
func (p *TabPool) Release(page *rod.Page) {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		page.Close()
		return
	}
	p.mu.Unlock()

	// Clean up the page before returning to pool
	_ = page.Navigate("about:blank")

	select {
	case p.activeTabs <- page:
		// Successfully returned to pool
	default:
		// Pool is full (shouldn't happen normally)
		page.Close()
	}
}

// Close closes all tabs and the pool
func (p *TabPool) Close() error {
	p.mu.Lock()
	if p.closed {
		p.mu.Unlock()
		return nil
	}
	p.closed = true
	p.mu.Unlock()

	close(p.activeTabs)

	// Close remaining pages
	for page := range p.activeTabs {
		page.Close()
	}

	return nil
}

// Size returns the current number of available tabs
func (p *TabPool) Size() int {
	return len(p.activeTabs)
}

// MaxSize returns the maximum pool size
func (p *TabPool) MaxSize() int {
	return p.maxTabs
}

// ErrPoolClosed is returned when trying to acquire from a closed pool
var ErrPoolClosed = &poolError{message: "pool is closed"}

type poolError struct {
	message string
}

func (e *poolError) Error() string {
	return e.message
}
</file>
<file path="internal/renderer/rod.go">
package renderer

import (
	"context"
	"fmt"
	"net/http"
	"net/url"
	"os"
	"time"

	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/launcher"
	"github.com/go-rod/rod/lib/proto"
	"github.com/quantmind-br/repodocs-go/internal/domain"
)

// Renderer provides JavaScript rendering using headless Chrome
type Renderer struct {
	browser  *rod.Browser
	pool     *TabPool
	timeout  time.Duration
	stealth  bool
	headless bool
}

// RendererOptions contains options for creating a Renderer
type RendererOptions struct {
	Timeout     time.Duration
	MaxTabs     int
	Stealth     bool
	Headless    bool
	BrowserPath string
	NoSandbox   bool // Required for running in CI/Docker environments
}

// DefaultRendererOptions returns default renderer options
func DefaultRendererOptions() RendererOptions {
	return RendererOptions{
		Timeout:     60 * time.Second,
		MaxTabs:     5,
		Stealth:     true,
		Headless:    true,
		BrowserPath: "",
		NoSandbox:   isCI(), // Auto-detect CI environment
	}
}

// isCI returns true if running in a CI environment
func isCI() bool {
	return os.Getenv("CI") != "" || os.Getenv("GITHUB_ACTIONS") != ""
}

// NewRenderer creates a new headless browser renderer
func NewRenderer(opts RendererOptions) (*Renderer, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = 60 * time.Second
	}
	if opts.MaxTabs <= 0 {
		opts.MaxTabs = 5
	}

	// Create launcher
	l := launcher.New()

	if opts.BrowserPath != "" {
		l = l.Bin(opts.BrowserPath)
	}

	if opts.Headless {
		l = l.Headless(true)
	}

	// Additional flags for stealth
	if opts.Stealth {
		l = l.Set("disable-blink-features", "AutomationControlled")
	}

	// NoSandbox is required for running in CI/Docker environments
	if opts.NoSandbox {
		l = l.NoSandbox(true)
	}

	// Launch browser
	controlURL, err := l.Launch()
	if err != nil {
		return nil, fmt.Errorf("failed to launch browser: %w", err)
	}

	browser := rod.New().ControlURL(controlURL)
	if err := browser.Connect(); err != nil {
		return nil, fmt.Errorf("failed to connect to browser: %w", err)
	}

	// Create tab pool
	pool, err := NewTabPool(browser, opts.MaxTabs)
	if err != nil {
		browser.Close()
		return nil, fmt.Errorf("failed to create tab pool: %w", err)
	}

	return &Renderer{
		browser:  browser,
		pool:     pool,
		timeout:  opts.Timeout,
		stealth:  opts.Stealth,
		headless: opts.Headless,
	}, nil
}

// Render fetches and renders a page with JavaScript
func (r *Renderer) Render(ctx context.Context, url string, opts domain.RenderOptions) (string, error) {
	if opts.Timeout <= 0 {
		opts.Timeout = r.timeout
	}

	// Create context with timeout
	ctx, cancel := context.WithTimeout(ctx, opts.Timeout)
	defer cancel()

	// Acquire a page from the pool
	page, err := r.pool.Acquire(ctx)
	if err != nil {
		return "", fmt.Errorf("failed to acquire page: %w", err)
	}
	defer r.pool.Release(page)

	// Apply context to page so all operations respect the timeout
	page = page.Context(ctx)

	// Apply stealth mode
	if r.stealth {
		if err := ApplyStealthMode(page); err != nil {
			return "", fmt.Errorf("failed to apply stealth mode: %w", err)
		}
	}

	// Set cookies if provided
	if len(opts.Cookies) > 0 {
		if err := r.setCookies(page, url, opts.Cookies); err != nil {
			return "", fmt.Errorf("failed to set cookies: %w", err)
		}
	}

	// Navigate to URL
	if err := page.Navigate(url); err != nil {
		return "", domain.NewFetchError(url, 0, fmt.Errorf("navigation failed: %w", err))
	}

	// Wait for page to load
	if err := page.WaitLoad(); err != nil {
		return "", fmt.Errorf("failed waiting for load: %w", err)
	}

	// Wait for specific selector if provided
	if opts.WaitFor != "" {
		if err := page.Timeout(opts.Timeout).MustElement(opts.WaitFor).WaitVisible(); err != nil {
			// Don't fail, just continue
		}
	}

	// Wait for network to be idle
	if opts.WaitStable > 0 {
		if err := page.WaitRequestIdle(opts.WaitStable, nil, nil, nil); err != nil {
			// Don't fail, just continue
		}
	}

	// Scroll to bottom to load lazy content
	if opts.ScrollToEnd {
		if err := r.scrollToEnd(page); err != nil {
			// Don't fail, just continue
		}
	}

	// Get rendered HTML
	html, err := page.HTML()
	if err != nil {
		return "", fmt.Errorf("failed to get HTML: %w", err)
	}

	return html, nil
}

// setCookies sets cookies on a page
func (r *Renderer) setCookies(page *rod.Page, pageURL string, cookies []*http.Cookie) error {
	// Parse URL to extract domain if cookie domain is empty
	parsedURL, err := url.Parse(pageURL)
	if err != nil {
		return fmt.Errorf("failed to parse URL for cookies: %w", err)
	}

	for _, cookie := range cookies {
		// Use cookie domain if set, otherwise extract from URL
		domain := cookie.Domain
		if domain == "" {
			domain = parsedURL.Hostname()
		}

		// Use cookie path if set, otherwise default to "/"
		path := cookie.Path
		if path == "" {
			path = "/"
		}

		err := page.SetCookies([]*proto.NetworkCookieParam{
			{
				Name:     cookie.Name,
				Value:    cookie.Value,
				Domain:   domain,
				Path:     path,
				Secure:   cookie.Secure,
				HTTPOnly: cookie.HttpOnly,
			},
		})
		if err != nil {
			return err
		}
	}
	return nil
}

// scrollToEnd scrolls to the bottom of the page to trigger lazy loading
func (r *Renderer) scrollToEnd(page *rod.Page) error {
	// Get initial scroll height
	result, err := page.Eval(`() => document.body.scrollHeight`)
	if err != nil {
		return err
	}
	lastHeight := result.Value.Int()

	for i := 0; i < 10; i++ { // Max 10 scroll iterations
		// Scroll to bottom
		_, err := page.Eval(`() => window.scrollTo(0, document.body.scrollHeight)`)
		if err != nil {
			return err
		}

		// Wait for content to load
		time.Sleep(500 * time.Millisecond)

		// Check new scroll height
		result, err := page.Eval(`() => document.body.scrollHeight`)
		if err != nil {
			return err
		}
		newHeight := result.Value.Int()

		// If height hasn't changed, we've reached the bottom
		if newHeight == lastHeight {
			break
		}
		lastHeight = newHeight
	}

	// Scroll back to top
	_, _ = page.Eval(`() => window.scrollTo(0, 0)`)

	return nil
}

// DefaultRenderOptions returns default render options
func DefaultRenderOptions() domain.RenderOptions {
	return domain.RenderOptions{
		Timeout:     60 * time.Second,
		WaitStable:  2 * time.Second,
		ScrollToEnd: true,
	}
}

// Close releases browser resources
func (r *Renderer) Close() error {
	if r.pool != nil {
		r.pool.Close()
		r.pool = nil
	}
	if r.browser != nil {
		browser := r.browser
		r.browser = nil
		return browser.Close()
	}
	return nil
}

// IsAvailable checks if the browser is available
func IsAvailable() bool {
	path, exists := launcher.LookPath()
	return exists && path != ""
}

// GetBrowserPath returns the detected browser path
func GetBrowserPath() (string, bool) {
	return launcher.LookPath()
}

// GetTabPool returns the tab pool for testing purposes
func (r *Renderer) GetTabPool() (*TabPool, error) {
	if r.pool == nil {
		return nil, fmt.Errorf("pool not initialized")
	}
	return r.pool, nil
}
</file>
<file path="internal/renderer/stealth.go">
package renderer

import (
	"github.com/go-rod/rod"
	"github.com/go-rod/rod/lib/proto"
	"github.com/go-rod/stealth"
)

// StealthPage creates a new stealth page that's harder to detect as automated
func StealthPage(browser *rod.Browser) (*rod.Page, error) {
	page, err := stealth.Page(browser)
	if err != nil {
		return nil, err
	}
	return page, nil
}

// ApplyStealthMode applies stealth mode configurations to a page
// This includes removing webdriver flags and emulating real browser behavior
func ApplyStealthMode(page *rod.Page) error {
	// The stealth package already handles most of this, but we can add extra measures

	// Set a realistic viewport using proto.EmulationSetDeviceMetricsOverride
	err := page.SetViewport(&proto.EmulationSetDeviceMetricsOverride{
		Width:  1920,
		Height: 1080,
	})
	if err != nil {
		return err
	}

	// Simple stealth: just hide webdriver flag
	// Rod expects arrow function format: () => expression
	js := `() => { Object.defineProperty(navigator, 'webdriver', { get: () => undefined }); return true; }`
	_, err = page.Eval(js)
	return err
}

// StealthOptions contains options for stealth mode
type StealthOptions struct {
	// HideWebdriver hides the webdriver property
	HideWebdriver bool
	// EmulatePlugins emulates real browser plugins
	EmulatePlugins bool
	// RandomizeViewport randomizes the viewport size
	RandomizeViewport bool
	// DisableAutomationFlags disables Chrome automation flags
	DisableAutomationFlags bool
}

// DefaultStealthOptions returns default stealth options
func DefaultStealthOptions() StealthOptions {
	return StealthOptions{
		HideWebdriver:          true,
		EmulatePlugins:         true,
		RandomizeViewport:      false,
		DisableAutomationFlags: true,
	}
}
</file>
<file path="internal/state/errors.go">
package state

import "errors"

var (
	// ErrStateNotFound indicates the state file does not exist
	ErrStateNotFound = errors.New("state file not found")

	// ErrStateCorrupted indicates the state file contains invalid JSON
	ErrStateCorrupted = errors.New("state file is corrupted")

	// ErrVersionMismatch indicates an incompatible state schema version
	ErrVersionMismatch = errors.New("state version mismatch")
)
</file>
<file path="internal/state/manager.go">
package state

import (
	"context"
	"encoding/json"
	"os"
	"path/filepath"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

const StateFileName = ".repodocs-state.json"

type Manager struct {
	baseDir  string
	state    *SyncState
	mu       sync.RWMutex
	dirty    bool
	logger   *utils.Logger
	disabled bool
	seenURLs sync.Map
}

type ManagerOptions struct {
	BaseDir   string
	SourceURL string
	Strategy  string
	Logger    *utils.Logger
	Disabled  bool
}

func NewManager(opts ManagerOptions) *Manager {
	return &Manager{
		baseDir:  opts.BaseDir,
		logger:   opts.Logger,
		disabled: opts.Disabled,
		state:    NewSyncState(opts.SourceURL, opts.Strategy),
	}
}

func (m *Manager) Load(ctx context.Context) error {
	if m.disabled {
		return nil
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	path := m.statePath()
	data, err := os.ReadFile(path)
	if os.IsNotExist(err) {
		return ErrStateNotFound
	}
	if err != nil {
		return err
	}

	var state SyncState
	if err := json.Unmarshal(data, &state); err != nil {
		return ErrStateCorrupted
	}

	if state.Version != StateVersion {
		if m.logger != nil {
			m.logger.Warn().
				Int("file_version", state.Version).
				Int("expected_version", StateVersion).
				Msg("State version mismatch, will rebuild state")
		}
		return ErrVersionMismatch
	}

	m.state = &state
	return nil
}

func (m *Manager) Save(ctx context.Context) error {
	if m.disabled {
		return nil
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	if !m.dirty {
		return nil
	}

	m.state.LastSync = time.Now()

	data, err := json.MarshalIndent(m.state, "", "  ")
	if err != nil {
		return err
	}

	path := m.statePath()
	if err := os.MkdirAll(filepath.Dir(path), 0755); err != nil {
		return err
	}

	if err := os.WriteFile(path, data, 0644); err != nil {
		return err
	}

	m.dirty = false
	if m.logger != nil {
		m.logger.Debug().
			Int("pages", len(m.state.Pages)).
			Str("path", path).
			Msg("State saved")
	}
	return nil
}

func (m *Manager) ShouldProcess(url, contentHash string) bool {
	if m.disabled {
		return true
	}

	m.mu.RLock()
	defer m.mu.RUnlock()

	page, exists := m.state.Pages[url]
	if !exists {
		return true
	}

	return page.ContentHash != contentHash
}

func (m *Manager) Update(url string, page PageState) {
	if m.disabled {
		return
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	m.state.Pages[url] = page
	m.dirty = true
}

func (m *Manager) MarkSeen(url string) {
	m.seenURLs.Store(url, true)
}

func (m *Manager) GetDeletedPages() []PageState {
	if m.disabled {
		return nil
	}

	m.mu.RLock()
	defer m.mu.RUnlock()

	var deleted []PageState
	for url, page := range m.state.Pages {
		if _, seen := m.seenURLs.Load(url); !seen {
			deleted = append(deleted, page)
		}
	}
	return deleted
}

func (m *Manager) RemoveDeletedFromState() {
	if m.disabled {
		return
	}

	m.mu.Lock()
	defer m.mu.Unlock()

	for url := range m.state.Pages {
		if _, seen := m.seenURLs.Load(url); !seen {
			delete(m.state.Pages, url)
			m.dirty = true
		}
	}
}

func (m *Manager) Stats() (total, cached int) {
	m.mu.RLock()
	defer m.mu.RUnlock()

	total = len(m.state.Pages)

	var seenCount int
	m.seenURLs.Range(func(_, _ any) bool {
		seenCount++
		return true
	})

	return total, total - seenCount
}

func (m *Manager) IsDisabled() bool {
	return m.disabled
}

func (m *Manager) statePath() string {
	return filepath.Join(m.baseDir, StateFileName)
}
</file>
<file path="internal/state/models.go">
package state

import "time"

// StateVersion is the schema version for state file migration
const StateVersion = 1

// SyncState represents the complete synchronization state for a source
type SyncState struct {
	Version   int                  `json:"version"`
	SourceURL string               `json:"source_url"`
	Strategy  string               `json:"strategy,omitempty"`
	LastSync  time.Time            `json:"last_sync"`
	Pages     map[string]PageState `json:"pages"`
}

// PageState represents the state of an individual processed page
type PageState struct {
	ContentHash string    `json:"content_hash"`
	FetchedAt   time.Time `json:"fetched_at"`
	FilePath    string    `json:"file_path"`
}

// NewSyncState creates a new empty sync state
func NewSyncState(sourceURL, strategy string) *SyncState {
	return &SyncState{
		Version:   StateVersion,
		SourceURL: sourceURL,
		Strategy:  strategy,
		LastSync:  time.Now(),
		Pages:     make(map[string]PageState),
	}
}

// PageCount returns the number of pages in the state
func (s *SyncState) PageCount() int {
	return len(s.Pages)
}

// HasPage checks if a page exists in the state
func (s *SyncState) HasPage(url string) bool {
	_, exists := s.Pages[url]
	return exists
}

// GetPage returns a page state by URL
func (s *SyncState) GetPage(url string) (PageState, bool) {
	page, exists := s.Pages[url]
	return page, exists
}

// SetPage updates or adds a page to the state
func (s *SyncState) SetPage(url string, page PageState) {
	s.Pages[url] = page
}

// RemovePage removes a page from the state
func (s *SyncState) RemovePage(url string) {
	delete(s.Pages, url)
}
</file>
<file path="internal/strategies/git/archive.go">
package git

import (
	"archive/tar"
	"compress/gzip"
	"context"
	"fmt"
	"io"
	"net/http"
	"os"
	"path/filepath"
	"strings"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type ArchiveFetcher struct {
	httpClient *http.Client
	logger     *utils.Logger
}

type ArchiveFetcherOptions struct {
	HTTPClient *http.Client
	Logger     *utils.Logger
}

func NewArchiveFetcher(opts ArchiveFetcherOptions) *ArchiveFetcher {
	return &ArchiveFetcher{
		httpClient: opts.HTTPClient,
		logger:     opts.Logger,
	}
}

func (f *ArchiveFetcher) Name() string {
	return "archive"
}

func (f *ArchiveFetcher) Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error) {
	archiveURL := f.BuildArchiveURL(info, branch)
	if f.logger != nil {
		f.logger.Debug().Str("archive_url", archiveURL).Msg("Downloading archive")
	}

	if err := f.DownloadAndExtract(ctx, archiveURL, destDir); err != nil {
		return nil, err
	}

	return &FetchResult{
		LocalPath: destDir,
		Branch:    branch,
		Method:    "archive",
	}, nil
}

func (f *ArchiveFetcher) BuildArchiveURL(info *RepoInfo, branch string) string {
	switch info.Platform {
	case PlatformGitHub:
		return fmt.Sprintf("https://github.com/%s/%s/archive/refs/heads/%s.tar.gz",
			info.Owner, info.Repo, branch)
	case PlatformGitLab:
		return fmt.Sprintf("https://gitlab.com/%s/%s/-/archive/%s/%s-%s.tar.gz",
			info.Owner, info.Repo, branch, info.Repo, branch)
	case PlatformBitbucket:
		return fmt.Sprintf("https://bitbucket.org/%s/%s/get/%s.tar.gz",
			info.Owner, info.Repo, branch)
	default:
		return fmt.Sprintf("https://github.com/%s/%s/archive/refs/heads/%s.tar.gz",
			info.Owner, info.Repo, branch)
	}
}

func (f *ArchiveFetcher) DownloadAndExtract(ctx context.Context, archiveURL, destDir string) error {
	req, err := http.NewRequestWithContext(ctx, "GET", archiveURL, nil)
	if err != nil {
		return err
	}

	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		req.Header.Set("Authorization", "token "+token)
	}

	resp, err := f.httpClient.Do(req)
	if err != nil {
		return fmt.Errorf("download request failed: %w", err)
	}
	defer resp.Body.Close()

	if resp.StatusCode == http.StatusNotFound {
		return fmt.Errorf("archive not found (404)")
	}
	if resp.StatusCode == http.StatusUnauthorized {
		return fmt.Errorf("authentication required (401)")
	}
	if resp.StatusCode != http.StatusOK {
		return fmt.Errorf("download failed with status: %d", resp.StatusCode)
	}

	return f.ExtractTarGz(resp.Body, destDir)
}

func (f *ArchiveFetcher) ExtractTarGz(r io.Reader, destDir string) error {
	gzr, err := gzip.NewReader(r)
	if err != nil {
		return fmt.Errorf("gzip reader failed: %w", err)
	}
	defer gzr.Close()

	tr := tar.NewReader(gzr)

	for {
		header, err := tr.Next()
		if err == io.EOF {
			break
		}
		if err != nil {
			return fmt.Errorf("tar read failed: %w", err)
		}

		parts := strings.SplitN(header.Name, "/", 2)
		if len(parts) < 2 || parts[1] == "" {
			continue
		}
		relativePath := parts[1]

		targetPath := filepath.Join(destDir, relativePath)

		if !strings.HasPrefix(filepath.Clean(targetPath), filepath.Clean(destDir)) {
			continue
		}

		switch header.Typeflag {
		case tar.TypeDir:
			if err := os.MkdirAll(targetPath, 0755); err != nil {
				return fmt.Errorf("mkdir failed: %w", err)
			}
		case tar.TypeReg:
			if err := os.MkdirAll(filepath.Dir(targetPath), 0755); err != nil {
				return fmt.Errorf("mkdir failed: %w", err)
			}

			file, err := os.OpenFile(targetPath, os.O_CREATE|os.O_WRONLY|os.O_TRUNC, os.FileMode(header.Mode))
			if err != nil {
				return fmt.Errorf("create file failed: %w", err)
			}

			if _, err := io.Copy(file, tr); err != nil {
				file.Close()
				return fmt.Errorf("copy failed: %w", err)
			}
			file.Close()
		}
	}

	return nil
}
</file>
<file path="internal/strategies/git/clone.go">
package git

import (
	"bufio"
	"context"
	"fmt"
	"os"
	"os/exec"
	"strings"

	"github.com/go-git/go-git/v5"
	githttp "github.com/go-git/go-git/v5/plumbing/transport/http"

	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type CloneFetcher struct {
	logger *utils.Logger
}

type CloneFetcherOptions struct {
	Logger *utils.Logger
}

func NewCloneFetcher(opts CloneFetcherOptions) *CloneFetcher {
	return &CloneFetcher{logger: opts.Logger}
}

func (f *CloneFetcher) Name() string {
	return "clone"
}

func (f *CloneFetcher) Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error) {
	if f.logger != nil {
		f.logger.Info().Str("url", info.URL).Msg("Cloning repository")
	}

	cloneOpts := &git.CloneOptions{
		URL:      info.URL,
		Depth:    1,
		Progress: os.Stdout,
	}

	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		cloneOpts.Auth = &githttp.BasicAuth{
			Username: "token",
			Password: token,
		}
	}

	repo, err := git.PlainCloneContext(ctx, destDir, false, cloneOpts)
	if err != nil {
		return nil, err
	}

	detectedBranch := branch
	head, err := repo.Head()
	if err == nil {
		refName := head.Name().String()
		if strings.HasPrefix(refName, "refs/heads/") {
			detectedBranch = strings.TrimPrefix(refName, "refs/heads/")
		}
	}

	if detectedBranch == "" {
		detectedBranch = "main"
	}

	return &FetchResult{
		LocalPath: destDir,
		Branch:    detectedBranch,
		Method:    "clone",
	}, nil
}

func DetectDefaultBranch(ctx context.Context, url string) (string, error) {
	cmd := exec.CommandContext(ctx, "git", "ls-remote", "--symref", url, "HEAD")
	output, err := cmd.Output()
	if err != nil {
		return "", fmt.Errorf("git ls-remote failed: %w", err)
	}

	scanner := bufio.NewScanner(strings.NewReader(string(output)))
	for scanner.Scan() {
		line := scanner.Text()
		if strings.HasPrefix(line, "ref: refs/heads/") {
			parts := strings.Split(line, "\t")
			if len(parts) >= 1 {
				branch := strings.TrimPrefix(parts[0], "ref: refs/heads/")
				return branch, nil
			}
		}
	}

	return "", fmt.Errorf("could not determine default branch")
}
</file>
<file path="internal/strategies/git/doc.go">
// Package git implements the git repository extraction strategy.
//
// It supports extracting documentation from GitHub, GitLab, and Bitbucket
// repositories using either archive download (faster) or git clone (fallback).
//
// Architecture:
//   - Strategy: Coordinator implementing strategies.Strategy interface
//   - Parser: URL parsing and platform detection
//   - ArchiveFetcher: HTTP-based tar.gz download and extraction
//   - CloneFetcher: go-git based repository cloning
//   - Processor: File discovery and document conversion
//
// Usage:
//
//	strategy := git.NewStrategy(deps)
//	if strategy.CanHandle(url) {
//	    err := strategy.Execute(ctx, url, opts)
//	}
package git
</file>
<file path="internal/strategies/git/fetcher.go">
package git

import "context"

type RepoFetcher interface {
	Fetch(ctx context.Context, info *RepoInfo, branch, destDir string) (*FetchResult, error)
	Name() string
}
</file>
<file path="internal/strategies/git/parser.go">
package git

import (
	"fmt"
	"net/url"
	"path/filepath"
	"regexp"
	"strings"
)

type platformPattern struct {
	platform    Platform
	repoPattern *regexp.Regexp
	treePattern *regexp.Regexp
}

type Parser struct {
	patterns []platformPattern
}

func NewParser() *Parser {
	return &Parser{
		patterns: []platformPattern{
			{
				platform:    PlatformGitHub,
				repoPattern: regexp.MustCompile(`^(https?://github\.com/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/tree/([^/]+)(?:/(.+))?$`),
			},
			{
				platform:    PlatformGitLab,
				repoPattern: regexp.MustCompile(`^(https?://gitlab\.com/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/-/tree/([^/]+)(?:/(.+))?$`),
			},
			{
				platform:    PlatformBitbucket,
				repoPattern: regexp.MustCompile(`^(https?://bitbucket\.org/([^/]+)/([^/]+?))(\.git)?(/|$)`),
				treePattern: regexp.MustCompile(`/src/([^/]+)(?:/(.+))?$`),
			},
		},
	}
}

func (p *Parser) ParseURL(rawURL string) (*RepoInfo, error) {
	patterns := []struct {
		platform Platform
		regex    *regexp.Regexp
	}{
		{PlatformGitHub, regexp.MustCompile(`github\.com[:/]([^/]+)/([^/.]+)`)},
		{PlatformGitLab, regexp.MustCompile(`gitlab\.com[:/]([^/]+)/([^/.]+)`)},
		{PlatformBitbucket, regexp.MustCompile(`bitbucket\.org[:/]([^/]+)/([^/.]+)`)},
	}

	for _, pat := range patterns {
		if matches := pat.regex.FindStringSubmatch(rawURL); len(matches) == 3 {
			return &RepoInfo{
				Platform: pat.platform,
				Owner:    matches[1],
				Repo:     strings.TrimSuffix(matches[2], ".git"),
				URL:      rawURL,
			}, nil
		}
	}

	return nil, fmt.Errorf("unsupported git URL format: %s", rawURL)
}

func (p *Parser) ParseURLWithPath(rawURL string) (*GitURLInfo, error) {
	info := &GitURLInfo{}
	lower := strings.ToLower(rawURL)

	for _, pat := range p.patterns {
		if !strings.Contains(lower, string(pat.platform)) {
			continue
		}

		repoMatches := pat.repoPattern.FindStringSubmatch(rawURL)
		if len(repoMatches) < 4 {
			continue
		}

		info.Platform = pat.platform
		info.RepoURL = repoMatches[1]
		info.Owner = repoMatches[2]
		info.Repo = strings.TrimSuffix(repoMatches[3], ".git")

		treeMatches := pat.treePattern.FindStringSubmatch(rawURL)
		if len(treeMatches) >= 2 {
			info.Branch = treeMatches[1]
			if len(treeMatches) >= 3 && treeMatches[2] != "" {
				info.SubPath = NormalizeFilterPath(treeMatches[2])
			}
		}

		return info, nil
	}

	if strings.HasPrefix(rawURL, "http://") || strings.HasPrefix(rawURL, "https://") {
		info.Platform = PlatformGeneric
		info.RepoURL = rawURL
		return info, nil
	}

	return nil, fmt.Errorf("unsupported git URL format: %s", rawURL)
}

func NormalizeFilterPath(path string) string {
	if path == "" {
		return ""
	}

	if strings.HasPrefix(path, "http://") || strings.HasPrefix(path, "https://") {
		path = ExtractPathFromTreeURL(path)
	}

	decoded, err := url.PathUnescape(path)
	if err == nil {
		path = decoded
	}

	path = strings.ReplaceAll(path, "\\", "/")
	path = strings.Trim(path, "/")
	path = filepath.Clean(path)

	return path
}

func ExtractPathFromTreeURL(rawURL string) string {
	patterns := []*regexp.Regexp{
		regexp.MustCompile(`github\.com/[^/]+/[^/]+/(?:tree|blob)/[^/]+/(.+)$`),
		regexp.MustCompile(`gitlab\.com/[^/]+/[^/]+/-/(?:tree|blob)/[^/]+/(.+)$`),
		regexp.MustCompile(`bitbucket\.org/[^/]+/[^/]+/src/[^/]+/(.+)$`),
	}

	for _, pat := range patterns {
		if matches := pat.FindStringSubmatch(rawURL); len(matches) >= 2 {
			return matches[1]
		}
	}

	return rawURL
}
</file>
<file path="internal/strategies/git/processor.go">
package git

import (
	"context"
	"crypto/sha256"
	"encoding/hex"
	"fmt"
	"io/fs"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type Processor struct {
	logger *utils.Logger
}

type ProcessorOptions struct {
	Logger *utils.Logger
}

func NewProcessor(opts ProcessorOptions) *Processor {
	return &Processor{logger: opts.Logger}
}

type ProcessOptions struct {
	RepoURL      string
	Branch       string
	FilterPath   string
	Concurrency  int
	Limit        int
	DryRun       bool
	WriteFunc    func(ctx context.Context, doc *domain.Document) error
	StateManager *state.Manager
}

func (p *Processor) FindDocumentationFiles(dir string, filterPath string) ([]string, error) {
	var files []string

	walkDir := dir
	if filterPath != "" {
		walkDir = filepath.Join(dir, filterPath)

		info, err := os.Stat(walkDir)
		if err != nil {
			if os.IsNotExist(err) {
				return nil, fmt.Errorf("filter path does not exist in repository: %s", filterPath)
			}
			return nil, fmt.Errorf("failed to access filter path: %w", err)
		}
		if !info.IsDir() {
			return nil, fmt.Errorf("filter path is not a directory: %s", filterPath)
		}

		if p.logger != nil {
			p.logger.Debug().Str("filter_path", filterPath).Str("walk_dir", walkDir).Msg("Walking filtered directory")
		}
	}

	err := filepath.WalkDir(walkDir, func(path string, d fs.DirEntry, err error) error {
		if err != nil {
			return err
		}

		if d.IsDir() {
			if IgnoreDirs[d.Name()] {
				return fs.SkipDir
			}
			return nil
		}

		ext := strings.ToLower(filepath.Ext(path))
		if DocumentExtensions[ext] {
			files = append(files, path)
		}

		return nil
	})

	return files, err
}

func (p *Processor) ProcessFiles(ctx context.Context, files []string, tmpDir string, opts ProcessOptions) error {
	bar := utils.NewProgressBar(len(files), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, files, opts.Concurrency, func(ctx context.Context, file string) error {
		defer bar.Add(1)

		if err := p.ProcessFile(ctx, file, tmpDir, opts); err != nil {
			if p.logger != nil {
				p.logger.Warn().Err(err).Str("file", file).Msg("Failed to process file")
			}
		}
		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	if p.logger != nil {
		p.logger.Info().Msg("Git extraction completed")
	}
	return nil
}

func (p *Processor) ProcessFile(ctx context.Context, path, tmpDir string, opts ProcessOptions) error {
	content, err := os.ReadFile(path)
	if err != nil {
		return err
	}

	if len(content) > 10*1024*1024 {
		return nil
	}

	relPath, _ := filepath.Rel(tmpDir, path)
	relPathURL := strings.ReplaceAll(relPath, "\\", "/")
	fileURL := opts.RepoURL + "/blob/" + opts.Branch + "/" + relPathURL

	contentHash := computeHash(content)

	doc := &domain.Document{
		URL:            fileURL,
		Title:          ExtractTitleFromPath(relPath),
		Content:        string(content),
		ContentHash:    contentHash,
		FetchedAt:      time.Now(),
		WordCount:      len(strings.Fields(string(content))),
		CharCount:      len(content),
		SourceStrategy: "git",
		RelativePath:   relPath,
	}

	ext := strings.ToLower(filepath.Ext(path))
	if ext != ".md" && ext != ".mdx" {
		doc.Content = "```\n" + string(content) + "\n```"
	}

	if opts.StateManager != nil {
		opts.StateManager.MarkSeen(fileURL)
		if !opts.StateManager.ShouldProcess(fileURL, contentHash) {
			if p.logger != nil {
				p.logger.Debug().Str("file", relPath).Msg("Skipping unchanged file")
			}
			return nil
		}
	}

	if !opts.DryRun && opts.WriteFunc != nil {
		return opts.WriteFunc(ctx, doc)
	}

	return nil
}

func computeHash(content []byte) string {
	hash := sha256.Sum256(content)
	return hex.EncodeToString(hash[:])
}

func ExtractTitleFromPath(path string) string {
	base := filepath.Base(path)
	ext := filepath.Ext(base)
	name := strings.TrimSuffix(base, ext)

	name = strings.ReplaceAll(name, "-", " ")
	name = strings.ReplaceAll(name, "_", " ")

	if len(name) > 0 {
		name = strings.ToUpper(name[:1]) + name[1:]
	}

	return name
}
</file>
<file path="internal/strategies/git/strategy.go">
package git

import (
	"context"
	"fmt"
	"net/http"
	"os"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type StrategyDependencies struct {
	Writer       *output.Writer
	Logger       *utils.Logger
	HTTPClient   *http.Client
	WriteFunc    func(ctx context.Context, doc *domain.Document) error
	StateManager *state.Manager
}

type Strategy struct {
	deps             *StrategyDependencies
	parser           *Parser
	archiveFetcher   *ArchiveFetcher
	cloneFetcher     *CloneFetcher
	processor        *Processor
	logger           *utils.Logger
	httpClient       *http.Client
	skipBranchDetect bool
}

func NewStrategy(deps *StrategyDependencies) *Strategy {
	var client *http.Client
	var skipBranchDetect bool

	if deps == nil {
		client = createDefaultHTTPClient()
		return &Strategy{
			httpClient: client,
			parser:     NewParser(),
		}
	}

	client = deps.HTTPClient
	if client == nil {
		client = createDefaultHTTPClient()
	} else {
		skipBranchDetect = true
	}

	logger := deps.Logger

	return &Strategy{
		deps:   deps,
		parser: NewParser(),
		archiveFetcher: NewArchiveFetcher(ArchiveFetcherOptions{
			HTTPClient: client,
			Logger:     logger,
		}),
		cloneFetcher: NewCloneFetcher(CloneFetcherOptions{
			Logger: logger,
		}),
		processor: NewProcessor(ProcessorOptions{
			Logger: logger,
		}),
		logger:           logger,
		httpClient:       client,
		skipBranchDetect: skipBranchDetect,
	}
}

func (s *Strategy) Name() string {
	return "git"
}

func (s *Strategy) CanHandle(url string) bool {
	lower := strings.ToLower(url)

	isDocsSubdomain := strings.Contains(lower, "docs.github.com") ||
		strings.Contains(lower, "pages.github.io") ||
		strings.Contains(lower, "github.io")

	if isDocsSubdomain {
		return false
	}

	if isWikiURL(url) {
		return false
	}

	return strings.HasPrefix(url, "git@") ||
		strings.HasSuffix(lower, ".git") ||
		(strings.Contains(lower, "github.com") && !strings.Contains(lower, "/blob/")) ||
		(strings.Contains(lower, "gitlab.com") && !strings.Contains(lower, "/-/blob/")) ||
		strings.Contains(lower, "bitbucket.org")
}

type ExecuteOptions struct {
	Output      string
	Concurrency int
	Limit       int
	DryRun      bool
	FilterURL   string
}

func (s *Strategy) Execute(ctx context.Context, rawURL string, opts ExecuteOptions) error {
	if s.logger != nil {
		s.logger.Info().Str("url", rawURL).Msg("Starting git extraction")
	}

	urlInfo, err := s.parser.ParseURLWithPath(rawURL)
	if err != nil {
		return fmt.Errorf("failed to parse git URL: %w", err)
	}

	filterPath := urlInfo.SubPath
	if filterPath == "" && opts.FilterURL != "" {
		filterPath = NormalizeFilterPath(opts.FilterURL)
	}

	if filterPath != "" && s.logger != nil {
		s.logger.Info().Str("filter_path", filterPath).Msg("Path filter active")
	}

	tmpDir, err := os.MkdirTemp("", "repodocs-git-*")
	if err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tmpDir)

	repoURL := urlInfo.RepoURL
	branch, method, err := s.TryArchiveDownload(ctx, repoURL, tmpDir)
	if err != nil {
		if s.logger != nil {
			s.logger.Info().Err(err).Msg("Archive download failed, using git clone")
		}
		branch, err = s.CloneRepository(ctx, repoURL, tmpDir)
		if err != nil {
			return fmt.Errorf("failed to acquire repository: %w", err)
		}
		method = "clone"
	}

	if urlInfo.Branch != "" {
		branch = urlInfo.Branch
	}

	if s.logger != nil {
		s.logger.Info().
			Str("method", method).
			Str("branch", branch).
			Msg("Repository acquired successfully")
	}

	files, err := s.processor.FindDocumentationFiles(tmpDir, filterPath)
	if err != nil {
		return err
	}

	if len(files) == 0 && filterPath != "" {
		return fmt.Errorf("no documentation files found under path: %s", filterPath)
	}

	if s.logger != nil {
		s.logger.Info().Int("count", len(files)).Msg("Found documentation files")
	}

	if opts.Limit > 0 && len(files) > opts.Limit {
		files = files[:opts.Limit]
	}

	processOpts := ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		FilterPath:   filterPath,
		Concurrency:  opts.Concurrency,
		Limit:        opts.Limit,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteFunc,
		StateManager: s.deps.StateManager,
	}

	return s.processor.ProcessFiles(ctx, files, tmpDir, processOpts)
}

func (s *Strategy) TryArchiveDownload(ctx context.Context, url, destDir string) (branch, method string, err error) {
	if strings.HasPrefix(url, "git@") {
		return "", "", fmt.Errorf("SSH URLs not supported for archive download")
	}

	info, err := s.parser.ParseURL(url)
	if err != nil {
		return "", "", err
	}

	if !s.skipBranchDetect {
		branch, err = DetectDefaultBranch(ctx, url)
		if err != nil {
			if s.logger != nil {
				s.logger.Warn().Err(err).Msg("Failed to detect branch, using 'main'")
			}
			branch = "main"
		}
	} else {
		branch = "main"
	}

	result, err := s.archiveFetcher.Fetch(ctx, info, branch, destDir)
	if err != nil {
		if branch == "main" {
			if s.logger != nil {
				s.logger.Debug().Msg("Trying 'master' branch")
			}
			result, err = s.archiveFetcher.Fetch(ctx, info, "master", destDir)
			if err == nil {
				return "master", "archive", nil
			}
		}
		return "", "", err
	}

	return result.Branch, result.Method, nil
}

func (s *Strategy) CloneRepository(ctx context.Context, url, destDir string) (string, error) {
	info := &RepoInfo{URL: url}
	result, err := s.cloneFetcher.Fetch(ctx, info, "", destDir)
	if err != nil {
		return "", err
	}
	return result.Branch, nil
}

func isWikiURL(url string) bool {
	lower := strings.ToLower(url)
	return strings.Contains(lower, "/wiki") ||
		strings.HasSuffix(lower, ".wiki.git")
}

func createDefaultHTTPClient() *http.Client {
	return &http.Client{
		Timeout: 10 * time.Minute,
		CheckRedirect: func(req *http.Request, via []*http.Request) error {
			if len(via) >= 10 {
				return fmt.Errorf("too many redirects")
			}
			return nil
		},
	}
}
</file>
<file path="internal/strategies/git/types.go">
package git

// Platform represents a git hosting platform
type Platform string

const (
	PlatformGitHub    Platform = "github"
	PlatformGitLab    Platform = "gitlab"
	PlatformBitbucket Platform = "bitbucket"
	PlatformGeneric   Platform = "generic"
)

// RepoInfo contains parsed repository information
type RepoInfo struct {
	Platform Platform
	Owner    string
	Repo     string
	URL      string // Original URL
}

// GitURLInfo contains parsed Git URL information including optional path
type GitURLInfo struct {
	RepoURL  string // Clean repository URL (without /tree/... suffix)
	Platform Platform
	Owner    string
	Repo     string
	Branch   string // Branch from URL (empty if not specified)
	SubPath  string // Subdirectory path (empty if root)
}

// FetchResult contains the result of a repository fetch operation
type FetchResult struct {
	LocalPath string // Path to extracted/cloned repo
	Branch    string // Detected or specified branch
	Method    string // "archive" or "clone"
}

// DocumentExtensions are file extensions to process (markdown only)
var DocumentExtensions = map[string]bool{
	".md":  true,
	".mdx": true,
}

// IgnoreDirs are directories to skip during file discovery
var IgnoreDirs = map[string]bool{
	".git":         true,
	"node_modules": true,
	"vendor":       true,
	"__pycache__":  true,
	".venv":        true,
	"venv":         true,
	"dist":         true,
	"build":        true,
	".next":        true,
	".nuxt":        true,
}
</file>
<file path="internal/strategies/crawler.go">
package strategies

import (
	"context"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/gocolly/colly/v2"
	"github.com/schollz/progressbar/v3"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// CrawlerStrategy crawls websites to extract documentation
type CrawlerStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// crawlContext holds shared state between concurrent crawler callbacks.
type crawlContext struct {
	ctx            context.Context
	baseURL        string
	opts           Options
	visited        *sync.Map
	processedCount *int
	mu             *sync.Mutex
	bar            *progressbar.ProgressBar
	barMu          *sync.Mutex
	excludeRegexps []*regexp.Regexp
}

func newCrawlContext(ctx context.Context, baseURL string, opts Options) *crawlContext {
	var excludeRegexps []*regexp.Regexp
	for _, pattern := range opts.Exclude {
		if re, err := regexp.Compile(pattern); err == nil {
			excludeRegexps = append(excludeRegexps, re)
		}
	}

	var processedCount int
	return &crawlContext{
		ctx:            ctx,
		baseURL:        baseURL,
		opts:           opts,
		visited:        &sync.Map{},
		processedCount: &processedCount,
		mu:             &sync.Mutex{},
		bar:            utils.NewProgressBar(-1, utils.DescExtracting),
		barMu:          &sync.Mutex{},
		excludeRegexps: excludeRegexps,
	}
}

func (s *CrawlerStrategy) shouldProcessURL(link, baseURL string, cctx *crawlContext) bool {
	if link == "" {
		return false
	}

	if !utils.IsSameDomain(link, baseURL) {
		return false
	}

	if cctx.opts.FilterURL != "" && !utils.HasBaseURL(link, cctx.opts.FilterURL) {
		return false
	}

	for _, re := range cctx.excludeRegexps {
		if re.MatchString(link) {
			return false
		}
	}

	cctx.mu.Lock()
	if cctx.opts.Limit > 0 && *cctx.processedCount >= cctx.opts.Limit {
		cctx.mu.Unlock()
		return false
	}
	cctx.mu.Unlock()

	if _, exists := cctx.visited.LoadOrStore(link, true); exists {
		return false
	}

	return true
}

func (s *CrawlerStrategy) processMarkdownResponse(body []byte, url string) (*domain.Document, error) {
	doc, err := s.markdownReader.Read(string(body), url)
	if err != nil {
		s.logger.Warn().Err(err).Str("url", url).Msg("Failed to read markdown")
		return nil, err
	}
	return doc, nil
}

func (s *CrawlerStrategy) processHTMLResponse(ctx context.Context, body []byte, url string, opts Options) (*domain.Document, error) {
	html := string(body)

	if opts.RenderJS || renderer.NeedsJSRendering(html) {
		if r, err := s.deps.GetRenderer(); err == nil {
			s.renderer = r
			rendered, err := s.renderer.Render(ctx, url, domain.RenderOptions{
				Timeout:     60 * time.Second,
				WaitStable:  2 * time.Second,
				ScrollToEnd: true,
			})
			if err == nil {
				html = rendered
			}
		}
	}

	doc, err := s.converter.Convert(ctx, html, url)
	if err != nil {
		s.logger.Warn().Err(err).Str("url", url).Msg("Failed to convert page")
		return nil, err
	}

	return doc, nil
}

func (s *CrawlerStrategy) processResponse(ctx context.Context, r *colly.Response, cctx *crawlContext) {
	select {
	case <-ctx.Done():
		return
	default:
	}

	contentType := r.Headers.Get("Content-Type")
	currentURL := r.Request.URL.String()
	isMarkdown := converter.IsMarkdownContent(contentType, currentURL)
	isHTML := IsHTMLContentType(contentType)

	if !isMarkdown && !isHTML {
		return
	}

	cctx.mu.Lock()
	if cctx.opts.Limit > 0 && *cctx.processedCount >= cctx.opts.Limit {
		cctx.mu.Unlock()
		return
	}
	*cctx.processedCount++
	cctx.mu.Unlock()

	cctx.barMu.Lock()
	cctx.bar.Add(1)
	cctx.barMu.Unlock()

	if !cctx.opts.Force && s.writer.Exists(currentURL) {
		return
	}

	var doc *domain.Document
	var err error

	if isMarkdown {
		doc, err = s.processMarkdownResponse(r.Body, currentURL)
	} else {
		doc, err = s.processHTMLResponse(ctx, r.Body, currentURL, cctx.opts)
	}

	if err != nil || doc == nil {
		return
	}

	doc.SourceStrategy = s.Name()
	doc.FetchedAt = time.Now()

	if s.deps.StateManager != nil {
		s.deps.StateManager.MarkSeen(currentURL)
		if doc.ContentHash != "" && !s.deps.StateManager.ShouldProcess(currentURL, doc.ContentHash) {
			s.logger.Debug().Str("url", currentURL).Msg("Skipping unchanged page")
			return
		}
	}

	if !cctx.opts.DryRun {
		if err := s.deps.WriteDocument(ctx, doc); err != nil {
			s.logger.Warn().Err(err).Str("url", currentURL).Msg("Failed to write document")
		}
	}
}

// NewCrawlerStrategy creates a new crawler strategy
func NewCrawlerStrategy(deps *Dependencies) *CrawlerStrategy {
	if deps == nil {
		return &CrawlerStrategy{}
	}
	return &CrawlerStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *CrawlerStrategy) Name() string {
	return "crawler"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *CrawlerStrategy) CanHandle(url string) bool {
	return utils.IsHTTPURL(url)
}

// SetFetcher allows setting a custom fetcher for testing
func (s *CrawlerStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

func (s *CrawlerStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Starting web crawl")

	if opts.FilterURL != "" {
		s.logger.Info().Str("filter", opts.FilterURL).Msg("URL filter active - only crawling URLs under this path")
	}

	cctx := newCrawlContext(ctx, url, opts)

	c := colly.NewCollector(
		colly.Async(true),
		colly.MaxDepth(opts.MaxDepth),
	)

	c.WithTransport(s.fetcher.Transport())

	_ = c.Limit(&colly.LimitRule{
		DomainGlob:  "*",
		Parallelism: opts.Concurrency,
		RandomDelay: 2 * time.Second,
	})

	c.OnHTML("a[href]", func(e *colly.HTMLElement) {
		link := e.Request.AbsoluteURL(e.Attr("href"))
		if s.shouldProcessURL(link, url, cctx) {
			_ = e.Request.Visit(link)
		}
	})

	c.OnResponse(func(r *colly.Response) {
		s.processResponse(ctx, r, cctx)
	})

	c.OnError(func(r *colly.Response, err error) {
		s.logger.Debug().Err(err).Str("url", r.Request.URL.String()).Msg("Request failed")
	})

	if err := c.Visit(url); err != nil {
		return err
	}

	done := make(chan struct{})
	go func() {
		c.Wait()
		close(done)
	}()

	select {
	case <-ctx.Done():
		return ctx.Err()
	case <-done:
	}

	s.logger.Info().Int("pages", *cctx.processedCount).Msg("Crawl completed")
	return nil
}

// IsHTMLContentType checks if content type is HTML
func IsHTMLContentType(contentType string) bool {
	if contentType == "" {
		return true
	}
	lower := strings.ToLower(contentType)
	return strings.Contains(lower, "text/html") ||
		strings.Contains(lower, "application/xhtml")
}
</file>
<file path="internal/strategies/docsrs.go">
package strategies

import (
	"context"
	"fmt"
	"net/url"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type DocsRSURL struct {
	CrateName    string
	Version      string
	ModulePath   string
	IsCratePage  bool
	IsSourceView bool
}

type DocsRSStrategy struct {
	deps     *Dependencies
	fetcher  domain.Fetcher
	writer   *output.Writer
	logger   *utils.Logger
	baseHost string
}

func NewDocsRSStrategy(deps *Dependencies) *DocsRSStrategy {
	if deps == nil {
		return &DocsRSStrategy{baseHost: "docs.rs"}
	}
	return &DocsRSStrategy{
		deps:     deps,
		fetcher:  deps.Fetcher,
		writer:   deps.Writer,
		logger:   deps.Logger,
		baseHost: "docs.rs",
	}
}

func (s *DocsRSStrategy) Name() string {
	return "docsrs"
}

func (s *DocsRSStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

func (s *DocsRSStrategy) SetBaseHost(host string) {
	s.baseHost = host
}

func (s *DocsRSStrategy) parseURL(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPathWithHost(rawURL, s.baseHost)
}

func (s *DocsRSStrategy) CanHandle(rawURL string) bool {
	parsed, err := parseDocsRSPath(rawURL)
	if err != nil {
		return false
	}

	if parsed.IsSourceView {
		return false
	}

	return parsed.CrateName != ""
}

func parseDocsRSPath(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPathWithHost(rawURL, "docs.rs")
}

func parseDocsRSPathWithHost(rawURL, expectedHost string) (*DocsRSURL, error) {
	u, err := url.Parse(rawURL)
	if err != nil {
		return nil, err
	}

	if !strings.Contains(u.Host, expectedHost) {
		return nil, fmt.Errorf("not a docs.rs URL")
	}

	u.Fragment = ""
	u.RawQuery = ""

	segments := strings.Split(strings.Trim(u.Path, "/"), "/")
	if len(segments) == 0 || segments[0] == "" {
		return nil, fmt.Errorf("empty path")
	}

	result := &DocsRSURL{}

	if segments[0] == "crate" {
		result.IsCratePage = true
		if len(segments) >= 2 {
			result.CrateName = segments[1]
		}
		if len(segments) >= 3 {
			result.Version = segments[2]
		} else {
			result.Version = "latest"
		}
		if len(segments) >= 4 && (segments[3] == "source" || segments[3] == "src") {
			result.IsSourceView = true
		}
		return result, nil
	}

	for _, seg := range segments {
		if seg == "src" || seg == "source" {
			result.IsSourceView = true
		}
	}

	result.CrateName = segments[0]

	if len(segments) >= 2 {
		result.Version = segments[1]
	} else {
		result.Version = "latest"
	}

	if len(segments) >= 4 {
		result.ModulePath = strings.Join(segments[3:], "/")
	}

	return result, nil
}

func (s *DocsRSStrategy) Execute(ctx context.Context, rawURL string, opts Options) error {
	s.logger.Info().Str("url", rawURL).Msg("Starting docs.rs JSON extraction")

	if s.fetcher == nil {
		return fmt.Errorf("docsrs strategy fetcher is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("docsrs strategy writer is nil")
	}

	baseInfo, err := s.parseURL(rawURL)
	if err != nil {
		return fmt.Errorf("invalid docs.rs URL: %w", err)
	}

	s.logger.Info().
		Str("crate", baseInfo.CrateName).
		Str("version", baseInfo.Version).
		Msg("Parsed docs.rs URL")

	index, err := s.fetchRustdocJSON(ctx, baseInfo.CrateName, baseInfo.Version)
	if err != nil {
		return fmt.Errorf("failed to fetch rustdoc JSON: %w", err)
	}

	if err := s.checkFormatVersion(index.FormatVersion); err != nil {
		return err
	}

	renderer := NewRustdocRenderer(index, baseInfo.CrateName, baseInfo.Version)

	items := s.collectItems(index, opts)
	s.logger.Info().Int("count", len(items)).Msg("Collected items to process")

	if opts.Limit > 0 && len(items) > opts.Limit {
		items = items[:opts.Limit]
		s.logger.Info().Int("limit", opts.Limit).Msg("Applied item limit")
	}

	bar := utils.NewProgressBar(len(items), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, items, opts.Concurrency, func(ctx context.Context, item *RustdocItem) error {
		defer bar.Add(1)
		return s.processItem(ctx, item, renderer, baseInfo, opts)
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Int("items", len(items)).Msg("docs.rs JSON extraction completed")
	return nil
}

func (s *DocsRSStrategy) processItem(ctx context.Context, item *RustdocItem, renderer *RustdocRenderer, baseInfo *DocsRSURL, opts Options) error {
	itemURL := s.buildItemURL(item, baseInfo)

	if !opts.Force && s.writer.Exists(itemURL) {
		return nil
	}

	markdown := renderer.RenderItem(item)
	if markdown == "" {
		return nil
	}

	doc := &domain.Document{
		URL:            itemURL,
		Title:          s.buildItemTitle(item),
		Content:        markdown,
		Description:    s.buildItemDescription(item, baseInfo),
		SourceStrategy: s.Name(),
		FetchedAt:      time.Now(),
		Tags:           s.buildItemTags(item, baseInfo),
	}

	if !opts.DryRun {
		if err := s.deps.WriteDocument(ctx, doc); err != nil {
			s.logger.Warn().Err(err).Str("url", itemURL).Msg("Failed to write document")
			return nil
		}
	}

	return nil
}

func (s *DocsRSStrategy) buildItemURL(item *RustdocItem, baseInfo *DocsRSURL) string {
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	itemType := ""
	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return fmt.Sprintf("https://docs.rs/%s/%s/%s/",
				baseInfo.CrateName, baseInfo.Version, baseInfo.CrateName)
		}
		itemType = "mod"
	} else if item.GetStruct() != nil {
		itemType = "struct"
	} else if item.GetEnum() != nil {
		itemType = "enum"
	} else if item.GetTrait() != nil {
		itemType = "trait"
	} else if item.GetFunction() != nil {
		itemType = "fn"
	} else if item.GetTypeAlias() != nil {
		itemType = "type"
	} else if item.GetConstant() != nil {
		itemType = "constant"
	} else if item.GetMacro() != nil {
		itemType = "macro"
	} else {
		itemType = "item"
	}

	path := baseInfo.CrateName
	if item.Span != nil && item.Span.Filename != "" {
		spanPath := strings.TrimPrefix(item.Span.Filename, "src/")
		spanPath = strings.TrimSuffix(spanPath, ".rs")
		spanPath = strings.TrimSuffix(spanPath, "/mod")
		if spanPath != "lib" && spanPath != "" {
			path = baseInfo.CrateName + "/" + strings.ReplaceAll(spanPath, "/", "::")
		}
	}

	return fmt.Sprintf("https://docs.rs/%s/%s/%s/%s.%s.html",
		baseInfo.CrateName, baseInfo.Version, path, itemType, name)
}

func (s *DocsRSStrategy) buildItemTitle(item *RustdocItem) string {
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return fmt.Sprintf("Crate %s", name)
		}
		return fmt.Sprintf("Module %s", name)
	}
	if item.GetStruct() != nil {
		return fmt.Sprintf("Struct %s", name)
	}
	if item.GetEnum() != nil {
		return fmt.Sprintf("Enum %s", name)
	}
	if item.GetTrait() != nil {
		return fmt.Sprintf("Trait %s", name)
	}
	if item.GetFunction() != nil {
		return fmt.Sprintf("Function %s", name)
	}
	if item.GetTypeAlias() != nil {
		return fmt.Sprintf("Type %s", name)
	}
	if item.GetMacro() != nil {
		return fmt.Sprintf("Macro %s", name)
	}
	return name
}

func (s *DocsRSStrategy) buildItemDescription(item *RustdocItem, baseInfo *DocsRSURL) string {
	itemType := s.getItemTypeName(item)
	stability := "stable"
	if item.Deprecation != nil {
		stability = "deprecated"
	}

	return fmt.Sprintf("crate:%s version:%s type:%s stability:%s",
		baseInfo.CrateName, baseInfo.Version, itemType, stability)
}

func (s *DocsRSStrategy) buildItemTags(item *RustdocItem, baseInfo *DocsRSURL) []string {
	itemType := s.getItemTypeName(item)

	tags := []string{
		"docs.rs",
		"rust",
		baseInfo.CrateName,
		itemType,
	}

	if item.Deprecation != nil {
		tags = append(tags, "deprecated")
	}

	return tags
}

func (s *DocsRSStrategy) getItemTypeName(item *RustdocItem) string {
	if item.GetModule() != nil {
		return "module"
	}
	if item.GetStruct() != nil {
		return "struct"
	}
	if item.GetEnum() != nil {
		return "enum"
	}
	if item.GetTrait() != nil {
		return "trait"
	}
	if item.GetFunction() != nil {
		return "function"
	}
	if item.GetTypeAlias() != nil {
		return "type"
	}
	if item.GetMacro() != nil {
		return "macro"
	}
	return "item"
}

func ParseDocsRSPathForTest(rawURL string) (*DocsRSURL, error) {
	return parseDocsRSPath(rawURL)
}

// BuildItemURLForTest exposes buildItemURL for testing
func (s *DocsRSStrategy) BuildItemURLForTest(item *RustdocItem, baseInfo *DocsRSURL) string {
	return s.buildItemURL(item, baseInfo)
}

// BuildItemTitleForTest exposes buildItemTitle for testing
func (s *DocsRSStrategy) BuildItemTitleForTest(item *RustdocItem) string {
	return s.buildItemTitle(item)
}

// GetItemTypeNameForTest exposes getItemTypeName for testing
func (s *DocsRSStrategy) GetItemTypeNameForTest(item *RustdocItem) string {
	return s.getItemTypeName(item)
}

// BuildItemDescriptionForTest exposes buildItemDescription for testing
func (s *DocsRSStrategy) BuildItemDescriptionForTest(item *RustdocItem, baseInfo *DocsRSURL) string {
	return s.buildItemDescription(item, baseInfo)
}

// BuildItemTagsForTest exposes buildItemTags for testing
func (s *DocsRSStrategy) BuildItemTagsForTest(item *RustdocItem, baseInfo *DocsRSURL) []string {
	return s.buildItemTags(item, baseInfo)
}
</file>
<file path="internal/strategies/docsrs_json.go">
package strategies

import (
	"context"
	"encoding/json"
	"fmt"
	"strings"

	"github.com/klauspost/compress/zstd"
)

func DocsRSJSONEndpoint(crateName, version string) string {
	return fmt.Sprintf("https://docs.rs/crate/%s/%s/json", crateName, version)
}

func ParseRustdocJSON(data []byte) (*RustdocIndex, error) {
	var index RustdocIndex
	if err := json.Unmarshal(data, &index); err != nil {
		return nil, fmt.Errorf("failed to parse rustdoc JSON: %w", err)
	}
	return &index, nil
}

func (s *DocsRSStrategy) buildJSONEndpoint(crateName, version string) string {
	if s.baseHost != "docs.rs" {
		return fmt.Sprintf("http://%s/crate/%s/%s/json", s.baseHost, crateName, version)
	}
	return DocsRSJSONEndpoint(crateName, version)
}

func (s *DocsRSStrategy) fetchRustdocJSON(ctx context.Context, crateName, version string) (*RustdocIndex, error) {
	endpoint := s.buildJSONEndpoint(crateName, version)

	s.logger.Info().
		Str("crate", crateName).
		Str("version", version).
		Str("endpoint", endpoint).
		Msg("Fetching rustdoc JSON")

	resp, err := s.fetcher.Get(ctx, endpoint)
	if err != nil {
		return nil, fmt.Errorf("failed to fetch rustdoc JSON: %w", err)
	}

	var jsonData []byte
	contentType := resp.ContentType

	// Check for zstd compression via content-type OR magic bytes
	// Magic bytes 0x28 0xB5 0x2F 0xFD indicate zstd compression
	// This handles cached responses where content-type may be lost
	isZstd := strings.Contains(contentType, "zstd") ||
		strings.Contains(contentType, "x-zstd") ||
		strings.HasSuffix(endpoint, ".zst") ||
		(len(resp.Body) >= 4 && resp.Body[0] == 0x28 && resp.Body[1] == 0xB5 && resp.Body[2] == 0x2F && resp.Body[3] == 0xFD)

	if isZstd {
		decoder, err := zstd.NewReader(nil)
		if err != nil {
			return nil, fmt.Errorf("failed to create zstd decoder: %w", err)
		}
		defer decoder.Close()

		jsonData, err = decoder.DecodeAll(resp.Body, nil)
		if err != nil {
			return nil, fmt.Errorf("failed to decompress zstd: %w", err)
		}
	} else {
		jsonData = resp.Body
	}

	s.logger.Debug().
		Int("compressed_size", len(resp.Body)).
		Int("decompressed_size", len(jsonData)).
		Msg("Processed rustdoc JSON")

	index, err := ParseRustdocJSON(jsonData)
	if err != nil {
		return nil, err
	}

	s.logger.Info().
		Int("items", len(index.Index)).
		Int("format_version", index.FormatVersion).
		Str("crate_version", index.CrateVersion).
		Msg("Parsed rustdoc index")

	return index, nil
}

const (
	MinFormatVersion = 30
	MaxFormatVersion = 60
)

func (s *DocsRSStrategy) checkFormatVersion(version int) error {
	if version < MinFormatVersion {
		return fmt.Errorf("rustdoc JSON format version %d is too old (min: %d)", version, MinFormatVersion)
	}
	if version > MaxFormatVersion {
		s.logger.Warn().Int("version", version).Msg("Untested format version, proceeding anyway")
	}
	return nil
}

func (s *DocsRSStrategy) getItemByID(index *RustdocIndex, id interface{}) *RustdocItem {
	switch v := id.(type) {
	case string:
		return index.Index[v]
	case float64:
		return index.Index[fmt.Sprintf("%.0f", v)]
	case int:
		return index.Index[fmt.Sprintf("%d", v)]
	default:
		return nil
	}
}

func (s *DocsRSStrategy) collectItems(index *RustdocIndex, opts Options) []*RustdocItem {
	var items []*RustdocItem

	for _, item := range index.Index {
		if item.CrateID != 0 {
			continue
		}

		if item.Name == nil {
			continue
		}

		if item.Docs == nil && !s.hasDocumentableChildren(item) {
			continue
		}

		if !item.IsPublic() {
			continue
		}

		if item.GetUse() != nil {
			continue
		}

		items = append(items, item)
	}

	return items
}

func (s *DocsRSStrategy) hasDocumentableChildren(item *RustdocItem) bool {
	if mod := item.GetModule(); mod != nil {
		return len(mod.Items) > 0
	}
	if trait := item.GetTrait(); trait != nil {
		return len(trait.Items) > 0
	}
	if st := item.GetStruct(); st != nil {
		return len(st.Impls) > 0
	}
	if en := item.GetEnum(); en != nil {
		return len(en.Variants) > 0 || len(en.Impls) > 0
	}
	return false
}
</file>
<file path="internal/strategies/docsrs_renderer.go">
package strategies

import (
	"fmt"
	"strings"
)

type RustdocRenderer struct {
	index     *RustdocIndex
	crateName string
	version   string
}

func NewRustdocRenderer(index *RustdocIndex, crateName, version string) *RustdocRenderer {
	return &RustdocRenderer{
		index:     index,
		crateName: crateName,
		version:   version,
	}
}

func (r *RustdocRenderer) RenderItem(item *RustdocItem) string {
	var sb strings.Builder

	itemType := r.getItemType(item)
	name := ""
	if item.Name != nil {
		name = *item.Name
	}

	if name != "" {
		sb.WriteString(fmt.Sprintf("# %s `%s`\n\n", itemType, name))
	}

	if item.Deprecation != nil {
		sb.WriteString("> **Deprecated**")
		if item.Deprecation.Since != "" {
			sb.WriteString(fmt.Sprintf(" since %s", item.Deprecation.Since))
		}
		if item.Deprecation.Note != "" {
			sb.WriteString(fmt.Sprintf(": %s", item.Deprecation.Note))
		}
		sb.WriteString("\n\n")
	}

	sig := r.renderSignature(item)
	if sig != "" {
		sb.WriteString("```rust\n")
		sb.WriteString(sig)
		sb.WriteString("\n```\n\n")
	}

	if item.Docs != nil && *item.Docs != "" {
		docs := r.resolveCrossRefs(*item.Docs, item.Links)
		sb.WriteString(docs)
		sb.WriteString("\n\n")
	}

	if mod := item.GetModule(); mod != nil {
		sb.WriteString(r.renderModuleContents(item))
	}

	if trait := item.GetTrait(); trait != nil {
		sb.WriteString(r.renderTraitContents(item))
	}

	if item.GetStruct() != nil || item.GetEnum() != nil {
		sb.WriteString(r.renderImplContents(item))
	}

	return sb.String()
}

func (r *RustdocRenderer) getItemType(item *RustdocItem) string {
	if mod := item.GetModule(); mod != nil {
		if mod.IsCrate {
			return "Crate"
		}
		return "Module"
	}
	if item.GetStruct() != nil {
		return "Struct"
	}
	if item.GetEnum() != nil {
		return "Enum"
	}
	if item.GetTrait() != nil {
		return "Trait"
	}
	if item.GetFunction() != nil {
		return "Function"
	}
	if item.GetTypeAlias() != nil {
		return "Type Alias"
	}
	if item.GetConstant() != nil {
		return "Constant"
	}
	if item.GetMacro() != nil {
		return "Macro"
	}
	if item.GetUse() != nil {
		return "Re-export"
	}
	if item.GetVariant() != nil {
		return "Variant"
	}
	return "Item"
}

func (r *RustdocRenderer) renderSignature(item *RustdocItem) string {
	if item.GetFunction() != nil {
		return r.renderFunctionSignature(item)
	}
	if item.GetTrait() != nil {
		return r.renderTraitSignature(item)
	}
	if item.GetStruct() != nil {
		return r.renderStructSignature(item)
	}
	if item.GetEnum() != nil {
		return r.renderEnumSignature(item)
	}
	if item.GetTypeAlias() != nil {
		return r.renderTypeAliasSignature(item)
	}
	if item.GetConstant() != nil {
		return r.renderConstantSignature(item)
	}
	return ""
}

func (r *RustdocRenderer) renderFunctionSignature(item *RustdocItem) string {
	fn := item.GetFunction()
	if fn == nil {
		return ""
	}

	var sb strings.Builder

	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	if fn.Header != nil {
		if fn.Header.IsConst {
			sb.WriteString("const ")
		}
		if fn.Header.IsAsync {
			sb.WriteString("async ")
		}
		if fn.Header.IsUnsafe {
			sb.WriteString("unsafe ")
		}
	}

	sb.WriteString("fn ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}

	if fn.Generics != nil {
		sb.WriteString(r.renderGenerics(fn.Generics))
	}

	sb.WriteString("(")
	if fn.Sig != nil {
		for i, input := range fn.Sig.Inputs {
			if i > 0 {
				sb.WriteString(", ")
			}
			if arr, ok := input.([]interface{}); ok && len(arr) >= 2 {
				name := fmt.Sprintf("%v", arr[0])
				typeStr := r.RenderType(arr[1])
				if name == "self" {
					sb.WriteString(typeStr)
				} else {
					sb.WriteString(fmt.Sprintf("%s: %s", name, typeStr))
				}
			}
		}
	}
	sb.WriteString(")")

	if fn.Sig != nil && fn.Sig.Output != nil {
		outputStr := r.RenderType(fn.Sig.Output)
		if outputStr != "" && outputStr != "()" {
			sb.WriteString(" -> ")
			sb.WriteString(outputStr)
		}
	}

	if fn.Generics != nil {
		sb.WriteString(r.renderWhereClauses(fn.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) RenderType(t interface{}) string {
	if t == nil {
		return "()"
	}

	switch v := t.(type) {
	case map[string]interface{}:
		return r.RenderTypeMap(v)
	case string:
		return v
	default:
		return fmt.Sprintf("%v", v)
	}
}

func (r *RustdocRenderer) RenderTypeMap(t map[string]interface{}) string {
	if prim, ok := t["primitive"]; ok {
		return fmt.Sprintf("%v", prim)
	}

	if gen, ok := t["generic"]; ok {
		return fmt.Sprintf("%v", gen)
	}

	if resolved, ok := t["resolved_path"].(map[string]interface{}); ok {
		path := fmt.Sprintf("%v", resolved["path"])
		if args := resolved["args"]; args != nil {
			if argsMap, ok := args.(map[string]interface{}); ok {
				if angleArgs, ok := argsMap["angle_bracketed"].(map[string]interface{}); ok {
					if typeArgs, ok := angleArgs["args"].([]interface{}); ok && len(typeArgs) > 0 {
						var argStrs []string
						for _, arg := range typeArgs {
							if argMap, ok := arg.(map[string]interface{}); ok {
								if typeArg, ok := argMap["type"]; ok {
									argStrs = append(argStrs, r.RenderType(typeArg))
								}
							}
						}
						if len(argStrs) > 0 {
							path += "<" + strings.Join(argStrs, ", ") + ">"
						}
					}
				}
			}
		}
		return path
	}

	if borrowed, ok := t["borrowed_ref"].(map[string]interface{}); ok {
		mut := ""
		if borrowed["is_mutable"] == true {
			mut = "mut "
		}
		lifetime := ""
		if l, ok := borrowed["lifetime"].(string); ok && l != "" {
			lifetime = l + " "
		}
		inner := r.RenderType(borrowed["type"])
		return fmt.Sprintf("&%s%s%s", lifetime, mut, inner)
	}

	if slice, ok := t["slice"]; ok {
		return fmt.Sprintf("[%s]", r.RenderType(slice))
	}

	if arr, ok := t["array"].(map[string]interface{}); ok {
		innerType := r.RenderType(arr["type"])
		length := arr["len"]
		return fmt.Sprintf("[%s; %v]", innerType, length)
	}

	if tuple, ok := t["tuple"].([]interface{}); ok {
		if len(tuple) == 0 {
			return "()"
		}
		parts := make([]string, len(tuple))
		for i, elem := range tuple {
			parts[i] = r.RenderType(elem)
		}
		return fmt.Sprintf("(%s)", strings.Join(parts, ", "))
	}

	if rawPtr, ok := t["raw_pointer"].(map[string]interface{}); ok {
		mut := "*const"
		if rawPtr["is_mutable"] == true {
			mut = "*mut"
		}
		inner := r.RenderType(rawPtr["type"])
		return fmt.Sprintf("%s %s", mut, inner)
	}

	if implTrait, ok := t["impl_trait"].([]interface{}); ok {
		var bounds []string
		for _, bound := range implTrait {
			if boundMap, ok := bound.(map[string]interface{}); ok {
				if traitBound, ok := boundMap["trait_bound"].(map[string]interface{}); ok {
					if trait, ok := traitBound["trait"].(map[string]interface{}); ok {
						if path, ok := trait["path"].(string); ok {
							bounds = append(bounds, path)
						}
					}
				}
			}
		}
		if len(bounds) > 0 {
			return "impl " + strings.Join(bounds, " + ")
		}
		return "impl ..."
	}

	if qualPath, ok := t["qualified_path"].(map[string]interface{}); ok {
		name := ""
		if n, ok := qualPath["name"].(string); ok {
			name = n
		}
		return name
	}

	return "..."
}

func (r *RustdocRenderer) renderGenerics(g *RustdocGenerics) string {
	if g == nil || len(g.Params) == 0 {
		return ""
	}

	var parts []string
	for _, p := range g.Params {
		if p.Name != "" && p.Name != "Self" {
			parts = append(parts, p.Name)
		}
	}

	if len(parts) == 0 {
		return ""
	}

	return fmt.Sprintf("<%s>", strings.Join(parts, ", "))
}

func (r *RustdocRenderer) renderWhereClauses(g *RustdocGenerics) string {
	if g == nil || len(g.WherePredicates) == 0 {
		return ""
	}
	return ""
}

func (r *RustdocRenderer) resolveCrossRefs(docs string, links map[string]interface{}) string {
	if links == nil || len(links) == 0 {
		return docs
	}

	result := docs
	for name, id := range links {
		targetItem := r.getItemByID(id)
		if targetItem == nil {
			continue
		}

		targetName := ""
		if targetItem.Name != nil {
			targetName = *targetItem.Name
		}

		targetURL := fmt.Sprintf("https://docs.rs/%s/%s/%s/%s",
			r.crateName, r.version, r.crateName, targetName)

		cleanName := strings.Trim(name, "`")
		result = strings.ReplaceAll(result,
			fmt.Sprintf("[%s]", name),
			fmt.Sprintf("[%s](%s)", cleanName, targetURL))
	}

	return result
}

func (r *RustdocRenderer) getItemByID(id interface{}) *RustdocItem {
	if r.index == nil {
		return nil
	}
	switch v := id.(type) {
	case string:
		return r.index.Index[v]
	case float64:
		return r.index.Index[fmt.Sprintf("%.0f", v)]
	case int:
		return r.index.Index[fmt.Sprintf("%d", v)]
	default:
		return nil
	}
}

func (r *RustdocRenderer) renderModuleContents(item *RustdocItem) string {
	mod := item.GetModule()
	if mod == nil || len(mod.Items) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Contents\n\n")

	groups := make(map[string][]*RustdocItem)
	for _, childID := range mod.Items {
		child := r.getItemByID(childID)
		if child == nil || child.Name == nil {
			continue
		}
		itemType := r.getItemType(child)
		groups[itemType] = append(groups[itemType], child)
	}

	order := []string{"Module", "Struct", "Enum", "Trait", "Function", "Type Alias", "Constant", "Macro"}
	for _, itemType := range order {
		if items, ok := groups[itemType]; ok && len(items) > 0 {
			sb.WriteString(fmt.Sprintf("### %ss\n\n", itemType))
			for _, child := range items {
				if child.Name != nil {
					sb.WriteString(fmt.Sprintf("- `%s`\n", *child.Name))
				}
			}
			sb.WriteString("\n")
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTraitContents(item *RustdocItem) string {
	trait := item.GetTrait()
	if trait == nil || len(trait.Items) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Required Methods\n\n")

	for _, childID := range trait.Items {
		child := r.getItemByID(childID)
		if child == nil {
			continue
		}

		if fn := child.GetFunction(); fn != nil && child.Name != nil {
			sb.WriteString(fmt.Sprintf("### `%s`\n\n", *child.Name))
			sb.WriteString("```rust\n")
			sb.WriteString(r.renderFunctionSignature(child))
			sb.WriteString("\n```\n\n")
			if child.Docs != nil {
				sb.WriteString(*child.Docs)
				sb.WriteString("\n\n")
			}
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderImplContents(item *RustdocItem) string {
	var impls []interface{}
	if st := item.GetStruct(); st != nil {
		impls = st.Impls
	} else if en := item.GetEnum(); en != nil {
		impls = en.Impls
	}

	if len(impls) == 0 {
		return ""
	}

	var sb strings.Builder
	sb.WriteString("## Implementations\n\n")

	for _, implID := range impls {
		implItem := r.getItemByID(implID)
		if implItem == nil {
			continue
		}

		impl := implItem.GetImpl()
		if impl == nil {
			continue
		}

		if impl.Trait != nil {
			if traitPath, ok := impl.Trait.(map[string]interface{}); ok {
				if path, ok := traitPath["path"].(string); ok {
					sb.WriteString(fmt.Sprintf("### impl %s\n\n", path))
				}
			}
		} else {
			sb.WriteString("### impl\n\n")
		}

		for _, methodID := range impl.Items {
			method := r.getItemByID(methodID)
			if method == nil || method.Name == nil {
				continue
			}

			sb.WriteString(fmt.Sprintf("#### `%s`\n\n", *method.Name))
			if fn := method.GetFunction(); fn != nil {
				sb.WriteString("```rust\n")
				sb.WriteString(r.renderFunctionSignature(method))
				sb.WriteString("\n```\n\n")
			}
			if method.Docs != nil {
				sb.WriteString(*method.Docs)
				sb.WriteString("\n\n")
			}
		}
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTraitSignature(item *RustdocItem) string {
	trait := item.GetTrait()
	if trait == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	if trait.IsUnsafe {
		sb.WriteString("unsafe ")
	}
	if trait.IsAuto {
		sb.WriteString("auto ")
	}
	sb.WriteString("trait ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if trait.Generics != nil {
		sb.WriteString(r.renderGenerics(trait.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderStructSignature(item *RustdocItem) string {
	st := item.GetStruct()
	if st == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("struct ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if st.Generics != nil {
		sb.WriteString(r.renderGenerics(st.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderEnumSignature(item *RustdocItem) string {
	en := item.GetEnum()
	if en == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("enum ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if en.Generics != nil {
		sb.WriteString(r.renderGenerics(en.Generics))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderTypeAliasSignature(item *RustdocItem) string {
	ta := item.GetTypeAlias()
	if ta == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("type ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if ta.Generics != nil {
		sb.WriteString(r.renderGenerics(ta.Generics))
	}
	if ta.Type != nil {
		sb.WriteString(" = ")
		sb.WriteString(r.RenderType(ta.Type))
	}

	return sb.String()
}

func (r *RustdocRenderer) renderConstantSignature(item *RustdocItem) string {
	c := item.GetConstant()
	if c == nil {
		return ""
	}

	var sb strings.Builder
	if item.IsPublic() {
		sb.WriteString("pub ")
	}
	sb.WriteString("const ")
	if item.Name != nil {
		sb.WriteString(*item.Name)
	}
	if c.Type != nil {
		sb.WriteString(": ")
		sb.WriteString(r.RenderType(c.Type))
	}

	return sb.String()
}
</file>
<file path="internal/strategies/docsrs_types.go">
package strategies

// RustdocIndex represents the top-level rustdoc JSON structure
type RustdocIndex struct {
	Root            interface{}               `json:"root"` // Can be int or string depending on format version
	CrateVersion    string                    `json:"crate_version"`
	FormatVersion   int                       `json:"format_version"`
	IncludesPrivate bool                      `json:"includes_private"`
	Index           map[string]*RustdocItem   `json:"index"`
	Paths           map[string]*RustdocPath   `json:"paths"`
	ExternalCrates  map[string]*ExternalCrate `json:"external_crates"`
}

// RustdocItem represents a single item in the index
type RustdocItem struct {
	ID          interface{}            `json:"id"` // Can be int or string
	CrateID     int                    `json:"crate_id"`
	Name        *string                `json:"name"` // nullable
	Span        *RustdocSpan           `json:"span"`
	Visibility  interface{}            `json:"visibility"` // Can be string or object
	Docs        *string                `json:"docs"`       // nullable, MARKDOWN!
	Links       map[string]interface{} `json:"links"`      // name -> item ID (int or string)
	Attrs       []interface{}          `json:"attrs"`
	Deprecation *RustdocDeprecation    `json:"deprecation"`
	Inner       map[string]interface{} `json:"inner"` // Dynamic based on item type
}

// RustdocSpan represents source code location
type RustdocSpan struct {
	Filename string `json:"filename"`
	Begin    [2]int `json:"begin"` // [line, column]
	End      [2]int `json:"end"`
}

// RustdocPath represents a path reference
type RustdocPath struct {
	CrateID int         `json:"crate_id"`
	Path    interface{} `json:"path"` // Can be string or []string depending on format version
	Kind    string      `json:"kind"`
}

// RustdocDeprecation represents deprecation info
type RustdocDeprecation struct {
	Since string `json:"since"`
	Note  string `json:"note"`
}

// ExternalCrate represents an external crate reference
type ExternalCrate struct {
	Name        string `json:"name"`
	HTMLRootURL string `json:"html_root_url"`
}

// RustdocModule represents a module item (extracted from inner)
type RustdocModule struct {
	IsCrate    bool          `json:"is_crate"`
	Items      []interface{} `json:"items"` // Can be int or string IDs
	IsStripped bool          `json:"is_stripped"`
}

// RustdocFunction represents a function/method (extracted from inner)
type RustdocFunction struct {
	Sig      *RustdocFunctionSig `json:"sig"`
	Generics *RustdocGenerics    `json:"generics"`
	Header   *RustdocHeader      `json:"header"`
	HasBody  bool                `json:"has_body"`
}

// RustdocFunctionSig represents a function signature
type RustdocFunctionSig struct {
	Inputs     []interface{} `json:"inputs"` // [[name, type], ...]
	Output     interface{}   `json:"output"` // nullable (void), type object
	IsVariadic bool          `json:"is_c_variadic"`
}

// RustdocGenerics represents generic parameters
type RustdocGenerics struct {
	Params          []RustdocGenericParam `json:"params"`
	WherePredicates []interface{}         `json:"where_predicates"`
}

// RustdocGenericParam represents a generic parameter
type RustdocGenericParam struct {
	Name string      `json:"name"`
	Kind interface{} `json:"kind"`
}

// RustdocHeader represents function header attributes
type RustdocHeader struct {
	IsConst  bool   `json:"is_const"`
	IsUnsafe bool   `json:"is_unsafe"`
	IsAsync  bool   `json:"is_async"`
	ABI      string `json:"abi"`
}

// RustdocTrait represents a trait (extracted from inner)
type RustdocTrait struct {
	IsAuto          bool             `json:"is_auto"`
	IsUnsafe        bool             `json:"is_unsafe"`
	IsDynCompatible bool             `json:"is_dyn_compatible"`
	Items           []interface{}    `json:"items"` // Can be int or string IDs
	Generics        *RustdocGenerics `json:"generics"`
	Bounds          []interface{}    `json:"bounds"`
	Implementations []interface{}    `json:"implementations"` // Can be int or string IDs
}

// RustdocStruct represents a struct (extracted from inner)
type RustdocStruct struct {
	Kind     interface{}      `json:"kind"` // "unit", "tuple", or struct fields
	Generics *RustdocGenerics `json:"generics"`
	Impls    []interface{}    `json:"impls"` // Can be int or string IDs
}

// RustdocEnum represents an enum (extracted from inner)
type RustdocEnum struct {
	Variants         []interface{}    `json:"variants"` // Can be int or string IDs
	Generics         *RustdocGenerics `json:"generics"`
	Impls            []interface{}    `json:"impls"` // Can be int or string IDs
	VariantsStripped bool             `json:"variants_stripped"`
}

// RustdocImpl represents an impl block (extracted from inner)
type RustdocImpl struct {
	IsUnsafe        bool             `json:"is_unsafe"`
	Generics        *RustdocGenerics `json:"generics"`
	ProvidedMethods []string         `json:"provided_trait_methods"`
	Trait           interface{}      `json:"trait"` // nullable, RustdocPath-like
	For             interface{}      `json:"for"`   // Type
	Items           []interface{}    `json:"items"` // Can be int or string IDs
	IsNegative      bool             `json:"is_negative"`
	IsSynthetic     bool             `json:"is_synthetic"`
	BlanketImpl     interface{}      `json:"blanket_impl"` // nullable, Type
}

// RustdocUse represents a re-export (use statement)
type RustdocUse struct {
	Source string      `json:"source"`
	Name   string      `json:"name"`
	ID     interface{} `json:"id"` // nullable if external
	IsGlob bool        `json:"is_glob"`
}

// RustdocTypeAlias represents a type alias (extracted from inner)
type RustdocTypeAlias struct {
	Type     interface{}      `json:"type"`
	Generics *RustdocGenerics `json:"generics"`
}

// RustdocConstant represents a constant (extracted from inner)
type RustdocConstant struct {
	Type   interface{} `json:"type"`
	Const_ interface{} `json:"const"` // The constant expression/value
}

// RustdocStatic represents a static variable (extracted from inner)
type RustdocStatic struct {
	Type      interface{} `json:"type"`
	IsMutable bool        `json:"mutable"`
	Expr      string      `json:"expr"`
}

// RustdocMacro represents a macro (extracted from inner)
type RustdocMacro struct {
	Macro string `json:"macro"`
}

// RustdocAssocType represents an associated type in a trait
type RustdocAssocType struct {
	Generics *RustdocGenerics `json:"generics"`
	Bounds   []interface{}    `json:"bounds"`
	Type     interface{}      `json:"type"` // Default type if any
}

// RustdocAssocConst represents an associated constant in a trait
type RustdocAssocConst struct {
	Type  interface{} `json:"type"`
	Value *string     `json:"value"` // Default value if any
}

// RustdocVariant represents an enum variant (extracted from inner)
type RustdocVariant struct {
	Kind         interface{} `json:"kind"` // "plain", "tuple", "struct"
	Discriminant interface{} `json:"discriminant"`
}

// Helper functions to extract typed inner values

// GetModule extracts module data from an item's inner field
func (item *RustdocItem) GetModule() *RustdocModule {
	if item.Inner == nil {
		return nil
	}
	if moduleData, ok := item.Inner["module"]; ok {
		return parseModule(moduleData)
	}
	return nil
}

// GetFunction extracts function data from an item's inner field
func (item *RustdocItem) GetFunction() *RustdocFunction {
	if item.Inner == nil {
		return nil
	}
	if fnData, ok := item.Inner["function"]; ok {
		return parseFunction(fnData)
	}
	return nil
}

// GetTrait extracts trait data from an item's inner field
func (item *RustdocItem) GetTrait() *RustdocTrait {
	if item.Inner == nil {
		return nil
	}
	if traitData, ok := item.Inner["trait"]; ok {
		return parseTrait(traitData)
	}
	return nil
}

// GetStruct extracts struct data from an item's inner field
func (item *RustdocItem) GetStruct() *RustdocStruct {
	if item.Inner == nil {
		return nil
	}
	if structData, ok := item.Inner["struct"]; ok {
		return parseStruct(structData)
	}
	return nil
}

// GetEnum extracts enum data from an item's inner field
func (item *RustdocItem) GetEnum() *RustdocEnum {
	if item.Inner == nil {
		return nil
	}
	if enumData, ok := item.Inner["enum"]; ok {
		return parseEnum(enumData)
	}
	return nil
}

// GetImpl extracts impl data from an item's inner field
func (item *RustdocItem) GetImpl() *RustdocImpl {
	if item.Inner == nil {
		return nil
	}
	if implData, ok := item.Inner["impl"]; ok {
		return parseImpl(implData)
	}
	return nil
}

// GetUse extracts use/re-export data from an item's inner field
func (item *RustdocItem) GetUse() *RustdocUse {
	if item.Inner == nil {
		return nil
	}
	if useData, ok := item.Inner["use"]; ok {
		return parseUse(useData)
	}
	return nil
}

// GetTypeAlias extracts type alias data from an item's inner field
func (item *RustdocItem) GetTypeAlias() *RustdocTypeAlias {
	if item.Inner == nil {
		return nil
	}
	if taData, ok := item.Inner["type_alias"]; ok {
		return parseTypeAlias(taData)
	}
	return nil
}

// GetConstant extracts constant data from an item's inner field
func (item *RustdocItem) GetConstant() *RustdocConstant {
	if item.Inner == nil {
		return nil
	}
	if constData, ok := item.Inner["constant"]; ok {
		return parseConstant(constData)
	}
	return nil
}

// GetMacro extracts macro data from an item's inner field
func (item *RustdocItem) GetMacro() *RustdocMacro {
	if item.Inner == nil {
		return nil
	}
	if macroData, ok := item.Inner["macro"]; ok {
		return parseMacro(macroData)
	}
	return nil
}

// GetVariant extracts variant data from an item's inner field
func (item *RustdocItem) GetVariant() *RustdocVariant {
	if item.Inner == nil {
		return nil
	}
	if variantData, ok := item.Inner["variant"]; ok {
		return parseVariant(variantData)
	}
	return nil
}

// IsPublic returns true if the item has public visibility
func (item *RustdocItem) IsPublic() bool {
	if item.Visibility == nil {
		return false
	}
	switch v := item.Visibility.(type) {
	case string:
		return v == "public"
	case map[string]interface{}:
		// Restricted visibility like pub(crate)
		return false
	default:
		return false
	}
}

// GetItemType returns the type of item based on the inner field
func (item *RustdocItem) GetItemType() string {
	if item.Inner == nil {
		return "unknown"
	}
	for key := range item.Inner {
		return key
	}
	return "unknown"
}

// Parser helper functions

func parseModule(data interface{}) *RustdocModule {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	mod := &RustdocModule{}
	if v, ok := m["is_crate"].(bool); ok {
		mod.IsCrate = v
	}
	if v, ok := m["items"].([]interface{}); ok {
		mod.Items = v
	}
	if v, ok := m["is_stripped"].(bool); ok {
		mod.IsStripped = v
	}
	return mod
}

func parseFunction(data interface{}) *RustdocFunction {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	fn := &RustdocFunction{}
	if sigData, ok := m["sig"].(map[string]interface{}); ok {
		fn.Sig = parseFunctionSig(sigData)
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		fn.Generics = parseGenerics(genData)
	}
	if headerData, ok := m["header"].(map[string]interface{}); ok {
		fn.Header = parseHeader(headerData)
	}
	if v, ok := m["has_body"].(bool); ok {
		fn.HasBody = v
	}
	return fn
}

func parseFunctionSig(data map[string]interface{}) *RustdocFunctionSig {
	sig := &RustdocFunctionSig{}
	if v, ok := data["inputs"].([]interface{}); ok {
		sig.Inputs = v
	}
	sig.Output = data["output"]
	if v, ok := data["is_c_variadic"].(bool); ok {
		sig.IsVariadic = v
	}
	return sig
}

func parseGenerics(data map[string]interface{}) *RustdocGenerics {
	gen := &RustdocGenerics{}
	if params, ok := data["params"].([]interface{}); ok {
		for _, p := range params {
			if pm, ok := p.(map[string]interface{}); ok {
				param := RustdocGenericParam{}
				if name, ok := pm["name"].(string); ok {
					param.Name = name
				}
				param.Kind = pm["kind"]
				gen.Params = append(gen.Params, param)
			}
		}
	}
	if wp, ok := data["where_predicates"].([]interface{}); ok {
		gen.WherePredicates = wp
	}
	return gen
}

func parseHeader(data map[string]interface{}) *RustdocHeader {
	header := &RustdocHeader{}
	if v, ok := data["is_const"].(bool); ok {
		header.IsConst = v
	}
	if v, ok := data["is_unsafe"].(bool); ok {
		header.IsUnsafe = v
	}
	if v, ok := data["is_async"].(bool); ok {
		header.IsAsync = v
	}
	if v, ok := data["abi"].(string); ok {
		header.ABI = v
	}
	return header
}

func parseTrait(data interface{}) *RustdocTrait {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	trait := &RustdocTrait{}
	if v, ok := m["is_auto"].(bool); ok {
		trait.IsAuto = v
	}
	if v, ok := m["is_unsafe"].(bool); ok {
		trait.IsUnsafe = v
	}
	if v, ok := m["is_dyn_compatible"].(bool); ok {
		trait.IsDynCompatible = v
	}
	if v, ok := m["items"].([]interface{}); ok {
		trait.Items = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		trait.Generics = parseGenerics(genData)
	}
	if v, ok := m["bounds"].([]interface{}); ok {
		trait.Bounds = v
	}
	if v, ok := m["implementations"].([]interface{}); ok {
		trait.Implementations = v
	}
	return trait
}

func parseStruct(data interface{}) *RustdocStruct {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	s := &RustdocStruct{}
	s.Kind = m["kind"]
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		s.Generics = parseGenerics(genData)
	}
	if v, ok := m["impls"].([]interface{}); ok {
		s.Impls = v
	}
	return s
}

func parseEnum(data interface{}) *RustdocEnum {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	e := &RustdocEnum{}
	if v, ok := m["variants"].([]interface{}); ok {
		e.Variants = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		e.Generics = parseGenerics(genData)
	}
	if v, ok := m["impls"].([]interface{}); ok {
		e.Impls = v
	}
	if v, ok := m["variants_stripped"].(bool); ok {
		e.VariantsStripped = v
	}
	return e
}

func parseImpl(data interface{}) *RustdocImpl {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	impl := &RustdocImpl{}
	if v, ok := m["is_unsafe"].(bool); ok {
		impl.IsUnsafe = v
	}
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		impl.Generics = parseGenerics(genData)
	}
	if v, ok := m["provided_trait_methods"].([]interface{}); ok {
		for _, s := range v {
			if str, ok := s.(string); ok {
				impl.ProvidedMethods = append(impl.ProvidedMethods, str)
			}
		}
	}
	impl.Trait = m["trait"]
	impl.For = m["for"]
	if v, ok := m["items"].([]interface{}); ok {
		impl.Items = v
	}
	if v, ok := m["is_negative"].(bool); ok {
		impl.IsNegative = v
	}
	if v, ok := m["is_synthetic"].(bool); ok {
		impl.IsSynthetic = v
	}
	impl.BlanketImpl = m["blanket_impl"]
	return impl
}

func parseUse(data interface{}) *RustdocUse {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	use := &RustdocUse{}
	if v, ok := m["source"].(string); ok {
		use.Source = v
	}
	if v, ok := m["name"].(string); ok {
		use.Name = v
	}
	use.ID = m["id"]
	if v, ok := m["is_glob"].(bool); ok {
		use.IsGlob = v
	}
	return use
}

func parseTypeAlias(data interface{}) *RustdocTypeAlias {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	ta := &RustdocTypeAlias{}
	ta.Type = m["type"]
	if genData, ok := m["generics"].(map[string]interface{}); ok {
		ta.Generics = parseGenerics(genData)
	}
	return ta
}

func parseConstant(data interface{}) *RustdocConstant {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	c := &RustdocConstant{}
	c.Type = m["type"]
	c.Const_ = m["const"]
	return c
}

func parseMacro(data interface{}) *RustdocMacro {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	macro := &RustdocMacro{}
	if v, ok := m["macro"].(string); ok {
		macro.Macro = v
	}
	return macro
}

func parseVariant(data interface{}) *RustdocVariant {
	m, ok := data.(map[string]interface{})
	if !ok {
		return nil
	}
	v := &RustdocVariant{}
	v.Kind = m["kind"]
	v.Discriminant = m["discriminant"]
	return v
}
</file>
<file path="internal/strategies/git_compat.go">
package strategies

import (
	"context"
	"io"
	"net/http"

	"github.com/quantmind-br/repodocs-go/internal/strategies/git"
)

var DocumentExtensions = git.DocumentExtensions
var IgnoreDirs = git.IgnoreDirs

type repoInfo = git.RepoInfo

type gitURLInfo = git.GitURLInfo

type GitStrategy struct {
	strategy       *git.Strategy
	deps           *Dependencies
	parser         *git.Parser
	archiveFetcher *git.ArchiveFetcher
	processor      *git.Processor
	httpClient     *http.Client
}

func NewGitStrategy(deps *Dependencies) *GitStrategy {
	var gitDeps *git.StrategyDependencies
	var httpClient *http.Client

	if deps != nil {
		gitDeps = &git.StrategyDependencies{
			Writer:       deps.Writer,
			Logger:       deps.Logger,
			HTTPClient:   deps.HTTPClient,
			WriteFunc:    deps.WriteDocument,
			StateManager: deps.StateManager,
		}
		httpClient = deps.HTTPClient
	}

	if httpClient == nil {
		httpClient = http.DefaultClient
	}

	var logger = deps.Logger
	if deps == nil {
		logger = nil
	}

	return &GitStrategy{
		strategy: git.NewStrategy(gitDeps),
		deps:     deps,
		parser:   git.NewParser(),
		archiveFetcher: git.NewArchiveFetcher(git.ArchiveFetcherOptions{
			HTTPClient: httpClient,
			Logger:     logger,
		}),
		processor: git.NewProcessor(git.ProcessorOptions{
			Logger: logger,
		}),
		httpClient: httpClient,
	}
}

func (s *GitStrategy) Name() string {
	return s.strategy.Name()
}

func (s *GitStrategy) CanHandle(url string) bool {
	return s.strategy.CanHandle(url)
}

func (s *GitStrategy) Execute(ctx context.Context, rawURL string, opts Options) error {
	gitOpts := git.ExecuteOptions{
		Output:      opts.Output,
		Concurrency: opts.Concurrency,
		Limit:       opts.Limit,
		DryRun:      opts.DryRun,
		FilterURL:   opts.FilterURL,
	}
	return s.strategy.Execute(ctx, rawURL, gitOpts)
}

func (s *GitStrategy) detectDefaultBranch(ctx context.Context, url string) (string, error) {
	return git.DetectDefaultBranch(ctx, url)
}

func (s *GitStrategy) buildArchiveURL(info *repoInfo, branch string) string {
	return s.archiveFetcher.BuildArchiveURL(info, branch)
}

func (s *GitStrategy) downloadAndExtract(ctx context.Context, archiveURL, destDir string) error {
	return s.archiveFetcher.DownloadAndExtract(ctx, archiveURL, destDir)
}

func (s *GitStrategy) extractTarGz(r io.Reader, destDir string) error {
	return s.archiveFetcher.ExtractTarGz(r, destDir)
}

func (s *GitStrategy) findDocumentationFiles(dir string, filterPath string) ([]string, error) {
	return s.processor.FindDocumentationFiles(dir, filterPath)
}

func (s *GitStrategy) processFiles(ctx context.Context, files []string, tmpDir, repoURL, branch string, opts Options) error {
	processOpts := git.ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		Concurrency:  opts.Concurrency,
		Limit:        opts.Limit,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteDocument,
		StateManager: s.deps.StateManager,
	}
	return s.processor.ProcessFiles(ctx, files, tmpDir, processOpts)
}

func (s *GitStrategy) processFile(ctx context.Context, path, tmpDir, repoURL, branch string, opts Options) error {
	processOpts := git.ProcessOptions{
		RepoURL:      repoURL,
		Branch:       branch,
		DryRun:       opts.DryRun,
		WriteFunc:    s.deps.WriteDocument,
		StateManager: s.deps.StateManager,
	}
	return s.processor.ProcessFile(ctx, path, tmpDir, processOpts)
}

func (s *GitStrategy) parseGitURLWithPath(rawURL string) (*gitURLInfo, error) {
	return s.parser.ParseURLWithPath(rawURL)
}

func (s *GitStrategy) tryArchiveDownload(ctx context.Context, url, destDir string) (branch, method string, err error) {
	return s.strategy.TryArchiveDownload(ctx, url, destDir)
}

func (s *GitStrategy) cloneRepository(ctx context.Context, url, destDir string) (string, error) {
	return s.strategy.CloneRepository(ctx, url, destDir)
}

func normalizeFilterPath(path string) string {
	return git.NormalizeFilterPath(path)
}

func extractTitleFromPath(path string) string {
	return git.ExtractTitleFromPath(path)
}
</file>
<file path="internal/strategies/github_pages.go">
package strategies

import (
	"context"
	"fmt"
	"net/url"
	"regexp"
	"strings"
	"sync"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// GitHubPagesStrategy extracts documentation from GitHub Pages sites (*.github.io)
type GitHubPagesStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// NewGitHubPagesStrategy creates a new GitHub Pages strategy
func NewGitHubPagesStrategy(deps *Dependencies) *GitHubPagesStrategy {
	if deps == nil {
		return &GitHubPagesStrategy{
			markdownReader: converter.NewMarkdownReader(),
		}
	}
	return &GitHubPagesStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *GitHubPagesStrategy) Name() string {
	return "github_pages"
}

// CanHandle returns true if URL is a GitHub Pages site
func (s *GitHubPagesStrategy) CanHandle(rawURL string) bool {
	return IsGitHubPagesURL(rawURL)
}

// IsGitHubPagesURL checks if a URL is a GitHub Pages site
func IsGitHubPagesURL(rawURL string) bool {
	parsed, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	host := strings.ToLower(parsed.Host)
	return strings.HasSuffix(host, ".github.io")
}

// Execute runs the GitHub Pages extraction strategy
func (s *GitHubPagesStrategy) Execute(ctx context.Context, inputURL string, opts Options) error {
	s.logger.Info().
		Str("url", inputURL).
		Msg("Starting GitHub Pages extraction")

	// Normalize base URL
	baseURL, err := s.normalizeBaseURL(inputURL)
	if err != nil {
		return fmt.Errorf("invalid URL: %w", err)
	}

	// Phase 1: Discover URLs (HTTP-first, browser fallback)
	urls, discoveryMethod, err := s.discoverURLs(ctx, baseURL, opts)
	if err != nil {
		return fmt.Errorf("URL discovery failed: %w", err)
	}

	if len(urls) == 0 {
		s.logger.Warn().Msg("No URLs discovered")
		return nil
	}

	s.logger.Info().
		Int("count", len(urls)).
		Str("method", discoveryMethod).
		Msg("URLs discovered")

	// Filter and deduplicate URLs
	urls = FilterAndDeduplicateURLs(urls, baseURL)

	// Apply filters and limits
	urls = s.filterURLs(urls, baseURL, opts)
	if opts.Limit > 0 && len(urls) > opts.Limit {
		urls = urls[:opts.Limit]
	}

	s.logger.Info().
		Int("count", len(urls)).
		Msg("Processing URLs")

	// Phase 2: Extract content (HTTP-first, browser fallback)
	return s.processURLs(ctx, urls, opts)
}

// discoverURLs finds all URLs using multi-tier discovery
func (s *GitHubPagesStrategy) discoverURLs(ctx context.Context, baseURL string, opts Options) ([]string, string, error) {
	// Tier 1: Try HTTP probes sequentially
	urls, method, err := s.discoverViaHTTPProbes(ctx, baseURL)
	if err == nil && len(urls) > 0 {
		return urls, method, nil
	}

	s.logger.Debug().Err(err).Msg("HTTP discovery failed, falling back to browser crawl")

	renderer, rendererErr := s.deps.GetRenderer()
	if rendererErr != nil {
		return nil, "", fmt.Errorf("no URLs found via HTTP probes and browser renderer failed: %w", rendererErr)
	}
	s.renderer = renderer

	urls, err = s.discoverViaBrowser(ctx, baseURL, opts)
	if err != nil {
		return nil, "", fmt.Errorf("browser discovery failed: %w", err)
	}

	return urls, "browser-crawl", nil
}

// discoverViaHTTPProbes tries all HTTP-based discovery methods
func (s *GitHubPagesStrategy) discoverViaHTTPProbes(ctx context.Context, baseURL string) ([]string, string, error) {
	probes := GetDiscoveryProbes()

	for _, probe := range probes {
		select {
		case <-ctx.Done():
			return nil, "", ctx.Err()
		default:
		}

		probeURL := strings.TrimSuffix(baseURL, "/") + probe.Path

		resp, err := s.fetcher.Get(ctx, probeURL)
		if err != nil {
			s.logger.Debug().Str("probe", probe.Name).Str("url", probeURL).Err(err).Msg("Probe failed")
			continue
		}

		if resp.StatusCode != 200 {
			s.logger.Debug().Str("probe", probe.Name).Int("status", resp.StatusCode).Msg("Probe returned non-200")
			continue
		}

		urls, err := probe.Parser(resp.Body, baseURL)
		if err != nil {
			s.logger.Debug().Str("probe", probe.Name).Err(err).Msg("Failed to parse probe response")
			continue
		}

		if len(urls) > 0 {
			s.logger.Info().
				Str("probe", probe.Name).
				Int("urls", len(urls)).
				Msg("Discovery probe succeeded")
			return urls, probe.Name, nil
		}
	}

	return nil, "", fmt.Errorf("all HTTP probes failed")
}

// discoverViaBrowser uses browser rendering to crawl and discover URLs
func (s *GitHubPagesStrategy) discoverViaBrowser(ctx context.Context, baseURL string, opts Options) ([]string, error) {
	visited := make(map[string]bool)
	toVisit := []string{baseURL}
	var discovered []string

	maxDepth := opts.MaxDepth
	if maxDepth <= 0 {
		maxDepth = 3
	}

	for depth := 0; depth < maxDepth && len(toVisit) > 0; depth++ {
		var nextLevel []string

		for _, pageURL := range toVisit {
			if visited[pageURL] {
				continue
			}
			visited[pageURL] = true
			discovered = append(discovered, pageURL)

			// Check limit during discovery
			if opts.Limit > 0 && len(discovered) >= opts.Limit*2 {
				return discovered, nil
			}

			// Render page and extract links
			links, err := s.extractLinksFromRenderedPage(ctx, pageURL, baseURL)
			if err != nil {
				s.logger.Debug().Err(err).Str("url", pageURL).Msg("Failed to extract links")
				continue
			}

			for _, link := range links {
				if !visited[link] {
					nextLevel = append(nextLevel, link)
				}
			}
		}

		toVisit = nextLevel
		s.logger.Debug().Int("depth", depth+1).Int("queued", len(toVisit)).Msg("Browser crawl depth completed")
	}

	return discovered, nil
}

// extractLinksFromRenderedPage renders a page and extracts internal links
func (s *GitHubPagesStrategy) extractLinksFromRenderedPage(ctx context.Context, pageURL, baseURL string) ([]string, error) {
	html, err := s.renderPage(ctx, pageURL)
	if err != nil {
		return nil, err
	}

	return s.extractLinksWithGoquery(html, baseURL)
}

// extractLinksWithGoquery uses goquery for robust HTML parsing
func (s *GitHubPagesStrategy) extractLinksWithGoquery(html, baseURL string) ([]string, error) {
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return nil, err
	}

	parsedBase, _ := url.Parse(baseURL)
	baseHost := parsedBase.Host

	seen := make(map[string]bool)
	var links []string

	// Extract from navigation elements first (most relevant)
	selectors := []string{
		"nav a[href]",
		"[role='navigation'] a[href]",
		".sidebar a[href]",
		".menu a[href]",
		".nav a[href]",
		".toc a[href]",
		"a[href]",
	}

	for _, selector := range selectors {
		doc.Find(selector).Each(func(_ int, sel *goquery.Selection) {
			href, exists := sel.Attr("href")
			if !exists || href == "" {
				return
			}

			// Skip non-navigable links
			if strings.HasPrefix(href, "#") ||
				strings.HasPrefix(href, "javascript:") ||
				strings.HasPrefix(href, "mailto:") ||
				strings.HasPrefix(href, "tel:") {
				return
			}

			// Resolve relative URLs
			resolved, err := url.Parse(href)
			if err != nil {
				return
			}

			if !resolved.IsAbs() {
				resolved = parsedBase.ResolveReference(resolved)
			}

			// Filter to same host
			if resolved.Host != baseHost {
				return
			}

			// Normalize: remove fragment, trailing slash
			resolved.Fragment = ""
			normalized := resolved.String()
			normalized = strings.TrimSuffix(normalized, "/")

			if !seen[normalized] && !ShouldSkipGitHubPagesURL(normalized) {
				seen[normalized] = true
				links = append(links, normalized)
			}
		})

		// If we found links in nav elements, prefer those
		if len(links) > 10 && selector != "a[href]" {
			break
		}
	}

	return links, nil
}

// renderPage renders a page using the browser
func (s *GitHubPagesStrategy) renderPage(ctx context.Context, pageURL string) (string, error) {
	return s.renderer.Render(ctx, pageURL, domain.RenderOptions{
		Timeout:     90 * time.Second,
		WaitStable:  3 * time.Second,
		ScrollToEnd: true,
	})
}

// filterURLs applies filter and exclude patterns
func (s *GitHubPagesStrategy) filterURLs(urls []string, baseURL string, opts Options) []string {
	var excludeRegexps []*regexp.Regexp
	for _, pattern := range opts.Exclude {
		if re, err := regexp.Compile(pattern); err == nil {
			excludeRegexps = append(excludeRegexps, re)
		}
	}

	var filtered []string
	for _, u := range urls {
		// Apply base URL filter
		if opts.FilterURL != "" && !strings.HasPrefix(u, opts.FilterURL) {
			continue
		}

		// Apply exclude patterns
		excluded := false
		for _, re := range excludeRegexps {
			if re.MatchString(u) {
				excluded = true
				break
			}
		}
		if excluded {
			continue
		}

		// Skip non-content URLs
		if ShouldSkipGitHubPagesURL(u) {
			continue
		}

		filtered = append(filtered, u)
	}

	return filtered
}

// processURLs processes all URLs using HTTP-first extraction with browser fallback
func (s *GitHubPagesStrategy) processURLs(ctx context.Context, urls []string, opts Options) error {
	bar := utils.NewProgressBar(len(urls), utils.DescExtracting)

	// Limit browser concurrency for stability
	concurrency := opts.Concurrency
	if concurrency > 5 {
		concurrency = 5
	}
	if concurrency <= 0 {
		concurrency = 2
	}

	var mu sync.Mutex
	var processedCount, successCount int

	errors := utils.ParallelForEach(ctx, urls, concurrency, func(ctx context.Context, pageURL string) error {
		defer func() {
			mu.Lock()
			bar.Add(1)
			processedCount++
			mu.Unlock()
		}()

		// Check if already exists
		if !opts.Force && s.writer.Exists(pageURL) {
			return nil
		}

		// HTTP-first fetch with browser fallback
		html, usedBrowser, err := s.fetchOrRenderPage(ctx, pageURL, opts)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to fetch/render page")
			return nil
		}

		// Validate content
		if s.isEmptyOrErrorContent(html) {
			s.logger.Debug().Str("url", pageURL).Msg("Empty or error content, skipping")
			return nil
		}

		// Convert HTML to document
		doc, err := s.converter.Convert(ctx, html, pageURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to convert page")
			return nil
		}

		// Validate converted content
		if len(strings.TrimSpace(doc.Content)) < 50 {
			s.logger.Debug().Str("url", pageURL).Msg("Converted content too short, skipping")
			return nil
		}

		// Set metadata
		doc.SourceStrategy = s.Name()
		doc.FetchedAt = time.Now()
		doc.RenderedWithJS = usedBrowser

		// Write document
		if !opts.DryRun {
			if err := s.deps.WriteDocument(ctx, doc); err != nil {
				s.logger.Warn().Err(err).Str("url", pageURL).Msg("Failed to write document")
				return nil
			}
			mu.Lock()
			successCount++
			mu.Unlock()
		}

		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().
		Int("processed", processedCount).
		Int("success", successCount).
		Int("total", len(urls)).
		Msg("GitHub Pages extraction completed")

	return nil
}

// fetchOrRenderPage attempts HTTP fetch first, falls back to browser rendering if needed
func (s *GitHubPagesStrategy) fetchOrRenderPage(ctx context.Context, pageURL string, opts Options) (string, bool, error) {
	// Try HTTP fetch first (unless RenderJS is forced)
	if !opts.RenderJS {
		resp, err := s.fetcher.Get(ctx, pageURL)
		if err == nil && resp.StatusCode == 200 {
			html := string(resp.Body)

			// Check if content looks like a valid page (not SPA shell)
			if !s.looksLikeSPAShell(html) && !renderer.NeedsJSRendering(html) {
				return html, false, nil
			}

			s.logger.Debug().Str("url", pageURL).Msg("Content appears to be SPA shell, using browser")
		}
	}

	r, err := s.deps.GetRenderer()
	if err != nil {
		return "", false, fmt.Errorf("browser renderer not available: %w", err)
	}
	s.renderer = r

	rendered, err := s.renderPage(ctx, pageURL)
	if err != nil {
		return "", false, err
	}

	return rendered, true, nil
}

// looksLikeSPAShell checks if HTML looks like an empty SPA shell
func (s *GitHubPagesStrategy) looksLikeSPAShell(html string) bool {
	// Check for minimal content indicators
	if len(html) < 500 {
		return true
	}

	lower := strings.ToLower(html)

	// Check for empty body or just script tags
	spaIndicators := []string{
		`<div id="app"></div>`,
		`<div id="root"></div>`,
		`<div id="__next"></div>`,
		`<div id="__nuxt"></div>`,
		"<body></body>",
		"<body> </body>",
	}

	for _, indicator := range spaIndicators {
		if strings.Contains(lower, indicator) {
			// Check if there's actual content after the indicator
			doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
			if err != nil {
				return true
			}

			bodyText := strings.TrimSpace(doc.Find("body").Text())
			if len(bodyText) < 100 {
				return true
			}
		}
	}

	return false
}

// isEmptyOrErrorContent checks if rendered content is empty or an error page
func (s *GitHubPagesStrategy) isEmptyOrErrorContent(html string) bool {
	if len(html) < 300 {
		return true
	}

	lower := strings.ToLower(html)
	errorIndicators := []string{
		"301 moved permanently",
		"302 found",
		"404 not found",
		"page not found",
		"access denied",
		"403 forbidden",
	}

	for _, indicator := range errorIndicators {
		if strings.Contains(lower, indicator) {
			return true
		}
	}

	// Check for minimal content (just boilerplate)
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(html))
	if err != nil {
		return true
	}

	// Check if body has meaningful text content
	bodyText := strings.TrimSpace(doc.Find("body").Text())
	if len(bodyText) < 60 {
		return true
	}

	return false
}

// normalizeBaseURL normalizes the input URL to a base URL
func (s *GitHubPagesStrategy) normalizeBaseURL(inputURL string) (string, error) {
	parsed, err := url.Parse(inputURL)
	if err != nil {
		return "", err
	}

	if parsed.Scheme == "" {
		parsed.Scheme = "https"
	}

	// Keep the path for project subpaths (e.g., /goose/)
	base := fmt.Sprintf("%s://%s", parsed.Scheme, parsed.Host)

	if parsed.Path != "" && parsed.Path != "/" {
		// Clean path: remove trailing slash, keep structure
		path := strings.TrimSuffix(parsed.Path, "/")
		base += path
	}

	return base, nil
}
</file>
<file path="internal/strategies/github_pages_discovery.go">
package strategies

import (
	"encoding/json"
	"encoding/xml"
	"fmt"
	"net/url"
	"strings"
)

// SitemapXMLForDiscovery represents the XML structure of a sitemap (for discovery)
type SitemapXMLForDiscovery struct {
	XMLName xml.Name                 `xml:"urlset"`
	URLs    []SitemapURLForDiscovery `xml:"url"`
}

// SitemapURLForDiscovery represents a URL entry in a sitemap
type SitemapURLForDiscovery struct {
	Loc string `xml:"loc"`
}

// SitemapIndexXMLForDiscovery represents the XML structure of a sitemap index
type SitemapIndexXMLForDiscovery struct {
	XMLName  xml.Name                      `xml:"sitemapindex"`
	Sitemaps []SitemapLocationForDiscovery `xml:"sitemap"`
}

// SitemapLocationForDiscovery represents a sitemap location in an index
type SitemapLocationForDiscovery struct {
	Loc string `xml:"loc"`
}

// DiscoveryProbe defines a URL discovery mechanism for GitHub Pages sites
type DiscoveryProbe struct {
	Path   string
	Parser func(content []byte, baseURL string) ([]string, error)
	Name   string
}

// GetDiscoveryProbes returns all discovery probes in priority order
func GetDiscoveryProbes() []DiscoveryProbe {
	return []DiscoveryProbe{
		// Tier 1: LLM-optimized (highest quality)
		{"/llms.txt", ParseLLMsTxt, "llms.txt"},

		// Tier 2: Sitemaps (most common)
		{"/sitemap.xml", ParseSitemapXML, "sitemap.xml"},
		{"/sitemap-0.xml", ParseSitemapXML, "sitemap-0.xml"},
		{"/sitemap_index.xml", ParseSitemapIndexXML, "sitemap_index.xml"},

		// Tier 3: MkDocs (very reliable)
		{"/search/search_index.json", ParseMkDocsIndex, "mkdocs-search"},

		// Tier 4: Docusaurus
		{"/search-index.json", ParseDocusaurusIndex, "docusaurus-search"},

		// Tier 5: Hugo / Generic
		{"/index.json", ParseHugoIndex, "hugo-index"},
		{"/search.json", ParseGenericSearchIndex, "search.json"},

		// Tier 6: Modern SSGs
		{"/hashmap.json", ParseVitePressHashmap, "vitepress"},
	}
}

// ParseLLMsTxt parses llms.txt format (markdown links)
func ParseLLMsTxt(content []byte, baseURL string) ([]string, error) {
	// Use the parseLLMSLinks function from llms.go (same package)
	links := parseLLMSLinks(string(content), baseURL)
	if len(links) == 0 {
		return nil, fmt.Errorf("no links found in llms.txt")
	}

	urls := make([]string, 0, len(links))
	for _, link := range links {
		urls = append(urls, link.URL)
	}
	return urls, nil
}

// FilterAndDeduplicateURLs filters URLs to the same host and deduplicates them
func FilterAndDeduplicateURLs(urls []string, baseURL string) []string {
	parsed, err := url.Parse(baseURL)
	if err != nil {
		return urls
	}
	baseHost := parsed.Host

	seen := make(map[string]bool)
	var result []string

	for _, u := range urls {
		parsedURL, err := url.Parse(u)
		if err != nil {
			continue
		}

		// Filter to same host
		if parsedURL.Host != "" && parsedURL.Host != baseHost {
			continue
		}

		// Normalize: remove fragment, trailing slash
		parsedURL.Fragment = ""
		normalized := parsedURL.String()
		normalized = strings.TrimSuffix(normalized, "/")

		if !seen[normalized] {
			seen[normalized] = true
			result = append(result, normalized)
		}
	}

	return result
}

// ShouldSkipGitHubPagesURL returns true for URLs that typically don't contain documentation
func ShouldSkipGitHubPagesURL(u string) bool {
	lower := strings.ToLower(u)
	skipPatterns := []string{
		"/assets/", "/static/", "/_next/", "/_nuxt/",
		"/img/", "/images/", "/media/",
		"/css/", "/js/", "/fonts/",
		".png", ".jpg", ".jpeg", ".gif", ".svg", ".ico", ".webp",
		".css", ".js", ".woff", ".woff2", ".ttf", ".eot",
		".pdf", ".zip", ".tar", ".gz",
		"/feed.xml", "/rss.xml", "/atom.xml",
	}

	for _, pattern := range skipPatterns {
		if strings.Contains(lower, pattern) {
			return true
		}
	}
	return false
}

// ParseSitemapXML parses standard sitemap.xml format
func ParseSitemapXML(content []byte, baseURL string) ([]string, error) {
	var sitemap SitemapXMLForDiscovery
	if err := xml.Unmarshal(content, &sitemap); err != nil {
		return nil, err
	}

	if len(sitemap.URLs) == 0 {
		return nil, fmt.Errorf("empty sitemap")
	}

	urls := make([]string, 0, len(sitemap.URLs))
	for _, u := range sitemap.URLs {
		if u.Loc != "" {
			urls = append(urls, u.Loc)
		}
	}
	return urls, nil
}

// ParseSitemapIndexXML parses sitemap index and fetches nested sitemaps
func ParseSitemapIndexXML(content []byte, baseURL string) ([]string, error) {
	var index SitemapIndexXMLForDiscovery
	if err := xml.Unmarshal(content, &index); err != nil {
		return nil, err
	}

	if len(index.Sitemaps) == 0 {
		return nil, fmt.Errorf("empty sitemap index")
	}

	// Return sitemap URLs for the caller to process
	urls := make([]string, 0, len(index.Sitemaps))
	for _, sm := range index.Sitemaps {
		urls = append(urls, sm.Loc)
	}
	return urls, nil
}

// MkDocsSearchIndex represents MkDocs search_index.json structure
type MkDocsSearchIndex struct {
	Docs []struct {
		Location string `json:"location"`
		Title    string `json:"title"`
		Text     string `json:"text"`
	} `json:"docs"`
}

// ParseMkDocsIndex parses MkDocs /search/search_index.json
func ParseMkDocsIndex(content []byte, baseURL string) ([]string, error) {
	var index MkDocsSearchIndex
	if err := json.Unmarshal(content, &index); err != nil {
		return nil, err
	}

	if len(index.Docs) == 0 {
		return nil, fmt.Errorf("empty MkDocs index")
	}

	seen := make(map[string]bool)
	var urls []string

	for _, doc := range index.Docs {
		loc := strings.Split(doc.Location, "#")[0]
		if loc == "" || loc == "." {
			loc = ""
		}

		fullURL := strings.TrimSuffix(baseURL, "/") + "/" + strings.TrimPrefix(loc, "/")

		if !seen[fullURL] {
			seen[fullURL] = true
			urls = append(urls, fullURL)
		}
	}

	return urls, nil
}

// DocusaurusSearchEntry represents a Docusaurus search index entry
type DocusaurusSearchEntry struct {
	URL     string `json:"url"`
	Title   string `json:"title"`
	Content string `json:"content"`
}

// ParseDocusaurusIndex parses Docusaurus /search-index.json
func ParseDocusaurusIndex(content []byte, baseURL string) ([]string, error) {
	var entries []DocusaurusSearchEntry
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty Docusaurus index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		if entry.URL != "" {
			urls = append(urls, resolveDiscoveryURL(entry.URL, baseURL))
		}
	}
	return urls, nil
}

// HugoSearchEntry represents a Hugo search index entry
type HugoSearchEntry struct {
	Permalink string `json:"permalink"`
	URL       string `json:"url"`
	Title     string `json:"title"`
}

// ParseHugoIndex parses Hugo /index.json
func ParseHugoIndex(content []byte, baseURL string) ([]string, error) {
	var entries []HugoSearchEntry
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty Hugo index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		urlStr := entry.Permalink
		if urlStr == "" {
			urlStr = entry.URL
		}
		if urlStr != "" {
			urls = append(urls, resolveDiscoveryURL(urlStr, baseURL))
		}
	}
	return urls, nil
}

// ParseGenericSearchIndex parses generic search.json format
func ParseGenericSearchIndex(content []byte, baseURL string) ([]string, error) {
	// Try array format first
	var entries []map[string]interface{}
	if err := json.Unmarshal(content, &entries); err != nil {
		return nil, err
	}

	if len(entries) == 0 {
		return nil, fmt.Errorf("empty search index")
	}

	urls := make([]string, 0, len(entries))
	for _, entry := range entries {
		// Try common URL field names
		for _, field := range []string{"url", "permalink", "href", "location", "path"} {
			if val, ok := entry[field].(string); ok && val != "" {
				urls = append(urls, resolveDiscoveryURL(val, baseURL))
				break
			}
		}
	}

	if len(urls) == 0 {
		return nil, fmt.Errorf("no URLs found in search index")
	}
	return urls, nil
}

// ParseVitePressHashmap parses VitePress hashmap.json
func ParseVitePressHashmap(content []byte, baseURL string) ([]string, error) {
	var hashmap map[string]string
	if err := json.Unmarshal(content, &hashmap); err != nil {
		return nil, err
	}

	if len(hashmap) == 0 {
		return nil, fmt.Errorf("empty VitePress hashmap")
	}

	urls := make([]string, 0, len(hashmap))
	for path := range hashmap {
		// VitePress hashmap keys are like "guide_getting-started.md"
		// Convert to URL path: /guide/getting-started
		urlPath := strings.ReplaceAll(path, "_", "/")
		urlPath = strings.TrimSuffix(urlPath, ".md")

		fullURL := strings.TrimSuffix(baseURL, "/") + "/" + urlPath
		urls = append(urls, fullURL)
	}
	return urls, nil
}

// resolveDiscoveryURL resolves a potentially relative URL against a base URL
func resolveDiscoveryURL(href, baseURL string) string {
	if strings.HasPrefix(href, "http://") || strings.HasPrefix(href, "https://") {
		return href
	}

	parsed, err := url.Parse(baseURL)
	if err != nil {
		return baseURL + "/" + strings.TrimPrefix(href, "/")
	}

	ref, err := url.Parse(href)
	if err != nil {
		return baseURL + "/" + strings.TrimPrefix(href, "/")
	}

	return parsed.ResolveReference(ref).String()
}
</file>
<file path="internal/strategies/llms.go">
package strategies

import (
	"context"
	"fmt"
	"regexp"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

type LLMSStrategy struct {
	deps            *Dependencies
	fetcher         domain.Fetcher
	converter       *converter.Pipeline
	markdownReader  *converter.MarkdownReader
	plainTextReader *converter.PlainTextReader
	writer          *output.Writer
	logger          *utils.Logger
}

func NewLLMSStrategy(deps *Dependencies) *LLMSStrategy {
	if deps == nil {
		return &LLMSStrategy{
			markdownReader:  converter.NewMarkdownReader(),
			plainTextReader: converter.NewPlainTextReader(),
		}
	}
	return &LLMSStrategy{
		deps:            deps,
		fetcher:         deps.Fetcher,
		converter:       deps.Converter,
		markdownReader:  converter.NewMarkdownReader(),
		plainTextReader: converter.NewPlainTextReader(),
		writer:          deps.Writer,
		logger:          deps.Logger,
	}
}

// Name returns the strategy name
func (s *LLMSStrategy) Name() string {
	return "llms"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *LLMSStrategy) CanHandle(url string) bool {
	// Only handle HTTP/HTTPS URLs
	if !strings.HasPrefix(url, "http://") && !strings.HasPrefix(url, "https://") {
		return false
	}

	lowerURL := strings.ToLower(url)
	return strings.HasSuffix(lowerURL, "/llms.txt") || strings.HasSuffix(lowerURL, "llms.txt")
}

// Execute runs the LLMS extraction strategy
func (s *LLMSStrategy) Execute(ctx context.Context, url string, opts Options) error {
	// Check context cancellation early
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	if s.fetcher == nil {
		return fmt.Errorf("llms strategy fetcher is nil")
	}
	if s.converter == nil {
		return fmt.Errorf("llms strategy converter is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("llms strategy writer is nil")
	}
	if s.logger == nil {
		return fmt.Errorf("llms strategy logger is nil")
	}

	s.logger.Info().Str("url", url).Msg("Fetching llms.txt")

	if opts.FilterURL != "" {
		s.logger.Info().Str("filter", opts.FilterURL).Msg("URL filter active - only downloading URLs under this path")
	}

	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	links := parseLLMSLinks(string(resp.Body), url)
	s.logger.Info().Int("count", len(links)).Msg("Found links in llms.txt")

	if opts.FilterURL != "" {
		links = filterLLMSLinks(links, opts.FilterURL)
		s.logger.Info().Int("count", len(links)).Str("filter", opts.FilterURL).Msg("Links after filter")
	}

	if opts.Limit > 0 && len(links) > opts.Limit {
		links = links[:opts.Limit]
	}

	// Create progress bar
	bar := utils.NewProgressBar(len(links), utils.DescExtracting)

	// Process links concurrently
	errors := utils.ParallelForEach(ctx, links, opts.Concurrency, func(ctx context.Context, link domain.LLMSLink) error {
		defer bar.Add(1)

		// Check if already exists
		if !opts.Force && s.writer.Exists(link.URL) {
			return nil
		}

		// Fetch page
		pageResp, err := s.fetcher.Get(ctx, link.URL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to fetch page")
			return nil // Continue with other pages
		}

		var doc *domain.Document
		if converter.IsMarkdownContent(pageResp.ContentType, link.URL) {
			doc, err = s.markdownReader.Read(string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to read markdown")
				return nil
			}
		} else if converter.IsPlainTextContent(pageResp.ContentType, link.URL) {
			doc, err = s.plainTextReader.Read(string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to read plain text")
				return nil
			}
		} else {
			doc, err = s.converter.Convert(ctx, string(pageResp.Body), link.URL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to convert page")
				return nil
			}
		}

		// Set metadata
		doc.SourceStrategy = s.Name()
		doc.CacheHit = pageResp.FromCache
		doc.FetchedAt = time.Now()

		// Use title from llms.txt if document title is empty
		if doc.Title == "" && link.Title != "" {
			doc.Title = link.Title
		}

		if !opts.DryRun {
			if s.deps != nil {
				if err := s.deps.WriteDocument(ctx, doc); err != nil {
					s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to write document")
					return nil
				}
			} else {
				if err := s.writer.Write(ctx, doc); err != nil {
					s.logger.Warn().Err(err).Str("url", link.URL).Msg("Failed to write document")
					return nil
				}
			}
		}

		return nil
	})

	// Check for errors
	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Msg("LLMS extraction completed")
	return nil
}

// linkRegex matches markdown links: [Title](url)
var linkRegex = regexp.MustCompile(`\[([^\]]+)\]\(([^)]+)\)`)

func parseLLMSLinks(content string, baseURL string) []domain.LLMSLink {
	links := make([]domain.LLMSLink, 0)

	matches := linkRegex.FindAllStringSubmatch(content, -1)
	for _, match := range matches {
		if len(match) == 3 {
			title := strings.TrimSpace(match[1])
			rawURL := strings.TrimSpace(match[2])

			if rawURL == "" || strings.HasPrefix(rawURL, "#") {
				continue
			}

			// Resolve relative URLs against base URL
			resolvedURL := rawURL
			if !strings.HasPrefix(rawURL, "http://") && !strings.HasPrefix(rawURL, "https://") {
				resolved, err := utils.ResolveURL(baseURL, rawURL)
				if err != nil {
					// Skip URLs that can't be resolved
					continue
				}
				resolvedURL = resolved
			}

			links = append(links, domain.LLMSLink{
				Title: title,
				URL:   resolvedURL,
			})
		}
	}

	return links
}

func filterLLMSLinks(links []domain.LLMSLink, filterURL string) []domain.LLMSLink {
	// Empty filter means no filtering - return all
	if filterURL == "" {
		return links
	}

	filtered := make([]domain.LLMSLink, 0, len(links))
	for _, link := range links {
		// Try HasBaseURL first (works with full URLs)
		if utils.HasBaseURL(link.URL, filterURL) {
			filtered = append(filtered, link)
			continue
		}

		// For path-only filters (e.g., "/docs"), check if URL path contains the filter
		if strings.HasPrefix(filterURL, "/") && strings.Contains(link.URL, filterURL) {
			filtered = append(filtered, link)
		}
	}
	return filtered
}
</file>
<file path="internal/strategies/pkggo.go">
package strategies

import (
	"context"
	"fmt"
	"strings"
	"time"

	"github.com/PuerkitoBio/goquery"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// PkgGoStrategy extracts documentation from pkg.go.dev
type PkgGoStrategy struct {
	deps      *Dependencies
	fetcher   domain.Fetcher
	converter *converter.Pipeline
	writer    *output.Writer
	logger    *utils.Logger
}

// NewPkgGoStrategy creates a new pkg.go.dev strategy
func NewPkgGoStrategy(deps *Dependencies) *PkgGoStrategy {
	if deps == nil {
		return &PkgGoStrategy{}
	}
	return &PkgGoStrategy{
		deps:      deps,
		fetcher:   deps.Fetcher,
		converter: deps.Converter,
		writer:    deps.Writer,
		logger:    deps.Logger,
	}
}

// Name returns the strategy name
func (s *PkgGoStrategy) Name() string {
	return "pkggo"
}

// SetFetcher allows setting a custom fetcher (used for testing)
func (s *PkgGoStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

// CanHandle returns true if this strategy can handle the given URL
func (s *PkgGoStrategy) CanHandle(url string) bool {
	return strings.Contains(url, "pkg.go.dev")
}

// Execute runs the pkg.go.dev extraction strategy
func (s *PkgGoStrategy) Execute(ctx context.Context, url string, opts Options) error {
	// Check context cancellation early
	select {
	case <-ctx.Done():
		return ctx.Err()
	default:
	}

	if s.fetcher == nil {
		return fmt.Errorf("pkggo strategy fetcher is nil")
	}
	if s.converter == nil {
		return fmt.Errorf("pkggo strategy converter is nil")
	}
	if s.writer == nil {
		return fmt.Errorf("pkggo strategy writer is nil")
	}
	if s.logger == nil {
		return fmt.Errorf("pkggo strategy logger is nil")
	}

	s.logger.Info().Str("url", url).Msg("Fetching pkg.go.dev documentation")

	// Fetch page
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	// Parse HTML
	doc, err := goquery.NewDocumentFromReader(strings.NewReader(string(resp.Body)))
	if err != nil {
		return err
	}

	// Extract package name
	packageName := doc.Find("h1.UnitHeader-title").First().Text()
	packageName = strings.TrimSpace(packageName)

	// If split option is enabled, extract sections separately
	if opts.Split {
		return s.extractSections(ctx, doc, url, packageName, opts)
	}

	// Extract main documentation content
	content := doc.Find("div.Documentation-content").First()
	if content.Length() == 0 {
		// Fallback to main content area
		content = doc.Find("main").First()
	}

	contentHTML, err := content.Html()
	if err != nil {
		return err
	}

	// Convert to document
	document, err := s.converter.Convert(ctx, contentHTML, url)
	if err != nil {
		return err
	}

	// Set metadata
	document.Title = packageName
	document.SourceStrategy = s.Name()
	document.CacheHit = resp.FromCache
	document.FetchedAt = time.Now()

	if !opts.DryRun {
		if s.deps != nil {
			return s.deps.WriteDocument(ctx, document)
		}
		return s.writer.Write(ctx, document)
	}

	return nil
}

// extractSections extracts documentation split by sections
func (s *PkgGoStrategy) extractSections(ctx context.Context, doc *goquery.Document, baseURL, packageName string, opts Options) error {
	sections := []struct {
		selector string
		name     string
	}{
		{"#pkg-overview", "Overview"},
		{"#pkg-index", "Index"},
		{"#pkg-constants", "Constants"},
		{"#pkg-variables", "Variables"},
		{"#pkg-functions", "Functions"},
		{"#pkg-types", "Types"},
	}

	for _, section := range sections {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		content := doc.Find(section.selector).First()
		if content.Length() == 0 {
			continue
		}

		// Get section HTML
		sectionHTML, err := content.Html()
		if err != nil {
			continue
		}

		// Skip empty sections
		if strings.TrimSpace(sectionHTML) == "" {
			continue
		}

		// Create section URL
		sectionURL := baseURL + section.selector

		// Convert to document
		document, err := s.converter.Convert(ctx, sectionHTML, sectionURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to convert section")
			continue
		}

		// Set metadata
		document.Title = packageName + " - " + section.name
		document.SourceStrategy = s.Name()
		document.FetchedAt = time.Now()

		if !opts.DryRun {
			if s.deps != nil {
				if err := s.deps.WriteDocument(ctx, document); err != nil {
					s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to write section")
				}
			} else {
				if err := s.writer.Write(ctx, document); err != nil {
					s.logger.Warn().Err(err).Str("section", section.name).Msg("Failed to write section")
				}
			}
		}
	}

	s.logger.Info().Msg("pkg.go.dev extraction completed")
	return nil
}
</file>
<file path="internal/strategies/sitemap.go">
package strategies

import (
	"compress/gzip"
	"context"
	"encoding/xml"
	"io"
	"sort"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// SitemapStrategy extracts documentation from sitemap XML files
type SitemapStrategy struct {
	deps           *Dependencies
	fetcher        domain.Fetcher
	renderer       domain.Renderer
	converter      *converter.Pipeline
	markdownReader *converter.MarkdownReader
	writer         *output.Writer
	logger         *utils.Logger
}

// NewSitemapStrategy creates a new sitemap strategy
func NewSitemapStrategy(deps *Dependencies) *SitemapStrategy {
	if deps == nil {
		return &SitemapStrategy{
			markdownReader: converter.NewMarkdownReader(),
		}
	}
	return &SitemapStrategy{
		deps:           deps,
		fetcher:        deps.Fetcher,
		renderer:       deps.Renderer,
		converter:      deps.Converter,
		markdownReader: converter.NewMarkdownReader(),
		writer:         deps.Writer,
		logger:         deps.Logger,
	}
}

// Name returns the strategy name
func (s *SitemapStrategy) Name() string {
	return "sitemap"
}

// SetFetcher allows setting a custom fetcher (used for testing)
func (s *SitemapStrategy) SetFetcher(f domain.Fetcher) {
	s.fetcher = f
}

// CanHandle returns true if this strategy can handle the given URL
func (s *SitemapStrategy) CanHandle(url string) bool {
	lower := strings.ToLower(url)
	return strings.HasSuffix(lower, "sitemap.xml") ||
		strings.HasSuffix(lower, "sitemap.xml.gz") ||
		strings.Contains(lower, "sitemap")
}

// Execute runs the sitemap extraction strategy
func (s *SitemapStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Fetching sitemap")

	// Fetch sitemap
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return err
	}

	// Decompress if gzipped
	content := resp.Body
	if strings.HasSuffix(strings.ToLower(url), ".gz") {
		content, err = decompressGzip(resp.Body)
		if err != nil {
			return err
		}
	}

	// Parse sitemap
	sitemap, err := parseSitemap(content, url)
	if err != nil {
		return err
	}

	// If it's a sitemap index, process each sitemap
	if sitemap.IsIndex {
		return s.processSitemapIndex(ctx, sitemap, opts)
	}

	// Sort by lastmod (most recent first)
	sortURLsByLastMod(sitemap.URLs)

	// Apply limit
	urls := sitemap.URLs
	if opts.Limit > 0 && len(urls) > opts.Limit {
		urls = urls[:opts.Limit]
	}

	s.logger.Info().Int("count", len(urls)).Msg("Processing URLs from sitemap")

	return s.processURLs(ctx, urls, opts)
}

// processSitemapIndex processes a sitemap index file by collecting all URLs first,
// then processing them with a single progress bar for consistent display.
func (s *SitemapStrategy) processSitemapIndex(ctx context.Context, sitemap *domain.Sitemap, opts Options) error {
	s.logger.Info().Int("count", len(sitemap.Sitemaps)).Msg("Processing sitemap index")

	// Collect all URLs from nested sitemaps first
	var allURLs []domain.SitemapURL
	for _, sitemapURL := range sitemap.Sitemaps {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		urls, err := s.collectURLsFromSitemap(ctx, sitemapURL)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", sitemapURL).Msg("Failed to fetch nested sitemap")
			continue
		}
		allURLs = append(allURLs, urls...)
	}

	if len(allURLs) == 0 {
		s.logger.Warn().Msg("No URLs found in sitemap index")
		return nil
	}

	// Sort by lastmod (most recent first)
	sortURLsByLastMod(allURLs)

	// Apply limit
	if opts.Limit > 0 && len(allURLs) > opts.Limit {
		allURLs = allURLs[:opts.Limit]
	}

	s.logger.Info().Int("count", len(allURLs)).Msg("Processing URLs from sitemap index")

	// Process all URLs with a single progress bar
	return s.processURLs(ctx, allURLs, opts)
}

// collectURLsFromSitemap fetches and parses a sitemap, returning its URLs.
// For sitemap indexes, it recursively collects URLs from all nested sitemaps.
func (s *SitemapStrategy) collectURLsFromSitemap(ctx context.Context, url string) ([]domain.SitemapURL, error) {
	resp, err := s.fetcher.Get(ctx, url)
	if err != nil {
		return nil, err
	}

	// Decompress if gzipped
	content := resp.Body
	if strings.HasSuffix(strings.ToLower(url), ".gz") {
		content, err = decompressGzip(resp.Body)
		if err != nil {
			return nil, err
		}
	}

	// Parse sitemap
	sitemap, err := parseSitemap(content, url)
	if err != nil {
		return nil, err
	}

	// If it's a nested index, recursively collect URLs
	if sitemap.IsIndex {
		var allURLs []domain.SitemapURL
		for _, nestedURL := range sitemap.Sitemaps {
			select {
			case <-ctx.Done():
				return nil, ctx.Err()
			default:
			}

			urls, err := s.collectURLsFromSitemap(ctx, nestedURL)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", nestedURL).Msg("Failed to fetch nested sitemap")
				continue
			}
			allURLs = append(allURLs, urls...)
		}
		return allURLs, nil
	}

	return sitemap.URLs, nil
}

func (s *SitemapStrategy) processURLs(ctx context.Context, urls []domain.SitemapURL, opts Options) error {
	bar := utils.NewProgressBar(len(urls), utils.DescExtracting)

	errors := utils.ParallelForEach(ctx, urls, opts.Concurrency, func(ctx context.Context, sitemapURL domain.SitemapURL) error {
		defer bar.Add(1)

		if !opts.Force && s.writer.Exists(sitemapURL.Loc) {
			return nil
		}

		pageResp, err := s.fetcher.Get(ctx, sitemapURL.Loc)
		if err != nil {
			s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to fetch page")
			return nil
		}

		var doc *domain.Document
		if converter.IsMarkdownContent(pageResp.ContentType, sitemapURL.Loc) {
			doc, err = s.markdownReader.Read(string(pageResp.Body), sitemapURL.Loc)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to read markdown")
				return nil
			}
		} else {
			html := string(pageResp.Body)

			if opts.RenderJS || renderer.NeedsJSRendering(html) {
				if r, err := s.deps.GetRenderer(); err == nil {
					s.renderer = r
					rendered, err := s.renderer.Render(ctx, sitemapURL.Loc, domain.RenderOptions{
						Timeout:     60 * time.Second,
						WaitStable:  2 * time.Second,
						ScrollToEnd: true,
					})
					if err == nil {
						html = rendered
					}
				}
			}

			doc, err = s.converter.Convert(ctx, html, sitemapURL.Loc)
			if err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to convert page")
				return nil
			}
		}

		doc.SourceStrategy = s.Name()
		doc.CacheHit = pageResp.FromCache
		doc.FetchedAt = time.Now()

		if !opts.DryRun {
			if err := s.deps.WriteDocument(ctx, doc); err != nil {
				s.logger.Warn().Err(err).Str("url", sitemapURL.Loc).Msg("Failed to write document")
				return nil
			}
		}

		return nil
	})

	if err := utils.FirstError(errors); err != nil {
		return err
	}

	s.logger.Info().Msg("Sitemap extraction completed")
	return nil
}

// sitemapXML represents the XML structure of a sitemap
type sitemapXML struct {
	XMLName xml.Name     `xml:"urlset"`
	URLs    []sitemapURL `xml:"url"`
}

type sitemapURL struct {
	Loc        string `xml:"loc"`
	LastMod    string `xml:"lastmod"`
	ChangeFreq string `xml:"changefreq"`
	Priority   string `xml:"priority"`
}

// sitemapIndexXML represents the XML structure of a sitemap index
type sitemapIndexXML struct {
	XMLName  xml.Name          `xml:"sitemapindex"`
	Sitemaps []sitemapLocation `xml:"sitemap"`
}

type sitemapLocation struct {
	Loc     string `xml:"loc"`
	LastMod string `xml:"lastmod"`
}

// parseSitemap parses sitemap XML content
func parseSitemap(content []byte, sourceURL string) (*domain.Sitemap, error) {
	// Try to parse as sitemap index first
	var index sitemapIndexXML
	if err := xml.Unmarshal(content, &index); err == nil && len(index.Sitemaps) > 0 {
		var sitemaps []string
		for _, sm := range index.Sitemaps {
			sitemaps = append(sitemaps, sm.Loc)
		}
		return &domain.Sitemap{
			IsIndex:   true,
			Sitemaps:  sitemaps,
			SourceURL: sourceURL,
		}, nil
	}

	// Parse as regular sitemap
	var sitemap sitemapXML
	if err := xml.Unmarshal(content, &sitemap); err != nil {
		return nil, err
	}

	var urls []domain.SitemapURL
	for _, u := range sitemap.URLs {
		lastMod, _ := parseLastMod(u.LastMod)
		urls = append(urls, domain.SitemapURL{
			Loc:        u.Loc,
			LastMod:    lastMod,
			LastModStr: u.LastMod,
			ChangeFreq: u.ChangeFreq,
		})
	}

	return &domain.Sitemap{
		URLs:      urls,
		IsIndex:   false,
		SourceURL: sourceURL,
	}, nil
}

// parseLastMod parses a lastmod date string
func parseLastMod(s string) (time.Time, error) {
	formats := []string{
		time.RFC3339,
		"2006-01-02T15:04:05-07:00",
		"2006-01-02T15:04:05Z",
		"2006-01-02",
	}

	for _, format := range formats {
		if t, err := time.Parse(format, s); err == nil {
			return t, nil
		}
	}

	return time.Time{}, nil
}

// sortURLsByLastMod sorts URLs by lastmod date (most recent first)
func sortURLsByLastMod(urls []domain.SitemapURL) {
	sort.Slice(urls, func(i, j int) bool {
		return urls[i].LastMod.After(urls[j].LastMod)
	})
}

// decompressGzip decompresses gzip content
func decompressGzip(data []byte) ([]byte, error) {
	reader, err := gzip.NewReader(strings.NewReader(string(data)))
	if err != nil {
		return nil, err
	}
	defer reader.Close()

	return io.ReadAll(reader)
}
</file>
<file path="internal/strategies/strategy.go">
package strategies

import (
	"context"
	"errors"
	"fmt"
	"net/http"
	"os"
	"sync"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/cache"
	"github.com/quantmind-br/repodocs-go/internal/config"
	"github.com/quantmind-br/repodocs-go/internal/converter"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	"github.com/quantmind-br/repodocs-go/internal/fetcher"
	"github.com/quantmind-br/repodocs-go/internal/llm"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/renderer"
	"github.com/quantmind-br/repodocs-go/internal/state"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// Strategy defines the interface for documentation extraction strategies
type Strategy interface {
	// Name returns the strategy name
	Name() string
	// CanHandle returns true if this strategy can handle the given URL
	CanHandle(url string) bool
	// Execute runs the extraction strategy
	Execute(ctx context.Context, url string, opts Options) error
}

// Options contains common options for all strategies
type Options struct {
	domain.CommonOptions
	Output          string
	Concurrency     int
	MaxDepth        int
	Exclude         []string
	NoFolders       bool
	Split           bool
	IncludeAssets   bool
	ContentSelector string
	ExcludeSelector string
	CacheTTL        string
	FilterURL       string
}

// DefaultOptions returns default strategy options
func DefaultOptions() Options {
	return Options{
		CommonOptions: domain.DefaultCommonOptions(),
		Output:        "./docs",
		Concurrency:   5,
		MaxDepth:      3,
		NoFolders:     false,
		Split:         false,
	}
}

// Dependencies contains shared dependencies for all strategies
type Dependencies struct {
	Fetcher          domain.Fetcher
	Renderer         domain.Renderer
	Cache            domain.Cache
	Converter        *converter.Pipeline
	Writer           *output.Writer
	Logger           *utils.Logger
	LLMProvider      domain.LLMProvider
	MetadataEnhancer *llm.MetadataEnhancer
	Collector        *output.MetadataCollector
	HTTPClient       *http.Client
	StateManager     *state.Manager

	rendererOnce sync.Once
	rendererOpts renderer.RendererOptions
	rendererErr  error
}

// NewDependencies creates new dependencies for strategies
func NewDependencies(opts DependencyOptions) (*Dependencies, error) {
	// Create fetcher
	fetcherClient, err := fetcher.NewClient(fetcher.ClientOptions{
		Timeout:     opts.Timeout,
		MaxRetries:  3,
		EnableCache: opts.EnableCache,
		CacheTTL:    opts.CacheTTL,
		UserAgent:   opts.UserAgent,
	})
	if err != nil {
		return nil, err
	}

	// Create cache if enabled
	var cacheImpl domain.Cache
	if opts.EnableCache {
		cacheImpl, err = cache.NewBadgerCache(cache.Options{
			Directory: opts.CacheDir,
		})
		if err != nil {
			return nil, err
		}
		fetcherClient.SetCache(cacheImpl)
	}

	// Prepare renderer options for lazy initialization
	rendererOpts := renderer.DefaultRendererOptions()
	if opts.RendererTimeout > 0 {
		rendererOpts.Timeout = opts.RendererTimeout
	}
	if opts.Concurrency > 0 {
		rendererOpts.MaxTabs = opts.Concurrency
	}

	// Create renderer eagerly only if explicitly requested
	var rendererImpl domain.Renderer
	if opts.EnableRenderer {
		r, err := renderer.NewRenderer(rendererOpts)
		if err == nil {
			rendererImpl = r
		}
	}

	// Create converter
	converterPipeline := converter.NewPipeline(converter.PipelineOptions{
		BaseURL:         "",
		ContentSelector: opts.ContentSelector,
		ExcludeSelector: opts.ExcludeSelector,
	})

	var collector *output.MetadataCollector
	if opts.JSONMetadata {
		collector = output.NewMetadataCollector(output.CollectorOptions{
			BaseDir:   opts.OutputDir,
			SourceURL: opts.SourceURL,
			Enabled:   true,
		})
	}

	// Create writer
	writer := output.NewWriter(output.WriterOptions{
		BaseDir:      opts.OutputDir,
		Flat:         opts.Flat,
		JSONMetadata: opts.JSONMetadata,
		Force:        opts.Force,
		DryRun:       opts.DryRun,
		Collector:    collector,
	})

	// Create logger
	logger := utils.NewLogger(utils.LoggerOptions{
		Level:   "info",
		Format:  "pretty",
		Verbose: opts.Verbose,
	})

	var llmProvider domain.LLMProvider
	var metadataEnhancer *llm.MetadataEnhancer
	if opts.LLMConfig != nil && opts.LLMConfig.EnhanceMetadata && opts.LLMConfig.Provider != "" {
		baseProvider, err := llm.NewProviderFromConfig(opts.LLMConfig)
		if err != nil {
			logger.Warn().Err(err).Msg("Failed to create LLM provider, metadata enhancement disabled")
		} else {
			if opts.LLMConfig.RateLimit.Enabled {
				llmProvider = llm.NewRateLimitedProvider(
					baseProvider,
					llm.RateLimitedProviderConfig{
						RequestsPerMinute:        opts.LLMConfig.RateLimit.RequestsPerMinute,
						BurstSize:                opts.LLMConfig.RateLimit.BurstSize,
						MaxRetries:               opts.LLMConfig.RateLimit.MaxRetries,
						InitialDelay:             opts.LLMConfig.RateLimit.InitialDelay,
						MaxDelay:                 opts.LLMConfig.RateLimit.MaxDelay,
						Multiplier:               opts.LLMConfig.RateLimit.Multiplier,
						CircuitBreakerEnabled:    opts.LLMConfig.RateLimit.CircuitBreaker.Enabled,
						FailureThreshold:         opts.LLMConfig.RateLimit.CircuitBreaker.FailureThreshold,
						SuccessThresholdHalfOpen: opts.LLMConfig.RateLimit.CircuitBreaker.SuccessThresholdHalfOpen,
						ResetTimeout:             opts.LLMConfig.RateLimit.CircuitBreaker.ResetTimeout,
					},
					logger,
				)
				logger.Info().
					Str("provider", opts.LLMConfig.Provider).
					Int("requests_per_minute", opts.LLMConfig.RateLimit.RequestsPerMinute).
					Int("burst_size", opts.LLMConfig.RateLimit.BurstSize).
					Msg("LLM metadata enhancement enabled with rate limiting")
			} else {
				llmProvider = baseProvider
				logger.Info().Str("provider", opts.LLMConfig.Provider).Msg("LLM metadata enhancement enabled")
			}
			metadataEnhancer = llm.NewMetadataEnhancer(llmProvider)
		}
	}

	var stateManager *state.Manager
	if opts.Sync && !opts.FullSync {
		stateManager = state.NewManager(state.ManagerOptions{
			BaseDir:   opts.OutputDir,
			SourceURL: opts.SourceURL,
			Logger:    logger,
			Disabled:  false,
		})
		if err := stateManager.Load(context.Background()); err != nil {
			if !errors.Is(err, state.ErrStateNotFound) {
				logger.Warn().Err(err).Msg("Failed to load state, starting fresh")
			}
		}
	}

	return &Dependencies{
		Fetcher:          fetcherClient,
		Renderer:         rendererImpl,
		Cache:            cacheImpl,
		Converter:        converterPipeline,
		Writer:           writer,
		Logger:           logger,
		LLMProvider:      llmProvider,
		MetadataEnhancer: metadataEnhancer,
		Collector:        collector,
		StateManager:     stateManager,
		rendererOpts:     rendererOpts,
	}, nil
}

// Close releases all resources
func (d *Dependencies) Close() error {
	if d.Fetcher != nil {
		d.Fetcher.Close()
	}
	if d.Renderer != nil {
		d.Renderer.Close()
	}
	if d.Cache != nil {
		d.Cache.Close()
	}
	if d.LLMProvider != nil {
		d.LLMProvider.Close()
	}
	return nil
}

func (d *Dependencies) FlushMetadata() error {
	if d.Collector != nil {
		return d.Collector.Flush()
	}
	return nil
}

func (d *Dependencies) SaveState(ctx context.Context) error {
	if d.StateManager != nil {
		return d.StateManager.Save(ctx)
	}
	return nil
}

func (d *Dependencies) PruneDeletedFiles(ctx context.Context) (int, error) {
	if d.StateManager == nil || d.StateManager.IsDisabled() {
		return 0, nil
	}

	deleted := d.StateManager.GetDeletedPages()
	if len(deleted) == 0 {
		return 0, nil
	}

	var pruned int
	for _, page := range deleted {
		if err := os.Remove(page.FilePath); err != nil {
			if !os.IsNotExist(err) {
				d.Logger.Warn().Err(err).Str("file", page.FilePath).Msg("Failed to remove deleted page")
				continue
			}
		}
		pruned++
		d.Logger.Info().Str("file", page.FilePath).Msg("Removed deleted page")
	}

	d.StateManager.RemoveDeletedFromState()
	return pruned, nil
}

func (d *Dependencies) GetStateManager() *state.Manager {
	return d.StateManager
}

func (d *Dependencies) SetStrategy(name string) {
	if d.Collector != nil {
		d.Collector.SetStrategy(name)
	}
}

func (d *Dependencies) SetSourceURL(url string) {
	if d.Collector != nil {
		d.Collector.SetSourceURL(url)
	}
}

func (d *Dependencies) GetRenderer() (domain.Renderer, error) {
	if d.Renderer != nil {
		return d.Renderer, nil
	}

	d.rendererOnce.Do(func() {
		opts := d.rendererOpts
		if opts.Timeout == 0 {
			opts = renderer.DefaultRendererOptions()
		}
		r, err := renderer.NewRenderer(opts)
		if err != nil {
			d.rendererErr = err
			d.Logger.Debug().Err(err).Msg("Failed to initialize browser renderer on demand")
			return
		}
		d.Renderer = r
		d.Logger.Info().Msg("Browser renderer initialized on demand")
	})

	if d.rendererErr != nil {
		return nil, d.rendererErr
	}
	return d.Renderer, nil
}

// WriteDocument enhances metadata (if configured) and writes the document
func (d *Dependencies) WriteDocument(ctx context.Context, doc *domain.Document) error {
	if d.MetadataEnhancer != nil {
		if err := d.MetadataEnhancer.Enhance(ctx, doc); err != nil {
			d.Logger.Warn().Err(err).Str("url", doc.URL).Msg("Failed to enhance metadata, writing without enhancement")
		}
	}

	if d.Writer == nil {
		return fmt.Errorf("writer is not configured")
	}

	if err := d.Writer.Write(ctx, doc); err != nil {
		return err
	}

	if d.StateManager != nil && doc.ContentHash != "" {
		filePath := d.Writer.GetPath(doc.URL)
		d.StateManager.Update(doc.URL, state.PageState{
			ContentHash: doc.ContentHash,
			FetchedAt:   doc.FetchedAt,
			FilePath:    filePath,
		})
	}

	return nil
}

// DependencyOptions contains options for creating dependencies
type DependencyOptions struct {
	domain.CommonOptions
	Timeout         time.Duration
	EnableCache     bool
	CacheTTL        time.Duration
	CacheDir        string
	UserAgent       string
	EnableRenderer  bool
	RendererTimeout time.Duration
	Concurrency     int
	ContentSelector string
	ExcludeSelector string
	OutputDir       string
	Flat            bool
	JSONMetadata    bool
	LLMConfig       *config.LLMConfig
	SourceURL       string
}
</file>
<file path="internal/strategies/wiki.go">
package strategies

import (
	"context"
	"fmt"
	"os"
	"path/filepath"
	"strings"
	"time"

	"github.com/go-git/go-git/v5"
	githttp "github.com/go-git/go-git/v5/plumbing/transport/http"
	"github.com/quantmind-br/repodocs-go/internal/domain"
	internalgit "github.com/quantmind-br/repodocs-go/internal/git"
	"github.com/quantmind-br/repodocs-go/internal/output"
	"github.com/quantmind-br/repodocs-go/internal/utils"
)

// WikiStrategy extracts documentation from GitHub wiki repositories
type WikiStrategy struct {
	deps      *Dependencies
	writer    *output.Writer
	logger    *utils.Logger
	gitClient internalgit.Client
}

// NewWikiStrategy creates a new wiki strategy
func NewWikiStrategy(deps *Dependencies) *WikiStrategy {
	if deps == nil {
		return &WikiStrategy{
			gitClient: internalgit.NewClient(),
		}
	}
	return &WikiStrategy{
		deps:      deps,
		writer:    deps.Writer,
		logger:    deps.Logger,
		gitClient: internalgit.NewClient(),
	}
}

// SetGitClient sets the git client (useful for testing)
func (s *WikiStrategy) SetGitClient(client internalgit.Client) {
	s.gitClient = client
}

// Name returns the strategy name
func (s *WikiStrategy) Name() string {
	return "wiki"
}

// CanHandle returns true if this strategy can handle the given URL
func (s *WikiStrategy) CanHandle(url string) bool {
	return IsWikiURL(url)
}

// IsWikiURL checks if a URL points to a wiki (GitHub or Bitbucket)
func IsWikiURL(url string) bool {
	lower := strings.ToLower(url)

	// Pattern 1: github.com/{owner}/{repo}/wiki
	if strings.Contains(lower, "github.com") && strings.Contains(lower, "/wiki") {
		return true
	}

	// Pattern 2: bitbucket.org/{owner}/{repo}/wiki
	if strings.Contains(lower, "bitbucket.org") && strings.Contains(lower, "/wiki") {
		return true
	}

	// Pattern 3: {repo}.wiki.git
	if strings.HasSuffix(lower, ".wiki.git") {
		return true
	}

	return false
}

// Execute runs the wiki extraction strategy
func (s *WikiStrategy) Execute(ctx context.Context, url string, opts Options) error {
	s.logger.Info().Str("url", url).Msg("Starting wiki extraction")

	// Step 1: Parse wiki URL
	wikiInfo, err := ParseWikiURL(url)
	if err != nil {
		return fmt.Errorf("failed to parse wiki URL: %w", err)
	}

	s.logger.Debug().
		Str("owner", wikiInfo.Owner).
		Str("repo", wikiInfo.Repo).
		Str("clone_url", wikiInfo.CloneURL).
		Msg("Parsed wiki URL")

	// Step 2: Create temporary directory
	tmpDir, err := os.MkdirTemp("", "repodocs-wiki-*")
	if err != nil {
		return fmt.Errorf("failed to create temp dir: %w", err)
	}
	defer os.RemoveAll(tmpDir)

	// Step 3: Clone wiki repository
	if err := s.cloneWiki(ctx, wikiInfo.CloneURL, tmpDir); err != nil {
		return fmt.Errorf("failed to clone wiki: %w", err)
	}

	// Step 4: Parse wiki structure
	structure, err := s.parseWikiStructure(tmpDir)
	if err != nil {
		return fmt.Errorf("failed to parse wiki structure: %w", err)
	}

	s.logger.Info().
		Int("pages", len(structure.Pages)).
		Int("sections", len(structure.Sections)).
		Bool("has_sidebar", structure.HasSidebar).
		Msg("Parsed wiki structure")

	// Step 5: Process and write documents
	return s.processPages(ctx, structure, wikiInfo, opts)
}

// cloneWiki clones the wiki repository
func (s *WikiStrategy) cloneWiki(ctx context.Context, cloneURL, destDir string) error {
	s.logger.Info().Str("url", cloneURL).Msg("Cloning wiki repository")

	cloneOpts := &git.CloneOptions{
		URL:      cloneURL,
		Depth:    1, // Shallow clone for speed
		Progress: nil,
	}

	// Use HTTPS auth if available
	if token := os.Getenv("GITHUB_TOKEN"); token != "" {
		cloneOpts.Auth = &githttp.BasicAuth{
			Username: "token",
			Password: token,
		}
	}

	_, err := s.gitClient.PlainCloneContext(ctx, destDir, false, cloneOpts)
	if err != nil {
		// Check if wiki doesn't exist
		if strings.Contains(err.Error(), "not found") ||
			strings.Contains(err.Error(), "404") ||
			strings.Contains(err.Error(), "repository not found") {
			return fmt.Errorf("wiki not found or not enabled for this repository")
		}
		return err
	}

	return nil
}

// parseWikiStructure parses the wiki file structure and sidebar
func (s *WikiStrategy) parseWikiStructure(dir string) (*WikiStructure, error) {
	structure := &WikiStructure{
		Pages:    make(map[string]*WikiPage),
		Sections: []WikiSection{},
	}

	// Read all markdown files
	entries, err := os.ReadDir(dir)
	if err != nil {
		return nil, err
	}

	for _, entry := range entries {
		if entry.IsDir() {
			continue
		}

		name := entry.Name()
		ext := strings.ToLower(filepath.Ext(name))

		// Only process markdown files
		if ext != ".md" && ext != ".markdown" {
			continue
		}

		content, err := os.ReadFile(filepath.Join(dir, name))
		if err != nil {
			s.logger.Warn().Err(err).Str("file", name).Msg("Failed to read file")
			continue
		}

		page := &WikiPage{
			Filename:  name,
			Title:     FilenameToTitle(name),
			Content:   string(content),
			IsHome:    strings.EqualFold(name, "Home.md"),
			IsSpecial: strings.HasPrefix(name, "_"),
		}

		structure.Pages[name] = page
	}

	if sidebarPage, exists := structure.Pages["_Sidebar.md"]; exists {
		structure.HasSidebar = true
		structure.Sections = ParseSidebarContent(sidebarPage.Content, structure.Pages)
	} else {
		structure.Sections = CreateDefaultStructure(structure.Pages)
	}

	return structure, nil
}

// processPages processes all wiki pages and writes them to output
func (s *WikiStrategy) processPages(
	ctx context.Context,
	structure *WikiStructure,
	wikiInfo *WikiInfo,
	opts Options,
) error {
	// Count processable pages (exclude special files)
	var processablePages []*WikiPage
	for _, page := range structure.Pages {
		if !page.IsSpecial {
			processablePages = append(processablePages, page)
		}
	}

	if len(processablePages) == 0 {
		s.logger.Warn().Msg("No processable pages found in wiki")
		return nil
	}

	// Apply limit
	if opts.Limit > 0 && len(processablePages) > opts.Limit {
		processablePages = processablePages[:opts.Limit]
	}

	// Create progress bar
	bar := utils.NewProgressBar(len(processablePages), utils.DescExtracting)

	// Build base wiki URL for references
	baseWikiURL := fmt.Sprintf("https://github.com/%s/%s/wiki", wikiInfo.Owner, wikiInfo.Repo)

	// Process each page
	for _, page := range processablePages {
		select {
		case <-ctx.Done():
			return ctx.Err()
		default:
		}

		if err := s.processPage(ctx, page, structure, baseWikiURL, opts); err != nil {
			s.logger.Warn().Err(err).Str("page", page.Filename).Msg("Failed to process page")
		}
		bar.Add(1)
	}

	s.logger.Info().
		Int("processed", len(processablePages)).
		Msg("Wiki extraction completed")

	return nil
}

// processPage processes a single wiki page
func (s *WikiStrategy) processPage(
	ctx context.Context,
	page *WikiPage,
	structure *WikiStructure,
	baseWikiURL string,
	opts Options,
) error {
	content := ConvertWikiLinks(page.Content, structure.Pages)

	pageName := strings.TrimSuffix(page.Filename, filepath.Ext(page.Filename))
	pageURL := baseWikiURL
	if !page.IsHome {
		pageURL = fmt.Sprintf("%s/%s", baseWikiURL, pageName)
	}

	relativePath := BuildRelativePath(page, structure, opts.NoFolders)

	// Create document
	doc := &domain.Document{
		URL:            pageURL,
		Title:          page.Title,
		Content:        content,
		FetchedAt:      time.Now(),
		WordCount:      len(strings.Fields(content)),
		CharCount:      len(content),
		SourceStrategy: s.Name(),
		RelativePath:   relativePath,
	}

	if !opts.DryRun {
		return s.deps.WriteDocument(ctx, doc)
	}

	return nil
}
</file>
<file path="internal/strategies/wiki_parser.go">
package strategies

import (
	"fmt"
	"path/filepath"
	"regexp"
	"sort"
	"strings"
)

type WikiPage struct {
	Filename  string
	Title     string
	Content   string
	Section   string
	Order     int
	Links     []string
	IsHome    bool
	IsSpecial bool
}

type WikiStructure struct {
	Sections   []WikiSection
	Pages      map[string]*WikiPage
	HasSidebar bool
}

type WikiSection struct {
	Name  string
	Order int
	Pages []string
}

type WikiInfo struct {
	Owner      string
	Repo       string
	CloneURL   string
	Platform   string
	TargetPage string
}

func ParseWikiURL(rawURL string) (*WikiInfo, error) {
	url := strings.TrimSuffix(rawURL, "/")

	// Check for SSH format: git@github.com:owner/repo.wiki.git
	if strings.HasPrefix(url, "git@") {
		return nil, fmt.Errorf("invalid wiki URL format (SSH not supported): %s", rawURL)
	}

	// github.com/{owner}/{repo}/wiki[/{page}] or {repo}.wiki.git
	wikiPattern := regexp.MustCompile(
		`github\.com[:/]([^/]+)/([^/]+?)(?:\.wiki)?(?:/wiki)?(?:/([^/]+))?(?:\.git)?$`,
	)

	matches := wikiPattern.FindStringSubmatch(url)
	if len(matches) < 3 {
		return nil, fmt.Errorf("invalid wiki URL format: %s", rawURL)
	}

	owner := matches[1]
	repo := strings.TrimSuffix(matches[2], ".wiki")

	var targetPage string
	if len(matches) > 3 && matches[3] != "" {
		targetPage = matches[3]
	}

	cloneURL := fmt.Sprintf("https://github.com/%s/%s.wiki.git", owner, repo)

	return &WikiInfo{
		Owner:      owner,
		Repo:       repo,
		CloneURL:   cloneURL,
		Platform:   "github",
		TargetPage: targetPage,
	}, nil
}

func FilenameToTitle(filename string) string {
	// Extract base name without extension
	ext := filepath.Ext(filename)
	name := strings.TrimSuffix(filename, ext)

	// If name is empty after removing extension, return empty
	if name == "" {
		return ""
	}

	name = strings.ReplaceAll(name, "-", " ")
	name = strings.ReplaceAll(name, "_", " ")

	words := strings.Fields(name)
	for i, word := range words {
		if len(word) > 0 {
			words[i] = strings.ToUpper(word[:1]) + word[1:]
		}
	}

	return strings.Join(words, " ")
}

func TitleToFilename(title string) string {
	return strings.ReplaceAll(title, " ", "-")
}

func ParseSidebarContent(content string, pages map[string]*WikiPage) []WikiSection {
	var sections []WikiSection
	var currentSection *WikiSection

	lines := strings.Split(content, "\n")
	sectionOrder := 0
	pageOrder := 0

	headerPattern := regexp.MustCompile(`^#+\s*(.+)$`)
	wikiLinkPattern := regexp.MustCompile(`\[\[([^\]|]+)(?:\|[^\]]+)?\]\]`)
	mdLinkPattern := regexp.MustCompile(`\[([^\]]+)\]\(([^)]+)\)`)

	for _, line := range lines {
		trimmed := strings.TrimSpace(line)

		if matches := headerPattern.FindStringSubmatch(trimmed); len(matches) > 1 {
			if currentSection != nil && len(currentSection.Pages) > 0 {
				sections = append(sections, *currentSection)
			}

			sectionOrder++
			pageOrder = 0
			currentSection = &WikiSection{
				Name:  strings.TrimSpace(matches[1]),
				Order: sectionOrder,
				Pages: []string{},
			}
			continue
		}

		if wikiMatches := wikiLinkPattern.FindAllStringSubmatch(trimmed, -1); len(wikiMatches) > 0 {
			if currentSection == nil {
				sectionOrder++
				currentSection = &WikiSection{
					Name:  "General",
					Order: sectionOrder,
					Pages: []string{},
				}
			}
			for _, match := range wikiMatches {
				pageName := match[1]
				filename := findPageFilename(pageName, pages)
				if filename != "" {
					pageOrder++
					if page, exists := pages[filename]; exists {
						page.Section = currentSection.Name
						page.Order = pageOrder
					}
					if currentSection != nil {
						currentSection.Pages = append(currentSection.Pages, filename)
					}
				}
			}
			continue
		}

		if mdMatches := mdLinkPattern.FindAllStringSubmatch(trimmed, -1); len(mdMatches) > 0 {
			if currentSection == nil {
				sectionOrder++
				currentSection = &WikiSection{
					Name:  "General",
					Order: sectionOrder,
					Pages: []string{},
				}
			}
			for _, match := range mdMatches {
				pageName := match[2]
				pageName = strings.TrimSuffix(pageName, ".md")
				filename := findPageFilename(pageName, pages)
				if filename != "" {
					pageOrder++
					if page, exists := pages[filename]; exists {
						page.Section = currentSection.Name
						page.Order = pageOrder
					}
					if currentSection != nil {
						currentSection.Pages = append(currentSection.Pages, filename)
					}
				}
			}
		}
	}

	if currentSection != nil && len(currentSection.Pages) > 0 {
		sections = append(sections, *currentSection)
	}

	return sections
}

func findPageFilename(pageName string, pages map[string]*WikiPage) string {
	// Exact match first
	if _, exists := pages[pageName]; exists {
		return pageName
	}

	// Match with .md extension
	if _, exists := pages[pageName+".md"]; exists {
		return pageName + ".md"
	}

	// Hyphenated match
	hyphenated := strings.ReplaceAll(pageName, " ", "-") + ".md"
	if _, exists := pages[hyphenated]; exists {
		return hyphenated
	}

	// Case-insensitive match
	for filename := range pages {
		baseName := strings.TrimSuffix(filename, ".md")
		if strings.EqualFold(baseName, pageName) ||
			strings.EqualFold(baseName, strings.ReplaceAll(pageName, " ", "-")) {
			return filename
		}
	}

	return ""
}

func CreateDefaultStructure(pages map[string]*WikiPage) []WikiSection {
	var pageNames []string
	for filename, page := range pages {
		if !page.IsSpecial {
			pageNames = append(pageNames, filename)
		}
	}

	sort.Strings(pageNames)

	for i, name := range pageNames {
		if strings.EqualFold(name, "Home.md") {
			pageNames = append([]string{name}, append(pageNames[:i], pageNames[i+1:]...)...)
			break
		}
	}

	for i, filename := range pageNames {
		if page, exists := pages[filename]; exists {
			page.Order = i + 1
			page.Section = "Documentation"
		}
	}

	return []WikiSection{
		{
			Name:  "Documentation",
			Order: 1,
			Pages: pageNames,
		},
	}
}

func ConvertWikiLinks(content string, _ map[string]*WikiPage) string {
	// [[Page Name|Custom Text]] -> [Custom Text](./page-name.md)
	pattern1 := regexp.MustCompile(`\[\[([^\]|]+)\|([^\]]+)\]\]`)
	content = pattern1.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern1.FindStringSubmatch(match)
		if len(matches) == 3 {
			pageName := matches[1]
			linkText := matches[2]
			filename := TitleToFilename(pageName) + ".md"
			return fmt.Sprintf("[%s](./%s)", linkText, strings.ToLower(filename))
		}
		return match
	})

	// [[Page Name#Section]] -> [Page Name](./page-name.md#section)
	pattern2 := regexp.MustCompile(`\[\[([^\]#]+)#([^\]]+)\]\]`)
	content = pattern2.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern2.FindStringSubmatch(match)
		if len(matches) == 3 {
			pageName := matches[1]
			section := matches[2]
			filename := TitleToFilename(pageName) + ".md"
			anchor := strings.ToLower(strings.ReplaceAll(section, " ", "-"))
			return fmt.Sprintf("[%s](./%s#%s)", pageName, strings.ToLower(filename), anchor)
		}
		return match
	})

	// [[Page Name]] -> [Page Name](./page-name.md)
	pattern3 := regexp.MustCompile(`\[\[([^\]]+)\]\]`)
	content = pattern3.ReplaceAllStringFunc(content, func(match string) string {
		matches := pattern3.FindStringSubmatch(match)
		if len(matches) == 2 {
			pageName := matches[1]
			filename := TitleToFilename(pageName) + ".md"
			return fmt.Sprintf("[%s](./%s)", pageName, strings.ToLower(filename))
		}
		return match
	})

	return content
}

func BuildRelativePath(page *WikiPage, structure *WikiStructure, flat bool) string {
	if page.IsHome {
		return "index.md"
	}

	if flat || len(structure.Sections) == 0 || page.Section == "" {
		return strings.ToLower(page.Filename)
	}

	sectionDir := strings.ToLower(strings.ReplaceAll(page.Section, " ", "-"))
	filename := strings.ToLower(page.Filename)

	return filepath.Join(sectionDir, filename)
}
</file>
<file path="internal/tui/app.go">
package tui

import (
	"fmt"
	"strings"

	tea "github.com/charmbracelet/bubbletea"
	"github.com/charmbracelet/huh"
	"github.com/charmbracelet/lipgloss"

	"github.com/quantmind-br/repodocs-go/internal/config"
)

type state int

const (
	stateMenu state = iota
	stateSubMenu
	stateForm
	stateConfirm
	stateSaved
	stateError
)

type Model struct {
	state           state
	values          *ConfigValues
	originalConfig  *config.Config
	menuIndex       int
	currentForm     *huh.Form
	err             error
	width           int
	height          int
	dirty           bool
	saveFunc        func(*config.Config) error
	accessible      bool
	subMenuIndex    int
	parentCategory  *Category
	cameFromSubMenu bool
}

type Options struct {
	Config     *config.Config
	SaveFunc   func(*config.Config) error
	Accessible bool
}

func NewModel(opts Options) Model {
	cfg := opts.Config
	if cfg == nil {
		cfg = config.Default()
	}

	return Model{
		state:          stateMenu,
		values:         FromConfig(cfg),
		originalConfig: cfg,
		saveFunc:       opts.SaveFunc,
		accessible:     opts.Accessible,
	}
}

func (m Model) Init() tea.Cmd {
	return nil
}

func (m Model) Update(msg tea.Msg) (tea.Model, tea.Cmd) {
	switch msg := msg.(type) {
	case tea.WindowSizeMsg:
		m.width = msg.Width
		m.height = msg.Height
		if m.state == stateForm && m.currentForm != nil {
			form, cmd := m.currentForm.Update(msg)
			if f, ok := form.(*huh.Form); ok {
				m.currentForm = f
			}
			return m, cmd
		}
		return m, nil

	case tea.KeyMsg:
		switch m.state {
		case stateMenu:
			return m.updateMenu(msg)
		case stateSubMenu:
			return m.updateSubMenu(msg)
		case stateForm:
			return m.updateForm(msg)
		case stateConfirm:
			return m.updateConfirm(msg)
		case stateSaved, stateError:
			return m, tea.Quit
		}
	}

	if m.state == stateForm && m.currentForm != nil {
		form, cmd := m.currentForm.Update(msg)
		if f, ok := form.(*huh.Form); ok {
			m.currentForm = f
		}
		if m.currentForm.State == huh.StateCompleted {
			m.dirty = true
			m.state = stateMenu
			return m, nil
		}
		return m, cmd
	}

	return m, nil
}

func (m Model) updateMenu(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "q", "ctrl+c":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit

	case "up", "k":
		if m.menuIndex > 0 {
			m.menuIndex--
		}

	case "down", "j":
		if m.menuIndex < len(Categories) {
			m.menuIndex++
		}

	case "enter":
		if m.menuIndex == len(Categories) {
			return m.handleSave()
		}
		category := Categories[m.menuIndex]

		// Check if category has sub-categories
		if category.HasSubCategories() {
			m.state = stateSubMenu
			m.parentCategory = &Categories[m.menuIndex]
			m.subMenuIndex = 0
			return m, nil
		}

		// No sub-categories - open form directly (existing behavior)
		m.state = stateForm
		m.currentForm = GetFormForCategory(category.ID, m.values)
		m.cameFromSubMenu = false
		if m.accessible {
			m.currentForm = m.currentForm.WithAccessible(true)
		}
		return m, m.currentForm.Init()

	case "s":
		return m.handleSave()

	case "esc":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit
	}

	return m, nil
}

func (m Model) updateSubMenu(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "q", "ctrl+c":
		if m.dirty {
			m.state = stateConfirm
			return m, nil
		}
		return m, tea.Quit

	case "up", "k":
		if m.subMenuIndex > 0 {
			m.subMenuIndex--
		}

	case "down", "j":
		if m.subMenuIndex < len(m.parentCategory.SubCategories)-1 {
			m.subMenuIndex++
		}

	case "enter":
		subCat := m.parentCategory.SubCategories[m.subMenuIndex]
		m.state = stateForm
		m.currentForm = GetFormForCategory(subCat.ID, m.values)
		m.cameFromSubMenu = true
		if m.accessible {
			m.currentForm = m.currentForm.WithAccessible(true)
		}
		return m, m.currentForm.Init()

	case "esc":
		m.state = stateMenu
		m.parentCategory = nil
		return m, nil
	}
	return m, nil
}

func (m Model) updateForm(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	if msg.String() == "esc" {
		if m.cameFromSubMenu && m.parentCategory != nil {
			m.state = stateSubMenu
		} else {
			m.state = stateMenu
		}
		return m, nil
	}
	if m.currentForm != nil {
		form, cmd := m.currentForm.Update(msg)
		if f, ok := form.(*huh.Form); ok {
			m.currentForm = f
		}
		if m.currentForm.State == huh.StateCompleted {
			m.dirty = true
			if m.cameFromSubMenu && m.parentCategory != nil {
				m.state = stateSubMenu
			} else {
				m.state = stateMenu
			}
			return m, nil
		}
		return m, cmd
	}
	return m, nil
}

func (m Model) updateConfirm(msg tea.KeyMsg) (tea.Model, tea.Cmd) {
	switch msg.String() {
	case "y", "Y":
		return m.handleSave()
	case "n", "N", "esc":
		return m, tea.Quit
	case "c":
		m.state = stateMenu
	}
	return m, nil
}

func (m Model) handleSave() (tea.Model, tea.Cmd) {
	cfg, err := m.values.ToConfig()
	if err != nil {
		m.state = stateError
		m.err = err
		return m, nil
	}

	if m.saveFunc != nil {
		if err := m.saveFunc(cfg); err != nil {
			m.state = stateError
			m.err = err
			return m, nil
		}
	}

	m.state = stateSaved
	m.dirty = false
	return m, nil
}

func (m Model) View() string {
	var s strings.Builder

	header := TitleStyle.Render("RepoDocs Configuration")
	s.WriteString(header)
	s.WriteString("\n\n")

	switch m.state {
	case stateMenu:
		s.WriteString(m.renderMenu())
	case stateSubMenu:
		s.WriteString(m.renderSubMenu())
	case stateForm:
		if m.currentForm != nil {
			s.WriteString(m.currentForm.View())
		}
	case stateConfirm:
		s.WriteString(m.renderConfirm())
	case stateSaved:
		s.WriteString(SuccessStyle.Render("Configuration saved successfully!"))
		s.WriteString("\n\nPress any key to exit.")
	case stateError:
		s.WriteString(ErrorStyle.Render(fmt.Sprintf("Error: %v", m.err)))
		s.WriteString("\n\nPress any key to exit.")
	}

	return s.String()
}

func (m Model) renderMenu() string {
	var s strings.Builder

	for i, cat := range Categories {
		cursor := "  "
		style := UnselectedStyle
		if i == m.menuIndex {
			cursor = "> "
			style = SelectedStyle
		}
		line := fmt.Sprintf("%s%s %s", cursor, cat.Icon, cat.Name)
		s.WriteString(style.Render(line))
		if i == m.menuIndex {
			s.WriteString(DescriptionStyle.Render("  " + cat.Description))
		}
		s.WriteString("\n")
	}

	saveStyle := UnselectedStyle
	saveCursor := "  "
	if m.menuIndex == len(Categories) {
		saveCursor = "> "
		saveStyle = SelectedStyle
	}
	saveText := fmt.Sprintf("%s Save Configuration", saveCursor)
	if m.dirty {
		saveText += " *"
	}
	s.WriteString("\n")
	s.WriteString(saveStyle.Render(saveText))
	s.WriteString("\n\n")

	help := HelpStyle.Render("↑/↓ navigate • enter select • s save • q quit")
	s.WriteString(help)

	return s.String()
}

func (m Model) renderSubMenu() string {
	var s strings.Builder

	s.WriteString(DescriptionStyle.Render(m.parentCategory.Name + " Settings"))
	s.WriteString("\n\n")

	for i, subCat := range m.parentCategory.SubCategories {
		cursor := "  "
		style := UnselectedStyle
		if i == m.subMenuIndex {
			cursor = "> "
			style = SelectedStyle
		}
		line := fmt.Sprintf("%s%s %s", cursor, subCat.Icon, subCat.Name)
		s.WriteString(style.Render(line))
		if i == m.subMenuIndex {
			s.WriteString(DescriptionStyle.Render("  " + subCat.Description))
		}
		s.WriteString("\n")
	}

	s.WriteString("\n")
	help := HelpStyle.Render("↑/↓ navigate • enter select • esc back")
	s.WriteString(help)

	return s.String()
}

func (m Model) renderConfirm() string {
	box := lipgloss.NewStyle().
		Border(lipgloss.RoundedBorder()).
		BorderForeground(warnColor).
		Padding(1, 2).
		Render("You have unsaved changes.\n\nSave before quitting?\n\n[y] Yes  [n] No  [c] Cancel")

	return box
}

func Run(opts Options) error {
	p := tea.NewProgram(NewModel(opts), tea.WithAltScreen())
	_, err := p.Run()
	return err
}
</file>
<file path="internal/tui/categories.go">
package tui

type Category struct {
	ID            string
	Name          string
	Description   string
	Icon          string
	SubCategories []Category
}

var Categories = []Category{
	{ID: "output", Name: "Output", Description: "Output directory and format settings", Icon: ""},
	{ID: "exclude", Name: "Exclude Patterns", Description: "URL/path patterns to skip", Icon: ""},
	{ID: "concurrency", Name: "Concurrency", Description: "Workers, timeout, and depth limits", Icon: ""},
	{ID: "cache", Name: "Cache", Description: "Caching behavior and TTL", Icon: ""},
	{ID: "rendering", Name: "Rendering", Description: "JavaScript rendering options", Icon: ""},
	{ID: "stealth", Name: "Stealth", Description: "User-agent and delay settings", Icon: ""},
	{ID: "logging", Name: "Logging", Description: "Log level and format", Icon: ""},
	{ID: "llm", Name: "LLM", Description: "AI provider configuration", Icon: "", SubCategories: []Category{
		{ID: "llm_basic", Name: "Basic Settings", Description: "Provider, API key, and model", Icon: ""},
		{ID: "llm_rate_limit", Name: "Rate Limit", Description: "Request throttling settings", Icon: ""},
		{ID: "llm_circuit_breaker", Name: "Circuit Breaker", Description: "Failure protection settings", Icon: ""},
	}},
}

func (c *Category) HasSubCategories() bool {
	return len(c.SubCategories) > 0
}

func GetCategoryByID(id string) *Category {
	for i := range Categories {
		if Categories[i].ID == id {
			return &Categories[i]
		}
	}
	return nil
}

func GetCategoryNames() []string {
	names := make([]string, len(Categories))
	for i, c := range Categories {
		names[i] = c.Name
	}
	return names
}
</file>
<file path="internal/tui/config_adapter.go">
package tui

import (
	"fmt"
	"strconv"
	"strings"
	"time"

	"github.com/quantmind-br/repodocs-go/internal/config"
)

// ConfigValues holds form values that map to Config struct.
// Numeric and duration fields are stored as strings for form editing.
type ConfigValues struct {
	OutputDirectory string
	OutputFlat      bool
	OutputOverwrite bool
	JSONMetadata    bool

	Workers  string
	Timeout  string
	MaxDepth string

	CacheEnabled   bool
	CacheTTL       string
	CacheDirectory string

	ForceJS     bool
	JSTimeout   string
	ScrollToEnd bool

	UserAgent      string
	RandomDelayMin string
	RandomDelayMax string

	LogLevel  string
	LogFormat string

	LLMProvider        string
	LLMAPIKey          string
	LLMBaseURL         string
	LLMModel           string
	LLMMaxTokens       string
	LLMTemperature     string
	LLMTimeout         string
	LLMEnhanceMetadata bool

	ExcludePatterns string

	RateLimitEnabled           bool
	RateLimitRequestsPerMinute string
	RateLimitBurstSize         string
	RateLimitMaxRetries        string
	RateLimitInitialDelay      string
	RateLimitMaxDelay          string
	RateLimitMultiplier        string

	CircuitBreakerEnabled          bool
	CircuitBreakerFailureThreshold string
	CircuitBreakerSuccessThreshold string
	CircuitBreakerResetTimeout     string

	Exclude []string
}

// FromConfig converts a Config to ConfigValues for form editing
func FromConfig(cfg *config.Config) *ConfigValues {
	return &ConfigValues{
		OutputDirectory: cfg.Output.Directory,
		OutputFlat:      cfg.Output.Flat,
		OutputOverwrite: cfg.Output.Overwrite,
		JSONMetadata:    cfg.Output.JSONMetadata,

		Workers:  strconv.Itoa(cfg.Concurrency.Workers),
		Timeout:  formatDuration(cfg.Concurrency.Timeout),
		MaxDepth: strconv.Itoa(cfg.Concurrency.MaxDepth),

		CacheEnabled:   cfg.Cache.Enabled,
		CacheTTL:       formatDuration(cfg.Cache.TTL),
		CacheDirectory: cfg.Cache.Directory,

		ForceJS:     cfg.Rendering.ForceJS,
		JSTimeout:   formatDuration(cfg.Rendering.JSTimeout),
		ScrollToEnd: cfg.Rendering.ScrollToEnd,

		UserAgent:      cfg.Stealth.UserAgent,
		RandomDelayMin: formatDuration(cfg.Stealth.RandomDelayMin),
		RandomDelayMax: formatDuration(cfg.Stealth.RandomDelayMax),

		LogLevel:  cfg.Logging.Level,
		LogFormat: cfg.Logging.Format,

		LLMProvider:        cfg.LLM.Provider,
		LLMAPIKey:          cfg.LLM.APIKey,
		LLMBaseURL:         cfg.LLM.BaseURL,
		LLMModel:           cfg.LLM.Model,
		LLMMaxTokens:       strconv.Itoa(cfg.LLM.MaxTokens),
		LLMTemperature:     strconv.FormatFloat(cfg.LLM.Temperature, 'f', 2, 64),
		LLMTimeout:         formatDuration(cfg.LLM.Timeout),
		LLMEnhanceMetadata: cfg.LLM.EnhanceMetadata,

		ExcludePatterns: strings.Join(cfg.Exclude, "\n"),

		RateLimitEnabled:           cfg.LLM.RateLimit.Enabled,
		RateLimitRequestsPerMinute: strconv.Itoa(cfg.LLM.RateLimit.RequestsPerMinute),
		RateLimitBurstSize:         strconv.Itoa(cfg.LLM.RateLimit.BurstSize),
		RateLimitMaxRetries:        strconv.Itoa(cfg.LLM.RateLimit.MaxRetries),
		RateLimitInitialDelay:      formatDuration(cfg.LLM.RateLimit.InitialDelay),
		RateLimitMaxDelay:          formatDuration(cfg.LLM.RateLimit.MaxDelay),
		RateLimitMultiplier:        strconv.FormatFloat(cfg.LLM.RateLimit.Multiplier, 'f', 2, 64),

		CircuitBreakerEnabled:          cfg.LLM.RateLimit.CircuitBreaker.Enabled,
		CircuitBreakerFailureThreshold: strconv.Itoa(cfg.LLM.RateLimit.CircuitBreaker.FailureThreshold),
		CircuitBreakerSuccessThreshold: strconv.Itoa(cfg.LLM.RateLimit.CircuitBreaker.SuccessThresholdHalfOpen),
		CircuitBreakerResetTimeout:     formatDuration(cfg.LLM.RateLimit.CircuitBreaker.ResetTimeout),

		Exclude: cfg.Exclude,
	}
}

// ToConfig converts ConfigValues back to a Config struct
func (v *ConfigValues) ToConfig() (*config.Config, error) {
	workers, err := parseIntOrDefault(v.Workers, config.DefaultWorkers)
	if err != nil {
		return nil, fmt.Errorf("invalid workers: %w", err)
	}

	maxDepth, err := parseIntOrDefault(v.MaxDepth, config.DefaultMaxDepth)
	if err != nil {
		return nil, fmt.Errorf("invalid max_depth: %w", err)
	}

	timeout, err := parseDurationOrDefault(v.Timeout, config.DefaultTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid timeout: %w", err)
	}

	cacheTTL, err := parseDurationOrDefault(v.CacheTTL, config.DefaultCacheTTL)
	if err != nil {
		return nil, fmt.Errorf("invalid cache_ttl: %w", err)
	}

	jsTimeout, err := parseDurationOrDefault(v.JSTimeout, config.DefaultJSTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid js_timeout: %w", err)
	}

	delayMin, err := parseDurationOrDefault(v.RandomDelayMin, config.DefaultRandomDelayMin)
	if err != nil {
		return nil, fmt.Errorf("invalid random_delay_min: %w", err)
	}

	delayMax, err := parseDurationOrDefault(v.RandomDelayMax, config.DefaultRandomDelayMax)
	if err != nil {
		return nil, fmt.Errorf("invalid random_delay_max: %w", err)
	}

	llmMaxTokens, err := parseIntOrDefault(v.LLMMaxTokens, config.DefaultLLMMaxTokens)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_max_tokens: %w", err)
	}

	llmTemperature, err := parseFloatOrDefault(v.LLMTemperature, config.DefaultLLMTemperature)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_temperature: %w", err)
	}

	llmTimeout, err := parseDurationOrDefault(v.LLMTimeout, 30*time.Second)
	if err != nil {
		return nil, fmt.Errorf("invalid llm_timeout: %w", err)
	}

	rateLimitRequestsPerMinute, err := parseIntOrDefault(v.RateLimitRequestsPerMinute, config.DefaultRateLimitRequestsPerMinute)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_requests_per_minute: %w", err)
	}

	rateLimitBurstSize, err := parseIntOrDefault(v.RateLimitBurstSize, config.DefaultRateLimitBurstSize)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_burst_size: %w", err)
	}

	rateLimitMaxRetries, err := parseIntOrDefault(v.RateLimitMaxRetries, config.DefaultRateLimitMaxRetries)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_max_retries: %w", err)
	}

	rateLimitInitialDelay, err := parseDurationOrDefault(v.RateLimitInitialDelay, config.DefaultRateLimitInitialDelay)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_initial_delay: %w", err)
	}

	rateLimitMaxDelay, err := parseDurationOrDefault(v.RateLimitMaxDelay, config.DefaultRateLimitMaxDelay)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_max_delay: %w", err)
	}

	rateLimitMultiplier, err := parseFloatOrDefault(v.RateLimitMultiplier, config.DefaultRateLimitMultiplier)
	if err != nil {
		return nil, fmt.Errorf("invalid rate_limit_multiplier: %w", err)
	}

	circuitBreakerFailureThreshold, err := parseIntOrDefault(v.CircuitBreakerFailureThreshold, config.DefaultCircuitBreakerFailureThreshold)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_failure_threshold: %w", err)
	}

	circuitBreakerSuccessThreshold, err := parseIntOrDefault(v.CircuitBreakerSuccessThreshold, config.DefaultCircuitBreakerSuccessThresholdHalfOpen)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_success_threshold: %w", err)
	}

	circuitBreakerResetTimeout, err := parseDurationOrDefault(v.CircuitBreakerResetTimeout, config.DefaultCircuitBreakerResetTimeout)
	if err != nil {
		return nil, fmt.Errorf("invalid circuit_breaker_reset_timeout: %w", err)
	}

	excludePatterns := strings.Split(v.ExcludePatterns, "\n")
	var excludeList []string
	for _, pattern := range excludePatterns {
		trimmed := strings.TrimSpace(pattern)
		if trimmed != "" {
			excludeList = append(excludeList, trimmed)
		}
	}

	cfg := &config.Config{
		Output: config.OutputConfig{
			Directory:    v.OutputDirectory,
			Flat:         v.OutputFlat,
			Overwrite:    v.OutputOverwrite,
			JSONMetadata: v.JSONMetadata,
		},
		Concurrency: config.ConcurrencyConfig{
			Workers:  workers,
			Timeout:  timeout,
			MaxDepth: maxDepth,
		},
		Cache: config.CacheConfig{
			Enabled:   v.CacheEnabled,
			TTL:       cacheTTL,
			Directory: v.CacheDirectory,
		},
		Rendering: config.RenderingConfig{
			ForceJS:     v.ForceJS,
			JSTimeout:   jsTimeout,
			ScrollToEnd: v.ScrollToEnd,
		},
		Stealth: config.StealthConfig{
			UserAgent:      v.UserAgent,
			RandomDelayMin: delayMin,
			RandomDelayMax: delayMax,
		},
		Logging: config.LoggingConfig{
			Level:  v.LogLevel,
			Format: v.LogFormat,
		},
		LLM: config.LLMConfig{
			Provider:        v.LLMProvider,
			APIKey:          v.LLMAPIKey,
			BaseURL:         v.LLMBaseURL,
			Model:           v.LLMModel,
			MaxTokens:       llmMaxTokens,
			Temperature:     llmTemperature,
			Timeout:         llmTimeout,
			EnhanceMetadata: v.LLMEnhanceMetadata,
			RateLimit: config.RateLimitConfig{
				Enabled:           v.RateLimitEnabled,
				RequestsPerMinute: rateLimitRequestsPerMinute,
				BurstSize:         rateLimitBurstSize,
				MaxRetries:        rateLimitMaxRetries,
				InitialDelay:      rateLimitInitialDelay,
				MaxDelay:          rateLimitMaxDelay,
				Multiplier:        rateLimitMultiplier,
				CircuitBreaker: config.CircuitBreakerConfig{
					Enabled:                  v.CircuitBreakerEnabled,
					FailureThreshold:         circuitBreakerFailureThreshold,
					SuccessThresholdHalfOpen: circuitBreakerSuccessThreshold,
					ResetTimeout:             circuitBreakerResetTimeout,
				},
			},
		},
		Exclude: excludeList,
	}

	return cfg, nil
}

func formatDuration(d time.Duration) string {
	if d == 0 {
		return ""
	}
	return d.String()
}

func parseDurationOrDefault(s string, defaultVal time.Duration) (time.Duration, error) {
	if s == "" {
		return defaultVal, nil
	}
	return time.ParseDuration(s)
}

func parseIntOrDefault(s string, defaultVal int) (int, error) {
	if s == "" {
		return defaultVal, nil
	}
	return strconv.Atoi(s)
}

func parseFloatOrDefault(s string, defaultVal float64) (float64, error) {
	if s == "" {
		return defaultVal, nil
	}
	return strconv.ParseFloat(s, 64)
}
</file>
<file path="internal/tui/forms.go">
package tui

import (
	"github.com/charmbracelet/huh"
)

func CreateOutputForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("directory").
				Title("Output Directory").
				Description("Where to save extracted documentation").
				Value(&values.OutputDirectory).
				Placeholder("./docs").
				CharLimit(256),

			huh.NewConfirm().
				Key("flat").
				Title("Flat Structure").
				Description("Save all files in a single directory (no subdirectories)").
				Value(&values.OutputFlat),

			huh.NewConfirm().
				Key("overwrite").
				Title("Overwrite Existing").
				Description("Overwrite existing files without prompting").
				Value(&values.OutputOverwrite),

			huh.NewConfirm().
				Key("json_metadata").
				Title("JSON Metadata").
				Description("Generate .json metadata files alongside markdown").
				Value(&values.JSONMetadata),
		),
	).WithTheme(GetTheme())
}

func CreateConcurrencyForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("workers").
				Title("Workers").
				Description("Number of concurrent workers (1-50)").
				Value(&values.Workers).
				Placeholder("5").
				CharLimit(3).
				Validate(ValidateIntRange(1, 50)),

			huh.NewInput().
				Key("timeout").
				Title("Request Timeout").
				Description("HTTP request timeout (e.g., 30s, 1m)").
				Value(&values.Timeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("max_depth").
				Title("Max Crawl Depth").
				Description("Maximum depth for recursive crawling (1-100)").
				Value(&values.MaxDepth).
				Placeholder("4").
				CharLimit(3).
				Validate(ValidateIntRange(1, 100)),
		),
	).WithTheme(GetTheme())
}

func CreateCacheForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Cache").
				Description("Cache fetched pages to reduce network requests").
				Value(&values.CacheEnabled),

			huh.NewInput().
				Key("ttl").
				Title("Cache TTL").
				Description("How long to keep cached pages (e.g., 24h, 7d)").
				Value(&values.CacheTTL).
				Placeholder("24h").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("directory").
				Title("Cache Directory").
				Description("Directory for cache storage").
				Value(&values.CacheDirectory).
				Placeholder("~/.repodocs/cache").
				CharLimit(256),
		),
	).WithTheme(GetTheme())
}

func CreateRenderingForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("force_js").
				Title("Force JavaScript Rendering").
				Description("Always render pages with headless browser").
				Value(&values.ForceJS),

			huh.NewInput().
				Key("js_timeout").
				Title("JS Timeout").
				Description("Timeout for JavaScript rendering (e.g., 10s, 30s)").
				Value(&values.JSTimeout).
				Placeholder("10s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewConfirm().
				Key("scroll_to_end").
				Title("Scroll to End").
				Description("Scroll page to load lazy content").
				Value(&values.ScrollToEnd),
		),
	).WithTheme(GetTheme())
}

func CreateStealthForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewInput().
				Key("user_agent").
				Title("User Agent").
				Description("Custom User-Agent header (leave empty for default)").
				Value(&values.UserAgent).
				Placeholder("Mozilla/5.0...").
				CharLimit(256),

			huh.NewInput().
				Key("delay_min").
				Title("Min Random Delay").
				Description("Minimum delay between requests (e.g., 100ms, 1s)").
				Value(&values.RandomDelayMin).
				Placeholder("100ms").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("delay_max").
				Title("Max Random Delay").
				Description("Maximum delay between requests (e.g., 500ms, 2s)").
				Value(&values.RandomDelayMax).
				Placeholder("500ms").
				CharLimit(10).
				Validate(ValidateDuration),
		),
	).WithTheme(GetTheme())
}

func CreateLoggingForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewSelect[string]().
				Key("level").
				Title("Log Level").
				Description("Minimum log level to display").
				Options(
					huh.NewOption("Trace", "trace"),
					huh.NewOption("Debug", "debug"),
					huh.NewOption("Info", "info"),
					huh.NewOption("Warn", "warn"),
					huh.NewOption("Error", "error"),
				).
				Value(&values.LogLevel),

			huh.NewSelect[string]().
				Key("format").
				Title("Log Format").
				Description("Output format for logs").
				Options(
					huh.NewOption("Pretty (human-readable)", "pretty"),
					huh.NewOption("JSON (structured)", "json"),
					huh.NewOption("Text (plain)", "text"),
				).
				Value(&values.LogFormat),
		),
	).WithTheme(GetTheme())
}

func CreateLLMForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewSelect[string]().
				Key("provider").
				Title("LLM Provider").
				Description("AI provider for metadata enrichment").
				Options(
					huh.NewOption("None (disabled)", ""),
					huh.NewOption("OpenAI", "openai"),
					huh.NewOption("Anthropic", "anthropic"),
					huh.NewOption("Google", "google"),
				).
				Value(&values.LLMProvider),

			huh.NewInput().
				Key("api_key").
				Title("API Key").
				Description("API key for the selected provider").
				Value(&values.LLMAPIKey).
				EchoMode(huh.EchoModePassword),

			huh.NewInput().
				Key("base_url").
				Title("Base URL").
				Description("Custom API endpoint (leave empty for default)").
				Value(&values.LLMBaseURL).
				Placeholder("https://api.openai.com/v1").
				CharLimit(256),

			huh.NewInput().
				Key("model").
				Title("Model").
				Description("Model name to use").
				Value(&values.LLMModel).
				Placeholder("gpt-4o-mini").
				CharLimit(64),
		),
		huh.NewGroup(
			huh.NewInput().
				Key("max_tokens").
				Title("Max Tokens").
				Description("Maximum tokens for LLM response").
				Value(&values.LLMMaxTokens).
				Placeholder("1000").
				CharLimit(10).
				Validate(ValidatePositiveInt),

			huh.NewInput().
				Key("temperature").
				Title("Temperature").
				Description("Creativity level (0.0-2.0)").
				Value(&values.LLMTemperature).
				Placeholder("0.7").
				CharLimit(10).
				Validate(ValidateFloatRange(0, 2)),

			huh.NewInput().
				Key("timeout").
				Title("LLM Timeout").
				Description("Timeout for LLM requests").
				Value(&values.LLMTimeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewConfirm().
				Key("enhance_metadata").
				Title("Enhance Metadata").
				Description("Use LLM to generate summaries and tags").
				Value(&values.LLMEnhanceMetadata),
		),
	).WithTheme(GetTheme())
}

func CreateExcludeForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewText().
				Key("exclude_patterns").
				Title("Exclude Patterns").
				Description("Regex patterns to exclude (one per line)").
				Value(&values.ExcludePatterns).
				Placeholder(".*\\.pdf$\n.*/login.*"),
		),
	).WithTheme(GetTheme())
}

func CreateRateLimitForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Rate Limiting").
				Description("Enable rate limiting for LLM API calls").
				Value(&values.RateLimitEnabled),

			huh.NewInput().
				Key("requests_per_minute").
				Title("Requests Per Minute").
				Description("Maximum requests per minute (1-1000)").
				Value(&values.RateLimitRequestsPerMinute).
				Placeholder("60").
				CharLimit(4).
				Validate(ValidateIntRange(1, 1000)),

			huh.NewInput().
				Key("burst_size").
				Title("Burst Size").
				Description("Maximum burst requests (1-100)").
				Value(&values.RateLimitBurstSize).
				Placeholder("10").
				CharLimit(3).
				Validate(ValidateIntRange(1, 100)),

			huh.NewInput().
				Key("max_retries").
				Title("Max Retries").
				Description("Maximum retry attempts (0-10)").
				Value(&values.RateLimitMaxRetries).
				Placeholder("3").
				CharLimit(2).
				Validate(ValidateIntRange(0, 10)),
		),
		huh.NewGroup(
			huh.NewInput().
				Key("initial_delay").
				Title("Initial Delay").
				Description("Initial delay between retries (e.g., 1s)").
				Value(&values.RateLimitInitialDelay).
				Placeholder("1s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("max_delay").
				Title("Max Delay").
				Description("Maximum delay between retries (e.g., 1m)").
				Value(&values.RateLimitMaxDelay).
				Placeholder("1m0s").
				CharLimit(10).
				Validate(ValidateDuration),

			huh.NewInput().
				Key("multiplier").
				Title("Backoff Multiplier").
				Description("Backoff multiplier (1.0-5.0)").
				Value(&values.RateLimitMultiplier).
				Placeholder("2.0").
				CharLimit(10).
				Validate(ValidateFloatRange(1.0, 5.0)),
		),
	).WithTheme(GetTheme())
}

func CreateCircuitBreakerForm(values *ConfigValues) *huh.Form {
	return huh.NewForm(
		huh.NewGroup(
			huh.NewConfirm().
				Key("enabled").
				Title("Enable Circuit Breaker").
				Description("Enable circuit breaker for LLM API calls").
				Value(&values.CircuitBreakerEnabled),

			huh.NewInput().
				Key("failure_threshold").
				Title("Failure Threshold").
				Description("Failures before opening circuit (1-50)").
				Value(&values.CircuitBreakerFailureThreshold).
				Placeholder("5").
				CharLimit(2).
				Validate(ValidateIntRange(1, 50)),

			huh.NewInput().
				Key("success_threshold").
				Title("Success Threshold").
				Description("Successes in half-open to close (1-10)").
				Value(&values.CircuitBreakerSuccessThreshold).
				Placeholder("1").
				CharLimit(2).
				Validate(ValidateIntRange(1, 10)),

			huh.NewInput().
				Key("reset_timeout").
				Title("Reset Timeout").
				Description("Time before half-open state (e.g., 30s)").
				Value(&values.CircuitBreakerResetTimeout).
				Placeholder("30s").
				CharLimit(10).
				Validate(ValidateDuration),
		),
	).WithTheme(GetTheme())
}

func GetFormForCategory(category string, values *ConfigValues) *huh.Form {
	switch category {
	case "output":
		return CreateOutputForm(values)
	case "exclude":
		return CreateExcludeForm(values)
	case "concurrency":
		return CreateConcurrencyForm(values)
	case "cache":
		return CreateCacheForm(values)
	case "rendering":
		return CreateRenderingForm(values)
	case "stealth":
		return CreateStealthForm(values)
	case "logging":
		return CreateLoggingForm(values)
	case "llm", "llm_basic":
		return CreateLLMForm(values)
	case "llm_rate_limit":
		return CreateRateLimitForm(values)
	case "llm_circuit_breaker":
		return CreateCircuitBreakerForm(values)
	default:
		return nil
	}
}
</file>
<file path="internal/tui/styles.go">
// Package tui provides an interactive terminal user interface for configuring RepoDocs.
package tui

import (
	"github.com/charmbracelet/huh"
	"github.com/charmbracelet/lipgloss"
)

var (
	// Theme colors
	primaryColor = lipgloss.AdaptiveColor{Light: "#5A56E0", Dark: "#7571F9"}
	successColor = lipgloss.AdaptiveColor{Light: "#02BA84", Dark: "#02BF87"}
	errorColor   = lipgloss.AdaptiveColor{Light: "#FE5F86", Dark: "#FE5F86"}
	mutedColor   = lipgloss.AdaptiveColor{Light: "#9B9B9B", Dark: "#5C5C5C"}
	warnColor    = lipgloss.AdaptiveColor{Light: "#FF9500", Dark: "#FFAA33"}

	// TitleStyle is used for main headers
	TitleStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(primaryColor).
			MarginBottom(1)

	// SubtitleStyle is used for section headers
	SubtitleStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(mutedColor)

	// DescriptionStyle is used for help text
	DescriptionStyle = lipgloss.NewStyle().
				Foreground(mutedColor)

	// SuccessStyle is used for success messages
	SuccessStyle = lipgloss.NewStyle().
			Foreground(successColor)

	// ErrorStyle is used for error messages
	ErrorStyle = lipgloss.NewStyle().
			Foreground(errorColor)

	// WarnStyle is used for warning messages
	WarnStyle = lipgloss.NewStyle().
			Foreground(warnColor)

	// BoxStyle is used for bordered containers
	BoxStyle = lipgloss.NewStyle().
			Border(lipgloss.RoundedBorder()).
			BorderForeground(primaryColor).
			Padding(1, 2)

	// SelectedStyle is used for highlighted menu items
	SelectedStyle = lipgloss.NewStyle().
			Bold(true).
			Foreground(primaryColor)

	// UnselectedStyle is used for normal menu items
	UnselectedStyle = lipgloss.NewStyle().
			Foreground(lipgloss.Color("252"))

	// HelpStyle is used for keyboard shortcut hints
	HelpStyle = lipgloss.NewStyle().
			Foreground(mutedColor).
			MarginTop(1)
)

// GetTheme returns the huh theme for forms
func GetTheme() *huh.Theme {
	return huh.ThemeCharm()
}

// GetAccessibleTheme returns an accessible theme for screen readers
func GetAccessibleTheme() *huh.Theme {
	return huh.ThemeBase()
}
</file>
<file path="internal/tui/validation.go">
package tui

import (
	"errors"
	"fmt"
	"strconv"
	"strings"
	"time"
)

// Validation error messages
var (
	ErrRequired      = errors.New("this field is required")
	ErrInvalidNumber = errors.New("must be a valid number")
	ErrPositiveInt   = errors.New("must be a positive integer")
	ErrInvalidRange  = errors.New("value out of valid range")
)

// ValidateRequired ensures a string value is not empty
func ValidateRequired(s string) error {
	if strings.TrimSpace(s) == "" {
		return ErrRequired
	}
	return nil
}

// ValidateDuration validates that a string can be parsed as a time.Duration
func ValidateDuration(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil // Empty is valid (will use default)
	}
	_, err := time.ParseDuration(s)
	if err != nil {
		return fmt.Errorf("invalid duration format (use: 30s, 5m, 1h): %w", err)
	}
	return nil
}

// ValidatePositiveInt validates that a string represents a positive integer
func ValidatePositiveInt(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil // Empty is valid (will use default)
	}
	n, err := strconv.Atoi(s)
	if err != nil {
		return ErrInvalidNumber
	}
	if n < 1 {
		return ErrPositiveInt
	}
	return nil
}

// ValidateIntRange validates that a string represents an integer within a range
func ValidateIntRange(min, max int) func(string) error {
	return func(s string) error {
		if strings.TrimSpace(s) == "" {
			return nil
		}
		n, err := strconv.Atoi(s)
		if err != nil {
			return ErrInvalidNumber
		}
		if n < min || n > max {
			return fmt.Errorf("%w: must be between %d and %d", ErrInvalidRange, min, max)
		}
		return nil
	}
}

// ValidateFloat validates that a string represents a valid float64
func ValidateFloat(s string) error {
	if strings.TrimSpace(s) == "" {
		return nil
	}
	_, err := strconv.ParseFloat(s, 64)
	if err != nil {
		return fmt.Errorf("must be a valid decimal number")
	}
	return nil
}

// ValidateFloatRange validates that a string represents a float within a range
func ValidateFloatRange(min, max float64) func(string) error {
	return func(s string) error {
		if strings.TrimSpace(s) == "" {
			return nil
		}
		n, err := strconv.ParseFloat(s, 64)
		if err != nil {
			return fmt.Errorf("must be a valid decimal number")
		}
		if n < min || n > max {
			return fmt.Errorf("%w: must be between %.2f and %.2f", ErrInvalidRange, min, max)
		}
		return nil
	}
}

// ValidateLogLevel validates log level values
func ValidateLogLevel(s string) error {
	validLevels := map[string]bool{
		"trace": true,
		"debug": true,
		"info":  true,
		"warn":  true,
		"error": true,
		"fatal": true,
		"panic": true,
	}
	if !validLevels[strings.ToLower(s)] {
		return fmt.Errorf("invalid log level: must be one of trace, debug, info, warn, error, fatal, panic")
	}
	return nil
}

// ValidateLogFormat validates log format values
func ValidateLogFormat(s string) error {
	validFormats := map[string]bool{
		"json":   true,
		"pretty": true,
		"text":   true,
	}
	if !validFormats[strings.ToLower(s)] {
		return fmt.Errorf("invalid log format: must be json, pretty, or text")
	}
	return nil
}

// ValidateLLMProvider validates LLM provider values
func ValidateLLMProvider(s string) error {
	if s == "" {
		return nil // Empty is valid (LLM disabled)
	}
	validProviders := map[string]bool{
		"openai":    true,
		"anthropic": true,
		"google":    true,
	}
	if !validProviders[strings.ToLower(s)] {
		return fmt.Errorf("invalid LLM provider: must be openai, anthropic, or google")
	}
	return nil
}
</file>
<file path="internal/utils/fs.go">
package utils

import (
	"net/url"
	"os"
	"path/filepath"
	"regexp"
	"strings"
	"unicode"
)

// MaxFilenameLength is the maximum length for a filename
const MaxFilenameLength = 200

// Windows reserved names
var windowsReserved = map[string]bool{
	"CON": true, "PRN": true, "AUX": true, "NUL": true,
	"COM1": true, "COM2": true, "COM3": true, "COM4": true,
	"COM5": true, "COM6": true, "COM7": true, "COM8": true, "COM9": true,
	"LPT1": true, "LPT2": true, "LPT3": true, "LPT4": true,
	"LPT5": true, "LPT6": true, "LPT7": true, "LPT8": true, "LPT9": true,
}

// invalidCharsRegex matches invalid filename characters
var invalidCharsRegex = regexp.MustCompile(`[<>:"|?*\\/]`)

// multipleSpacesRegex matches multiple consecutive spaces/dashes
var multipleSpacesRegex = regexp.MustCompile(`[-_\s]+`)

// SanitizeFilename sanitizes a string for use as a filename
func SanitizeFilename(name string) string {
	original := name

	// Remove invalid characters
	name = invalidCharsRegex.ReplaceAllString(name, "-")

	// Replace multiple spaces/dashes with single dash
	name = multipleSpacesRegex.ReplaceAllString(name, "-")

	// Separate extension from base name
	ext := filepath.Ext(name)
	baseName := strings.TrimSuffix(name, ext)

	// Trim leading/trailing dashes and spaces from base name
	baseName = strings.Trim(baseName, "- ")

	// Check if we had invalid character substitutions
	// If original had invalid chars that created dashes before extension,
	// and the extension exists, preserve one dash before extension
	hadSubstitutions := (original != name) && invalidCharsRegex.MatchString(original)
	if hadSubstitutions && ext != "" && strings.HasSuffix(name, "-."+ext[1:]) {
		// Reconstruct with dash before extension
		name = baseName + "-" + ext
	} else {
		// Reconstruct normally
		if ext != "" {
			name = baseName + ext
		} else {
			name = baseName
		}
	}

	// Check for Windows reserved names
	upper := strings.ToUpper(name)
	baseNameUpper := strings.TrimSuffix(upper, filepath.Ext(upper))
	if windowsReserved[baseNameUpper] {
		name = "_" + name
	}

	// Limit length
	if len(name) > MaxFilenameLength {
		ext := filepath.Ext(name)
		name = name[:MaxFilenameLength-len(ext)] + ext
	}

	// Ensure the name is not empty
	if name == "" {
		name = "untitled"
	}

	return name
}

// URLToFilename converts a URL to a safe filename
func URLToFilename(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return SanitizeFilename(rawURL)
	}

	// Get path and remove leading/trailing slashes
	path := strings.Trim(u.Path, "/")
	if path == "" {
		path = "index"
	}

	// Replace path separators with dashes for flat structure
	path = strings.ReplaceAll(path, "/", "-")

	// Remove common file extensions
	path = strings.TrimSuffix(path, ".html")
	path = strings.TrimSuffix(path, ".htm")
	path = strings.TrimSuffix(path, ".php")

	// Sanitize and add .md extension
	filename := SanitizeFilename(path)
	if !strings.HasSuffix(filename, ".md") {
		filename += ".md"
	}

	return filename
}

// URLToPath converts a URL to a nested directory path
func URLToPath(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return SanitizeFilename(rawURL) + ".md"
	}

	// Get path and remove leading/trailing slashes
	path := strings.Trim(u.Path, "/")
	if path == "" {
		path = "index"
	}

	// Remove common file extensions
	path = strings.TrimSuffix(path, ".html")
	path = strings.TrimSuffix(path, ".htm")
	path = strings.TrimSuffix(path, ".php")

	// Split path and sanitize each component
	parts := strings.Split(path, "/")
	for i, part := range parts {
		parts[i] = SanitizeFilename(part)
	}

	// Join with OS-specific separator
	result := filepath.Join(parts...)

	// Add .md extension if not present
	if !strings.HasSuffix(result, ".md") {
		result += ".md"
	}

	return result
}

// GeneratePath generates the output path for a URL
func GeneratePath(baseDir, rawURL string, flat bool) string {
	var relativePath string
	if flat {
		relativePath = URLToFilename(rawURL)
	} else {
		relativePath = URLToPath(rawURL)
	}
	return filepath.Join(baseDir, relativePath)
}

// GeneratePathFromRelative generates the output path from a relative file path
// Used for Git-sourced files to preserve the repository's directory structure
func GeneratePathFromRelative(baseDir, relPath string, flat bool) string {
	if flat {
		// For flat mode, convert full path to filename by replacing "/" with "-"
		// Example: docs/developers/tools/memory.md → docs-developers-tools-memory.md

		// Normalize separators to forward slash
		normalized := filepath.ToSlash(relPath)

		// Remove .md/.mdx extension if present
		ext := filepath.Ext(normalized)
		if ext == ".md" || ext == ".mdx" {
			normalized = strings.TrimSuffix(normalized, ext)
		}

		// Replace "/" with "-" to create flat filename
		flatName := strings.ReplaceAll(normalized, "/", "-")

		// Sanitize the result
		flatName = SanitizeFilename(flatName)

		// Add .md extension
		if !strings.HasSuffix(flatName, ".md") {
			flatName += ".md"
		}

		return filepath.Join(baseDir, flatName)
	}

	// For nested mode, preserve directory structure
	// Ensure path uses OS-specific separators
	relPath = filepath.FromSlash(relPath)

	// Sanitize each component
	parts := strings.Split(relPath, string(filepath.Separator))
	for i, part := range parts {
		parts[i] = SanitizeFilename(part)
	}
	result := filepath.Join(parts...)

	// Add .md extension if not present
	if !strings.HasSuffix(result, ".md") {
		result += ".md"
	}

	return filepath.Join(baseDir, result)
}

// JSONPath returns the corresponding JSON metadata path for a markdown file
func JSONPath(mdPath string) string {
	return strings.TrimSuffix(mdPath, ".md") + ".json"
}

// IsValidFilename checks if a filename is valid
func IsValidFilename(name string) bool {
	if name == "" || name == "." || name == ".." {
		return false
	}

	// Check for invalid characters
	if invalidCharsRegex.MatchString(name) {
		return false
	}

	// Check for Windows reserved names
	upper := strings.ToUpper(name)
	baseName := strings.TrimSuffix(upper, filepath.Ext(upper))
	if windowsReserved[baseName] {
		return false
	}

	// Check for control characters
	for _, r := range name {
		if unicode.IsControl(r) {
			return false
		}
	}

	return true
}

// EnsureDir ensures a directory exists, creating it if necessary
func EnsureDir(path string) error {
	dir := filepath.Dir(path)
	return os.MkdirAll(dir, 0755)
}

// ExpandPath expands ~ to the user's home directory
func ExpandPath(path string) string {
	if strings.HasPrefix(path, "~/") {
		home, err := os.UserHomeDir()
		if err != nil {
			return path
		}
		return filepath.Join(home, path[2:])
	}
	if path == "~" {
		home, err := os.UserHomeDir()
		if err != nil {
			return path
		}
		return home
	}
	return path
}
</file>
<file path="internal/utils/logger.go">
package utils

import (
	"io"
	"os"
	"time"

	"github.com/rs/zerolog"
)

// Logger is a wrapper around zerolog.Logger
type Logger struct {
	zerolog.Logger
}

// LoggerOptions contains options for creating a logger
type LoggerOptions struct {
	Level   string
	Format  string // "pretty" or "json"
	Output  io.Writer
	Verbose bool
}

// NewLogger creates a new logger with the given options
func NewLogger(opts LoggerOptions) *Logger {
	var output io.Writer = os.Stderr
	if opts.Output != nil {
		output = opts.Output
	}

	// Set up pretty or JSON output
	if opts.Format == "pretty" {
		output = zerolog.ConsoleWriter{
			Out:        output,
			TimeFormat: time.RFC3339,
		}
	}

	// Parse log level
	level := parseLogLevel(opts.Level)
	if opts.Verbose {
		level = zerolog.DebugLevel
	}

	// Create logger
	logger := zerolog.New(output).
		Level(level).
		With().
		Timestamp().
		Logger()

	return &Logger{Logger: logger}
}

// NewDefaultLogger creates a logger with default settings
func NewDefaultLogger() *Logger {
	return NewLogger(LoggerOptions{
		Level:  "info",
		Format: "pretty",
	})
}

// NewVerboseLogger creates a verbose logger
func NewVerboseLogger() *Logger {
	return NewLogger(LoggerOptions{
		Level:   "debug",
		Format:  "pretty",
		Verbose: true,
	})
}

// parseLogLevel parses a log level string
func parseLogLevel(level string) zerolog.Level {
	switch level {
	case "debug":
		return zerolog.DebugLevel
	case "info":
		return zerolog.InfoLevel
	case "warn":
		return zerolog.WarnLevel
	case "error":
		return zerolog.ErrorLevel
	default:
		return zerolog.InfoLevel
	}
}

// WithComponent returns a logger with a component field
func (l *Logger) WithComponent(component string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("component", component).Logger(),
	}
}

// WithURL returns a logger with a URL field
func (l *Logger) WithURL(url string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("url", url).Logger(),
	}
}

// WithStrategy returns a logger with a strategy field
func (l *Logger) WithStrategy(strategy string) *Logger {
	return &Logger{
		Logger: l.Logger.With().Str("strategy", strategy).Logger(),
	}
}

// SetGlobalLevel sets the global log level
func SetGlobalLevel(level string) {
	zerolog.SetGlobalLevel(parseLogLevel(level))
}
</file>
<file path="internal/utils/progress.go">
package utils

import "github.com/schollz/progressbar/v3"

// Standard progress bar descriptions
const (
	DescCrawling    = "Crawling"
	DescDownloading = "Downloading"
	DescProcessing  = "Processing"
	DescExtracting  = "Extracting"
)

// NewProgressBar creates a consistently styled progress bar.
//
// Parameters:
//   - total: Total number of items. Use -1 for unknown totals (indeterminate/spinner mode).
//   - description: Text description to show before the progress bar (e.g., DescCrawling, DescDownloading).
//
// Behavior:
//   - For unknown totals (total < 0): Uses spinner type 14 with blank state rendering.
//   - For known totals (total >= 0): Shows count and iterations/second (its).
//   - All progress bars show count.
//
// Example:
//
//	bar := utils.NewProgressBar(len(items), utils.DescDownloading)
//	defer bar.Finish()
//
//	for _, item := range items {
//	    // Process item
//	    bar.Add(1)
//	}
func NewProgressBar(total int, description string) *progressbar.ProgressBar {
	// Build common options
	opts := []progressbar.Option{
		progressbar.OptionSetDescription(description),
		progressbar.OptionShowCount(),
	}

	// Add options based on whether total is known
	if total < 0 {
		// Unknown total: use spinner mode
		opts = append(opts,
			progressbar.OptionSpinnerType(14),
			progressbar.OptionSetRenderBlankState(true),
		)
	} else {
		// Known total: show iterations/second
		opts = append(opts,
			progressbar.OptionShowIts(),
		)
	}

	return progressbar.NewOptions(total, opts...)
}
</file>
<file path="internal/utils/url.go">
package utils

import (
	"net/url"
	"path"
	"regexp"
	"strings"
)

// NormalizeURL normalizes a URL for consistent handling
func NormalizeURL(rawURL string) (string, error) {
	// If no scheme is present, prepend https:// before parsing
	// This ensures the host is correctly identified
	if !strings.Contains(rawURL, "://") && !strings.HasPrefix(rawURL, "//") {
		rawURL = "https://" + rawURL
	}

	u, err := url.Parse(rawURL)
	if err != nil {
		return "", err
	}

	// Ensure scheme
	if u.Scheme == "" {
		u.Scheme = "https"
	}

	// Normalize host to lowercase
	u.Host = strings.ToLower(u.Host)

	// Remove default ports
	if (u.Scheme == "http" && u.Port() == "80") ||
		(u.Scheme == "https" && u.Port() == "443") {
		u.Host = u.Hostname()
	}

	// Clean path
	if u.Path == "" {
		u.Path = "/"
	} else {
		u.Path = path.Clean(u.Path)
	}

	// Remove trailing slash (except for root)
	if u.Path != "/" && strings.HasSuffix(u.Path, "/") {
		u.Path = strings.TrimSuffix(u.Path, "/")
	}

	// Remove fragment
	u.Fragment = ""

	// Build the result manually to ensure trailing slash for root path
	result := u.String()

	// Ensure root path has trailing slash
	if u.Path == "/" && u.RawQuery == "" && !strings.HasSuffix(result, "/") {
		result += "/"
	}

	return result, nil
}

// NormalizeURLWithoutQuery normalizes a URL and removes query parameters
func NormalizeURLWithoutQuery(rawURL string) (string, error) {
	normalized, err := NormalizeURL(rawURL)
	if err != nil {
		return "", err
	}

	u, err := url.Parse(normalized)
	if err != nil {
		return "", err
	}

	u.RawQuery = ""
	return u.String(), nil
}

// ResolveURL resolves a relative URL against a base URL
func ResolveURL(base, ref string) (string, error) {
	// If the base doesn't end with / and doesn't have a file extension,
	// treat it as a directory by adding a trailing slash
	// This ensures that "../page" from "/docs/api" resolves to "/docs/page" not "/page"
	if !strings.HasSuffix(base, "/") && !strings.Contains(path.Base(base), ".") {
		base += "/"
	}

	baseURL, err := url.Parse(base)
	if err != nil {
		return "", err
	}

	refURL, err := url.Parse(ref)
	if err != nil {
		return "", err
	}

	resolved := baseURL.ResolveReference(refURL)
	return resolved.String(), nil
}

// GetDomain extracts the domain from a URL
func GetDomain(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return ""
	}
	return u.Host
}

// GetBaseDomain extracts the base domain from a URL, removing only the "www" prefix
// For example: "www.example.com" -> "example.com", "docs.example.com" -> "docs.example.com"
func GetBaseDomain(rawURL string) string {
	host := GetDomain(rawURL)
	if host == "" {
		return ""
	}

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	// Only strip "www." prefix, keep other subdomains
	if strings.HasPrefix(strings.ToLower(host), "www.") {
		return host[4:]
	}

	return host
}

// extractRootDomain extracts the root domain (domain + TLD) without any subdomains
// For example: "docs.example.com" -> "example.com", "www.example.com" -> "example.com"
func extractRootDomain(rawURL string) string {
	host := GetDomain(rawURL)
	if host == "" {
		return ""
	}

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	// Split by dots
	parts := strings.Split(strings.ToLower(host), ".")

	// Need at least 2 parts (domain + TLD)
	if len(parts) < 2 {
		return host
	}

	// Extract the last 2 parts (domain + TLD)
	// This works for most TLDs like .com, .org, .net
	// Note: This is a simplified approach and won't work perfectly for
	// compound TLDs like .co.uk, but covers the common cases
	return strings.Join(parts[len(parts)-2:], ".")
}

// IsSameDomain checks if two URLs have the same domain
func IsSameDomain(url1, url2 string) bool {
	return strings.EqualFold(GetDomain(url1), GetDomain(url2))
}

// IsSameBaseDomain checks if two URLs have the same base domain (ignoring subdomains)
// For example: "docs.example.com" and "api.example.com" return true
func IsSameBaseDomain(url1, url2 string) bool {
	return extractRootDomain(url1) == extractRootDomain(url2)
}

// IsAbsoluteURL checks if a URL is absolute
func IsAbsoluteURL(rawURL string) bool {
	// Protocol-relative URLs (starting with //) are considered absolute
	if strings.HasPrefix(rawURL, "//") {
		return true
	}

	u, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	return u.IsAbs()
}

// IsHTTPURL checks if a URL uses HTTP or HTTPS scheme
func IsHTTPURL(rawURL string) bool {
	u, err := url.Parse(rawURL)
	if err != nil {
		return false
	}
	return u.Scheme == "http" || u.Scheme == "https"
}

// IsGitURL checks if a URL is a git repository URL
func IsGitURL(rawURL string) bool {
	return strings.HasPrefix(rawURL, "git@") ||
		strings.HasSuffix(rawURL, ".git") ||
		strings.Contains(rawURL, "github.com") ||
		strings.Contains(rawURL, "gitlab.com") ||
		strings.Contains(rawURL, "bitbucket.org")
}

// IsSitemapURL checks if a URL points to a sitemap
func IsSitemapURL(rawURL string) bool {
	lower := strings.ToLower(rawURL)
	return strings.HasSuffix(lower, "sitemap.xml") ||
		strings.HasSuffix(lower, "sitemap.xml.gz") ||
		strings.Contains(lower, "sitemap")
}

// IsLLMSURL checks if a URL points to an llms.txt file
func IsLLMSURL(rawURL string) bool {
	lower := strings.ToLower(rawURL)
	return strings.HasSuffix(lower, "/llms.txt") ||
		strings.HasSuffix(lower, "llms.txt")
}

// IsPkgGoDevURL checks if a URL is a pkg.go.dev URL
func IsPkgGoDevURL(rawURL string) bool {
	return strings.Contains(rawURL, "pkg.go.dev")
}

// ExtractLinks extracts all href links from HTML content
// This is a simple regex-based extraction, use goquery for more robust parsing
func ExtractLinks(html, baseURL string) []string {
	linkRegex := regexp.MustCompile(`href=["']([^"']+)["']`)
	matches := linkRegex.FindAllStringSubmatch(html, -1)

	links := make([]string, 0, len(matches))
	for _, match := range matches {
		if len(match) > 1 {
			link := match[1]
			// Skip anchors, javascript, mailto, etc.
			if strings.HasPrefix(link, "#") ||
				strings.HasPrefix(link, "javascript:") ||
				strings.HasPrefix(link, "mailto:") ||
				strings.HasPrefix(link, "tel:") {
				continue
			}

			// Resolve relative URLs
			if !IsAbsoluteURL(link) {
				resolved, err := ResolveURL(baseURL, link)
				if err != nil {
					continue
				}
				link = resolved
			}

			links = append(links, link)
		}
	}

	return links
}

// GenerateOutputDirFromURL generates an output directory name from a URL
// Examples:
//   - https://github.com/QwenLM/qwen-code -> docs_qwen-code
//   - https://docs.crawl4ai.com/sitemap.xml -> docs_docscrawl4aicom
//   - https://docs.factory.ai/llms.txt -> docs_docsfactoryai
//   - https://pkg.go.dev/github.com/user/package -> docs_package
//   - https://docs.rs/ratatui/latest/ratatui/ -> docs_ratatui
func GenerateOutputDirFromURL(rawURL string) string {
	u, err := url.Parse(rawURL)
	if err != nil {
		return "docs"
	}

	host := strings.ToLower(u.Host)
	pathStr := strings.Trim(u.Path, "/")

	// Remove port if present
	if idx := strings.LastIndex(host, ":"); idx != -1 {
		host = host[:idx]
	}

	var name string

	// Handle Git repository URLs (GitHub, GitLab, Bitbucket)
	if strings.Contains(host, "github.com") ||
		strings.Contains(host, "gitlab.com") ||
		strings.Contains(host, "bitbucket.org") {
		// Extract repository name from path
		parts := strings.Split(pathStr, "/")
		if len(parts) >= 2 {
			// Use the repo name (second part: owner/repo)
			name = parts[1]
			// Remove .git suffix if present
			name = strings.TrimSuffix(name, ".git")
		} else if len(parts) == 1 && parts[0] != "" {
			name = parts[0]
		}
	}

	// Handle pkg.go.dev URLs
	if name == "" && strings.Contains(host, "pkg.go.dev") {
		// Path is like: /github.com/user/package or /package
		parts := strings.Split(pathStr, "/")
		if len(parts) > 0 {
			// Use the last significant part
			for i := len(parts) - 1; i >= 0; i-- {
				if parts[i] != "" && !strings.Contains(parts[i], ".") {
					name = parts[i]
					break
				}
			}
			// Fallback to last part
			if name == "" && len(parts) > 0 {
				name = parts[len(parts)-1]
			}
		}
	}

	// Handle docs.rs URLs
	if name == "" && strings.Contains(host, "docs.rs") {
		parts := strings.Split(pathStr, "/")
		if len(parts) >= 1 && parts[0] != "" {
			if parts[0] == "crate" && len(parts) >= 2 {
				name = parts[1]
			} else {
				name = parts[0]
			}
		}
	}

	// For other URLs, use sanitized hostname
	if name == "" {
		// Remove common prefixes
		host = strings.TrimPrefix(host, "www.")

		// Concatenate all parts of the domain (removes dots)
		// example.com -> examplecom
		// docs.crawl4ai.com -> docscrawl4aicom
		// This creates a clean directory name from the full domain
		name = sanitizeForDirName(host)
	}

	// Ensure we have a valid name
	if name == "" {
		return "docs"
	}

	// Sanitize the name for filesystem
	name = sanitizeForDirName(name)

	return "docs_" + name
}

// sanitizeForDirName removes characters that are not safe for directory names
func sanitizeForDirName(s string) string {
	// Remove dots, spaces, and special characters
	var result strings.Builder
	for _, r := range strings.ToLower(s) {
		if (r >= 'a' && r <= 'z') || (r >= '0' && r <= '9') || r == '-' || r == '_' {
			result.WriteRune(r)
		}
	}
	return result.String()
}

// HasBaseURL checks if a URL starts with the given base URL path
// Example: HasBaseURL("https://example.com/docs/api", "https://example.com/docs") returns true
// Example: HasBaseURL("https://example.com/blog", "https://example.com/docs") returns false
func HasBaseURL(targetURL, baseURL string) bool {
	if baseURL == "" {
		return true
	}

	targetParsed, err := url.Parse(targetURL)
	if err != nil {
		return false
	}

	baseParsed, err := url.Parse(baseURL)
	if err != nil {
		return false
	}

	// Must be same host
	if strings.ToLower(targetParsed.Host) != strings.ToLower(baseParsed.Host) {
		return false
	}

	// Normalize paths
	targetPath := strings.TrimSuffix(targetParsed.Path, "/")
	basePath := strings.TrimSuffix(baseParsed.Path, "/")

	// Target path must start with base path
	if basePath == "" || basePath == "/" {
		return true
	}

	return targetPath == basePath || strings.HasPrefix(targetPath, basePath+"/")
}

// FilterLinks filters links based on patterns
func FilterLinks(links []string, excludePatterns []string) []string {
	var regexps []*regexp.Regexp
	for _, pattern := range excludePatterns {
		re, err := regexp.Compile(pattern)
		if err != nil {
			continue
		}
		regexps = append(regexps, re)
	}

	filtered := make([]string, 0, len(links))
	for _, link := range links {
		excluded := false
		for _, re := range regexps {
			if re.MatchString(link) {
				excluded = true
				break
			}
		}
		if !excluded {
			filtered = append(filtered, link)
		}
	}

	return filtered
}
</file>
<file path="internal/utils/workerpool.go">
package utils

import (
	"context"
	"sync"
)

// Task represents a unit of work
type Task[T any] struct {
	Data   T
	Result any
	Err    error
}

// Worker is a function that processes a task
type Worker[T any] func(ctx context.Context, data T) (any, error)

// Pool is a worker pool for concurrent task processing
type Pool[T any] struct {
	workers    int
	taskQueue  chan *Task[T]
	resultChan chan *Task[T]
	wg         sync.WaitGroup
	worker     Worker[T]
	stopOnce   sync.Once
}

// NewPool creates a new worker pool
func NewPool[T any](workers int, worker Worker[T]) *Pool[T] {
	return &Pool[T]{
		workers:    workers,
		taskQueue:  make(chan *Task[T], workers*2),
		resultChan: make(chan *Task[T], workers*2),
		worker:     worker,
	}
}

// Start starts the worker pool
func (p *Pool[T]) Start(ctx context.Context) {
	for i := 0; i < p.workers; i++ {
		p.wg.Add(1)
		go p.runWorker(ctx)
	}
}

// runWorker runs a single worker
func (p *Pool[T]) runWorker(ctx context.Context) {
	defer p.wg.Done()

	for {
		select {
		case <-ctx.Done():
			return
		case task, ok := <-p.taskQueue:
			if !ok {
				return
			}
			result, err := p.worker(ctx, task.Data)
			task.Result = result
			task.Err = err

			select {
			case p.resultChan <- task:
			case <-ctx.Done():
				return
			}
		}
	}
}

// Submit submits a task to the pool
func (p *Pool[T]) Submit(data T) {
	p.taskQueue <- &Task[T]{Data: data}
}

// Results returns the results channel
func (p *Pool[T]) Results() <-chan *Task[T] {
	return p.resultChan
}

// Stop stops the pool and waits for workers to finish
func (p *Pool[T]) Stop() {
	p.stopOnce.Do(func() {
		close(p.taskQueue)
		p.wg.Wait()
		close(p.resultChan)
	})
}

// Process processes a slice of data items concurrently
func (p *Pool[T]) Process(ctx context.Context, items []T) ([]*Task[T], error) {
	// Handle empty slice case
	if len(items) == 0 {
		return []*Task[T]{}, nil
	}

	p.Start(ctx)

	// Submit all items
	go func() {
		for _, item := range items {
			select {
			case <-ctx.Done():
				return
			default:
				p.Submit(item)
			}
		}
		close(p.taskQueue)
	}()

	// Collect results with context awareness
	results := make([]*Task[T], 0, len(items))
	collectDone := false
	for !collectDone {
		select {
		case <-ctx.Done():
			collectDone = true
		case task, ok := <-p.resultChan:
			if !ok {
				collectDone = true
			} else {
				results = append(results, task)
				if len(results) == len(items) {
					collectDone = true
				}
			}
		}
	}

	p.wg.Wait()

	// Drain remaining results to avoid goroutine leak
	go func() {
		for range p.resultChan {
		}
	}()
	close(p.resultChan)

	// Check for context error
	if ctx.Err() != nil {
		return results, ctx.Err()
	}

	return results, nil
}

// SimplePool is a simpler worker pool without generics for basic use cases
type SimplePool struct {
	workers int
	wg      sync.WaitGroup
}

// NewSimplePool creates a new simple worker pool
func NewSimplePool(workers int) *SimplePool {
	return &SimplePool{workers: workers}
}

// Run runs tasks concurrently with the given function
func (p *SimplePool) Run(ctx context.Context, tasks []func(context.Context) error) []error {
	errors := make([]error, len(tasks))
	taskChan := make(chan int, len(tasks))
	var mu sync.Mutex

	// Start workers
	for i := 0; i < p.workers; i++ {
		p.wg.Add(1)
		go func() {
			defer p.wg.Done()
			for {
				select {
				case <-ctx.Done():
					return
				case idx, ok := <-taskChan:
					if !ok {
						return
					}
					err := tasks[idx](ctx)
					mu.Lock()
					errors[idx] = err
					mu.Unlock()
				}
			}
		}()
	}

	// Submit tasks
	for i := range tasks {
		select {
		case <-ctx.Done():
			close(taskChan)
			p.wg.Wait()
			return errors
		case taskChan <- i:
		}
	}

	close(taskChan)
	p.wg.Wait()

	return errors
}

// ParallelForEach executes a function for each item in parallel
func ParallelForEach[T any](ctx context.Context, items []T, workers int, fn func(context.Context, T) error) []error {
	if workers <= 0 {
		workers = 1
	}
	if workers > len(items) {
		workers = len(items)
	}

	errors := make([]error, len(items))
	taskChan := make(chan int, len(items))
	var wg sync.WaitGroup
	var mu sync.Mutex

	// Start workers
	for i := 0; i < workers; i++ {
		wg.Add(1)
		go func() {
			defer wg.Done()
			for {
				select {
				case <-ctx.Done():
					return
				case idx, ok := <-taskChan:
					if !ok {
						return
					}
					err := fn(ctx, items[idx])
					mu.Lock()
					errors[idx] = err
					mu.Unlock()
				}
			}
		}()
	}

	// Submit tasks
	for i := range items {
		select {
		case <-ctx.Done():
			close(taskChan)
			wg.Wait()
			return errors
		case taskChan <- i:
		}
	}

	close(taskChan)
	wg.Wait()

	return errors
}

// FirstError returns the first non-nil error from a slice of errors
func FirstError(errors []error) error {
	for _, err := range errors {
		if err != nil {
			return err
		}
	}
	return nil
}

// CollectErrors collects all non-nil errors from a slice
func CollectErrors(errors []error) []error {
	var result []error
	for _, err := range errors {
		if err != nil {
			result = append(result, err)
		}
	}
	return result
}
</file>
<file path="pkg/version/version.go">
package version

import (
	"fmt"
	"runtime"
)

// Build-time variables (set via ldflags)
var (
	Version   = "dev"
	BuildTime = "unknown"
	Commit    = "unknown"
)

// Info contains version information
type Info struct {
	Version   string `json:"version"`
	BuildTime string `json:"build_time"`
	Commit    string `json:"commit"`
	GoVersion string `json:"go_version"`
	OS        string `json:"os"`
	Arch      string `json:"arch"`
}

// Get returns the current version info
func Get() Info {
	return Info{
		Version:   Version,
		BuildTime: BuildTime,
		Commit:    Commit,
		GoVersion: runtime.Version(),
		OS:        runtime.GOOS,
		Arch:      runtime.GOARCH,
	}
}

// String returns a formatted version string
func (i Info) String() string {
	return fmt.Sprintf("repodocs %s (commit: %s, built: %s, %s %s/%s)",
		i.Version, i.Commit, i.BuildTime, i.GoVersion, i.OS, i.Arch)
}

// Short returns a short version string
func Short() string {
	return Version
}

// Full returns a full version string
func Full() string {
	return Get().String()
}
</file>
<file path="scripts/release.sh">
#!/usr/bin/env bash
set -euo pipefail

RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
CYAN='\033[0;36m'
DIM='\033[2m'
NC='\033[0m'

cd "$(dirname "${BASH_SOURCE[0]}")/.."

LATEST_TAG=$(git describe --tags --abbrev=0 2>/dev/null || echo "v0.0.0")
VERSION="${LATEST_TAG#v}"
IFS='.' read -r MAJOR MINOR PATCH <<< "$VERSION"

NEXT_PATCH="v${MAJOR}.${MINOR}.$((PATCH + 1))"
NEXT_MINOR="v${MAJOR}.$((MINOR + 1)).0"
NEXT_MAJOR="v$((MAJOR + 1)).0.0"

echo ""
echo -e "Current version: ${GREEN}${LATEST_TAG}${NC}"
echo ""
echo -e "  ${CYAN}1)${NC} patch  → ${GREEN}${NEXT_PATCH}${NC}"
echo -e "  ${CYAN}2)${NC} minor  → ${GREEN}${NEXT_MINOR}${NC}"
echo -e "  ${CYAN}3)${NC} major  → ${GREEN}${NEXT_MAJOR}${NC}"
echo -e "  ${CYAN}4)${NC} custom"
echo -e "  ${CYAN}q)${NC} quit"
echo ""

read -rp "Select [1-4/q]: " choice

case $choice in
    1) NEW_VERSION="$NEXT_PATCH" ;;
    2) NEW_VERSION="$NEXT_MINOR" ;;
    3) NEW_VERSION="$NEXT_MAJOR" ;;
    4) read -rp "Version (e.g., v1.2.3): " NEW_VERSION ;;
    q|Q) echo "Aborted."; exit 0 ;;
    *) echo -e "${RED}Invalid option${NC}"; exit 1 ;;
esac

if [[ ! "$NEW_VERSION" =~ ^v[0-9]+\.[0-9]+\.[0-9]+$ ]]; then
    echo -e "${RED}Error: Invalid semver format${NC}"
    exit 1
fi

if git rev-parse "$NEW_VERSION" >/dev/null 2>&1; then
    echo -e "${RED}Error: Tag $NEW_VERSION already exists${NC}"
    exit 1
fi

if [[ -n "$(git status --porcelain)" ]]; then
    echo -e "${RED}Error: Uncommitted changes. Commit first.${NC}"
    exit 1
fi

echo ""
echo -e "${CYAN}Commits since ${LATEST_TAG}:${NC}"
echo ""

CHANGELOG=$(git log "${LATEST_TAG}..HEAD" --pretty=format:"- %s" --no-merges 2>/dev/null || echo "- Initial release")
echo -e "${DIM}${CHANGELOG}${NC}"

echo ""
echo -e "Release: ${YELLOW}${LATEST_TAG}${NC} → ${GREEN}${NEW_VERSION}${NC}"
read -rp "Confirm? [y/N]: " confirm

if [[ "$confirm" != "y" && "$confirm" != "Y" ]]; then
    echo "Aborted."
    exit 0
fi

TAG_MESSAGE="Release ${NEW_VERSION}

Changes since ${LATEST_TAG}:
${CHANGELOG}"

echo ""
echo -e "${CYAN}Creating tag...${NC}"
git tag -a "$NEW_VERSION" -m "$TAG_MESSAGE"

echo -e "${CYAN}Pushing to origin...${NC}"
git push origin "$NEW_VERSION"

echo ""
echo -e "${GREEN}✓ Release $NEW_VERSION created!${NC}"
echo ""
echo "GitHub Actions will build and publish the release."
echo -e "Monitor: ${CYAN}https://github.com/quantmind-br/repodocs-go/actions${NC}"
</file>
<file path="AGENTS.md">
# AGENTS.md - Guidelines for repodocs-go

## Build & Test

```bash
make build              # Build binary to ./build/repodocs
make test               # Unit tests (fast, -short)
make test-integration   # Integration tests
make test-e2e           # E2E tests
make lint               # golangci-lint (govet + misspell)
make fmt                # Format code
make deps               # Download and tidy dependencies

# Run single test
go test -v -run TestName ./internal/converter/...
```

## Architecture

**Flow**: URL → Detector → Strategy → Fetcher/Renderer → Converter → Writer

| Package | Purpose |
|---------|---------|
| `internal/app` | Orchestrator, Detector (routes URLs to strategies) |
| `internal/strategies` | crawler, git, sitemap, llms, pkggo |
| `internal/fetcher` | Stealth HTTP client (tls-client) |
| `internal/renderer` | Headless browser (Rod/Chromium) |
| `internal/converter` | HTML → Markdown pipeline |
| `internal/cache` | BadgerDB persistent cache |
| `internal/output` | Markdown writer with frontmatter |
| `internal/domain` | Interfaces, models, errors |

## Code Style

### Imports (3 groups, blank-line separated)
```go
import (
    "context"                                    // 1. Standard library

    "github.com/stretchr/testify/assert"         // 2. External deps

    "github.com/quantmind-br/repodocs-go/internal/domain"  // 3. Internal
)
```

### Naming
- Interfaces: `Fetcher`, `Renderer`, `Cache` (verb-er)
- Structs: `CrawlerStrategy`, `ClientOptions` (PascalCase)
- Constructors: `NewClient()`, `NewOrchestrator()`
- Options: `ClientOptions`, `RetrierOptions`
- Tests: `TestGet_NotFound`

### Error Handling
```go
// Sentinel errors in internal/domain/errors.go
var ErrCacheMiss = errors.New("cache miss")

// Wrap with context
return fmt.Errorf("failed: %w", err)

// Check errors
if errors.Is(err, domain.ErrCacheMiss) { ... }
var fetchErr *domain.FetchError
if errors.As(err, &fetchErr) { ... }
```

### Interfaces (context first, error last)
```go
type Cache interface {
    Get(ctx context.Context, key string) ([]byte, error)
    Set(ctx context.Context, key string, value []byte, ttl time.Duration) error
}

// Options pattern: DefaultXxxOptions() + validation in NewXxx()
type ClientOptions struct { Timeout time.Duration; MaxRetries int }
func DefaultClientOptions() ClientOptions {
    return ClientOptions{Timeout: 30 * time.Second, MaxRetries: 3}
}
```

### Testing
```go
require.NoError(t, err)       // Fails immediately
assert.Equal(t, expected, actual)  // Continues

// Table-driven
tests := []struct {
    name    string
    input   string
    wantErr bool
}{
    {name: "valid", input: "test", wantErr: false},
}
for _, tt := range tests {
    t.Run(tt.name, func(t *testing.T) { ... })
}

// Use t.TempDir() for temp files (auto-cleanup)
```

Test location: `tests/unit/<package>/` mirrors `internal/<package>/`

### Logging (zerolog via utils.Logger)
```go
s.logger.Info().Str("url", url).Msg("Starting extraction")
s.logger.Warn().Err(err).Str("url", url).Msg("Failed")
```

### Concurrency
- Always accept `context.Context` for cancellation
- Use `sync.Map` for concurrent maps, `sync.Mutex` for simple state
- Check context in loops: `select { case <-ctx.Done(): return ctx.Err() default: }`

## DO NOT

- Suppress errors silently (`_ = err`)
- Use `panic` for recoverable errors
- Import from `cmd/` in `internal/`
- Create circular dependencies
- Use type assertions without error checking

## Task Tracking (bd)

```bash
bd ready                    # Find unblocked work
bd create "Title" -t task -p 2    # Create issue
bd update bd-42 --status in_progress   # Update
bd close bd-42 --reason "Done"   # Complete
bd sync                    # Sync with git (run at session end)
```

**Priorities**: 0=Critical, 1=High, 2=Medium, 3=Low, 4=Backlog
**Types**: bug, feature, task, epic, chore

**Workflow**:
1. `bd ready` to find work
2. Claim and implement
3. Create linked issues for discovered work (`--deps discovered-from:<id>`)
4. Complete and sync

## Session Completion

Work is NOT complete until `git push` succeeds:
```bash
git pull --rebase && bd sync && git push
```
</file>
<file path="CLAUDE.md">
# CLAUDE.md

## Quick Commands

```bash
make build              # Build binary to ./build/repodocs
make test               # Unit tests (fast, -short)
make test-integration   # Integration tests
make test-e2e           # E2E tests
make lint               # golangci-lint (v2)
make fmt                # Format code
make vet                # Run go vet
```

## Architecture

**repodocs-go** extracts documentation from websites, Git repos, sitemaps, pkg.go.dev, llms.txt and converts to Markdown.

**Flow**: URL → Detector → Strategy → Fetcher/Renderer → Converter → Writer

### Strategy Pattern
Strategies implement `internal/strategies.Strategy`:
- `Name() string`
- `CanHandle(url string) bool`
- `Execute(ctx context.Context, url string, opts Options) error`

Detection order: LLMS → PkgGo → Sitemap → Git → Crawler

### Dependency Injection
`strategies.Dependencies` is composition root:
```go
type Dependencies struct {
    Fetcher   *fetcher.Client
    Renderer  domain.Renderer
    Cache     domain.Cache
    Converter *converter.Pipeline
    Writer    *output.Writer
    Logger    *utils.Logger
}
```

### Converter Pipeline
1. UTF-8 normalization
2. Content extraction (CSS selector or Readability)
3. HTML sanitization (remove scripts, nav, ads)
4. Markdown conversion
5. Metadata extraction

## Test Structure

```
tests/
├── unit/           # Fast unit tests (make test)
├── integration/    # Network-dependent tests
├── e2e/            # Full CLI tests
├── mocks/          # Generated mocks (go.uber.org/mock)
├── testutil/       # Shared helpers
└── fixtures/       # Test HTML/data
```

## Task Tracking

Use `bd` for task tracking (see AGENTS.md).
</file>
<file path="go.mod">
module github.com/quantmind-br/repodocs-go

go 1.24.1

require (
	github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0
	github.com/PuerkitoBio/goquery v1.11.0
	github.com/bogdanfinn/fhttp v0.6.2
	github.com/bogdanfinn/tls-client v1.11.2
	github.com/cenkalti/backoff/v4 v4.3.0
	github.com/dgraph-io/badger/v4 v4.8.0
	github.com/go-git/go-git/v5 v5.16.4
	github.com/go-rod/rod v0.116.2
	github.com/go-rod/stealth v0.4.9
	github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0
	github.com/gocolly/colly/v2 v2.3.0
	github.com/klauspost/compress v1.18.2
	github.com/rs/zerolog v1.34.0
	github.com/schollz/progressbar/v3 v3.18.0
	github.com/spf13/cobra v1.10.2
	github.com/spf13/viper v1.21.0
	github.com/stretchr/testify v1.11.1
	go.uber.org/mock v0.5.0
	golang.org/x/net v0.47.0
	golang.org/x/text v0.31.0
	gopkg.in/yaml.v3 v3.0.1
)

require (
	dario.cat/mergo v1.0.0 // indirect
	github.com/JohannesKaufmann/dom v0.2.0 // indirect
	github.com/Microsoft/go-winio v0.6.2 // indirect
	github.com/ProtonMail/go-crypto v1.1.6 // indirect
	github.com/andybalholm/brotli v1.1.1 // indirect
	github.com/andybalholm/cascadia v1.3.3 // indirect
	github.com/antchfx/htmlquery v1.3.5 // indirect
	github.com/antchfx/xmlquery v1.5.0 // indirect
	github.com/antchfx/xpath v1.3.5 // indirect
	github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de // indirect
	github.com/atotto/clipboard v0.1.4 // indirect
	github.com/aymanbagabas/go-osc52/v2 v2.0.1 // indirect
	github.com/bits-and-blooms/bitset v1.24.4 // indirect
	github.com/bogdanfinn/quic-go-utls v1.0.4-utls // indirect
	github.com/bogdanfinn/utls v1.7.4-barnius // indirect
	github.com/catppuccin/go v0.3.0 // indirect
	github.com/cespare/xxhash/v2 v2.3.0 // indirect
	github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7 // indirect
	github.com/charmbracelet/bubbletea v1.3.6 // indirect
	github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc // indirect
	github.com/charmbracelet/huh v0.8.0 // indirect
	github.com/charmbracelet/lipgloss v1.1.0 // indirect
	github.com/charmbracelet/x/ansi v0.9.3 // indirect
	github.com/charmbracelet/x/cellbuf v0.0.13 // indirect
	github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0 // indirect
	github.com/charmbracelet/x/term v0.2.1 // indirect
	github.com/cloudflare/circl v1.6.1 // indirect
	github.com/cyphar/filepath-securejoin v0.4.1 // indirect
	github.com/davecgh/go-spew v1.1.1 // indirect
	github.com/dgraph-io/ristretto/v2 v2.2.0 // indirect
	github.com/dustin/go-humanize v1.0.1 // indirect
	github.com/emirpasic/gods v1.18.1 // indirect
	github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f // indirect
	github.com/fsnotify/fsnotify v1.9.0 // indirect
	github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376 // indirect
	github.com/go-git/go-billy/v5 v5.6.2 // indirect
	github.com/go-logr/logr v1.4.3 // indirect
	github.com/go-logr/stdr v1.2.2 // indirect
	github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c // indirect
	github.com/go-viper/mapstructure/v2 v2.4.0 // indirect
	github.com/gobwas/glob v0.2.3 // indirect
	github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f // indirect
	github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8 // indirect
	github.com/golang/protobuf v1.5.4 // indirect
	github.com/google/flatbuffers v25.2.10+incompatible // indirect
	github.com/inconshreveable/mousetrap v1.1.0 // indirect
	github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99 // indirect
	github.com/kennygrant/sanitize v1.2.4 // indirect
	github.com/kevinburke/ssh_config v1.2.0 // indirect
	github.com/lucasb-eyer/go-colorful v1.3.0 // indirect
	github.com/mattn/go-colorable v0.1.13 // indirect
	github.com/mattn/go-isatty v0.0.20 // indirect
	github.com/mattn/go-localereader v0.0.1 // indirect
	github.com/mattn/go-runewidth v0.0.16 // indirect
	github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db // indirect
	github.com/mitchellh/hashstructure/v2 v2.0.2 // indirect
	github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 // indirect
	github.com/muesli/cancelreader v0.2.2 // indirect
	github.com/muesli/termenv v0.16.0 // indirect
	github.com/nlnwa/whatwg-url v0.6.2 // indirect
	github.com/pelletier/go-toml/v2 v2.2.4 // indirect
	github.com/pjbgf/sha1cd v0.3.2 // indirect
	github.com/pmezard/go-difflib v1.0.0 // indirect
	github.com/quic-go/qpack v0.5.1 // indirect
	github.com/rivo/uniseg v0.4.7 // indirect
	github.com/sagikazarmark/locafero v0.11.0 // indirect
	github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d // indirect
	github.com/sergi/go-diff v1.4.0 // indirect
	github.com/skeema/knownhosts v1.3.1 // indirect
	github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 // indirect
	github.com/spf13/afero v1.15.0 // indirect
	github.com/spf13/cast v1.10.0 // indirect
	github.com/spf13/pflag v1.0.10 // indirect
	github.com/stretchr/objx v0.5.2 // indirect
	github.com/subosito/gotenv v1.6.0 // indirect
	github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5 // indirect
	github.com/temoto/robotstxt v1.1.2 // indirect
	github.com/xanzy/ssh-agent v0.3.3 // indirect
	github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e // indirect
	github.com/ysmood/fetchup v0.2.3 // indirect
	github.com/ysmood/goob v0.4.0 // indirect
	github.com/ysmood/got v0.40.0 // indirect
	github.com/ysmood/gson v0.7.3 // indirect
	github.com/ysmood/leakless v0.9.0 // indirect
	go.opentelemetry.io/auto/sdk v1.1.0 // indirect
	go.opentelemetry.io/otel v1.37.0 // indirect
	go.opentelemetry.io/otel/metric v1.37.0 // indirect
	go.opentelemetry.io/otel/trace v1.37.0 // indirect
	go.yaml.in/yaml/v3 v3.0.4 // indirect
	golang.org/x/crypto v0.44.0 // indirect
	golang.org/x/mod v0.29.0 // indirect
	golang.org/x/sync v0.18.0 // indirect
	golang.org/x/sys v0.38.0 // indirect
	golang.org/x/term v0.37.0 // indirect
	golang.org/x/tools v0.38.0 // indirect
	google.golang.org/appengine v1.6.8 // indirect
	google.golang.org/protobuf v1.36.10 // indirect
	gopkg.in/warnings.v0 v0.1.2 // indirect
)
</file>
<file path="go.sum">
dario.cat/mergo v1.0.0 h1:AGCNq9Evsj31mOgNPcLyXc+4PNABt905YmuqPYYpBWk=
dario.cat/mergo v1.0.0/go.mod h1:uNxQE+84aUszobStD9th8a29P2fMDhsBdgRYvZOxGmk=
github.com/JohannesKaufmann/dom v0.2.0 h1:1bragmEb19K8lHAqgFgqCpiPCFEZMTXzOIEjuxkUfLQ=
github.com/JohannesKaufmann/dom v0.2.0/go.mod h1:57iSUl5RKric4bUkgos4zu6Xt5LMHUnw3TF1l5CbGZo=
github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0 h1:mklaPbT4f/EiDr1Q+zPrEt9lgKAkVrIBtWf33d9GpVA=
github.com/JohannesKaufmann/html-to-markdown/v2 v2.5.0/go.mod h1:D56Cl9r8M5i3UwAchE+LlLc5hPN3kJtdZNVJn06lSHU=
github.com/Microsoft/go-winio v0.5.2/go.mod h1:WpS1mjBmmwHBEWmogvA2mj8546UReBk4v8QkMxJ6pZY=
github.com/Microsoft/go-winio v0.6.2 h1:F2VQgta7ecxGYO8k3ZZz3RS8fVIXVxONVUPlNERoyfY=
github.com/Microsoft/go-winio v0.6.2/go.mod h1:yd8OoFMLzJbo9gZq8j5qaps8bJ9aShtEA8Ipt1oGCvU=
github.com/ProtonMail/go-crypto v1.1.6 h1:ZcV+Ropw6Qn0AX9brlQLAUXfqLBc7Bl+f/DmNxpLfdw=
github.com/ProtonMail/go-crypto v1.1.6/go.mod h1:rA3QumHc/FZ8pAHreoekgiAbzpNsfQAosU5td4SnOrE=
github.com/PuerkitoBio/goquery v1.11.0 h1:jZ7pwMQXIITcUXNH83LLk+txlaEy6NVOfTuP43xxfqw=
github.com/PuerkitoBio/goquery v1.11.0/go.mod h1:wQHgxUOU3JGuj3oD/QFfxUdlzW6xPHfqyHre6VMY4DQ=
github.com/andybalholm/brotli v1.1.1 h1:PR2pgnyFznKEugtsUo0xLdDop5SKXd5Qf5ysW+7XdTA=
github.com/andybalholm/brotli v1.1.1/go.mod h1:05ib4cKhjx3OQYUY22hTVd34Bc8upXjOLL2rKwwZBoA=
github.com/andybalholm/cascadia v1.3.3 h1:AG2YHrzJIm4BZ19iwJ/DAua6Btl3IwJX+VI4kktS1LM=
github.com/andybalholm/cascadia v1.3.3/go.mod h1:xNd9bqTn98Ln4DwST8/nG+H0yuB8Hmgu1YHNnWw0GeA=
github.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be h1:9AeTilPcZAjCFIImctFaOjnTIavg87rW78vTPkQqLI8=
github.com/anmitsu/go-shlex v0.0.0-20200514113438-38f4b401e2be/go.mod h1:ySMOLuWl6zY27l47sB3qLNK6tF2fkHG55UZxx8oIVo4=
github.com/antchfx/htmlquery v1.3.5 h1:aYthDDClnG2a2xePf6tys/UyyM/kRcsFRm+ifhFKoU0=
github.com/antchfx/htmlquery v1.3.5/go.mod h1:5oyIPIa3ovYGtLqMPNjBF2Uf25NPCKsMjCnQ8lvjaoA=
github.com/antchfx/xmlquery v1.5.0 h1:uAi+mO40ZWfyU6mlUBxRVvL6uBNZ6LMU4M3+mQIBV4c=
github.com/antchfx/xmlquery v1.5.0/go.mod h1:lJfWRXzYMK1ss32zm1GQV3gMIW/HFey3xDZmkP1SuNc=
github.com/antchfx/xpath v1.3.5 h1:PqbXLC3TkfeZyakF5eeh3NTWEbYl4VHNVeufANzDbKQ=
github.com/antchfx/xpath v1.3.5/go.mod h1:i54GszH55fYfBmoZXapTHN8T8tkcHfRgLyVwwqzXNcs=
github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de h1:FxWPpzIjnTlhPwqqXc4/vE0f7GvRjuAsbW+HOIe8KnA=
github.com/araddon/dateparse v0.0.0-20210429162001-6b43995a97de/go.mod h1:DCaWoUhZrYW9p1lxo/cm8EmUOOzAPSEZNGF2DK1dJgw=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5 h1:0CwZNZbxp69SHPdPJAN/hZIm0C4OItdklCFmMRWYpio=
github.com/armon/go-socks5 v0.0.0-20160902184237-e75332964ef5/go.mod h1:wHh0iHkYZB8zMSxRWpUBQtwG5a7fFgvEO+odwuTv2gs=
github.com/atotto/clipboard v0.1.4 h1:EH0zSVneZPSuFR11BlR9YppQTVDbh5+16AmcJi4g1z4=
github.com/atotto/clipboard v0.1.4/go.mod h1:ZY9tmq7sm5xIbd9bOK4onWV4S6X0u6GY7Vn0Yu86PYI=
github.com/aymanbagabas/go-osc52/v2 v2.0.1 h1:HwpRHbFMcZLEVr42D4p7XBqjyuxQH5SMiErDT4WkJ2k=
github.com/aymanbagabas/go-osc52/v2 v2.0.1/go.mod h1:uYgXzlJ7ZpABp8OJ+exZzJJhRNQ2ASbcXHWsFqH8hp8=
github.com/bits-and-blooms/bitset v1.20.0/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=
github.com/bits-and-blooms/bitset v1.24.4 h1:95H15Og1clikBrKr/DuzMXkQzECs1M6hhoGXLwLQOZE=
github.com/bits-and-blooms/bitset v1.24.4/go.mod h1:7hO7Gc7Pp1vODcmWvKMRA9BNmbv6a/7QIWpPxHddWR8=
github.com/bogdanfinn/fhttp v0.6.2 h1:qmFu9fxKmSRR+tcKfgxthmiu365tYspz3Mi404ytZPE=
github.com/bogdanfinn/fhttp v0.6.2/go.mod h1:0irhEtS+wJ4m8SGhWO0wmbXMjCbH3WZpU6UcymRYKuk=
github.com/bogdanfinn/quic-go-utls v1.0.4-utls h1:zPjusVVNeJFA2ORMAP0rjnrZrBkV4Dnia4e6ToOfUDA=
github.com/bogdanfinn/quic-go-utls v1.0.4-utls/go.mod h1:UONJOaHGWho08kZtkkgH7GjktEPjMemGxjTcNpVPZVA=
github.com/bogdanfinn/tls-client v1.11.2 h1:o6qX0L1cEi+4MaBqujxqOeK254VZM20t3QR+A34/V6I=
github.com/bogdanfinn/tls-client v1.11.2/go.mod h1:qQIsVGe35NdxYEozNh9JuDZ+aOaOEq2tKAsu2iYEGZg=
github.com/bogdanfinn/utls v1.7.4-barnius h1:1ldNJEpKdkrx7b8hEc6MRkjnZIF8f2lDcTtRVxqY9zw=
github.com/bogdanfinn/utls v1.7.4-barnius/go.mod h1:SUn0CoHGVp/akGNuaqh99yvovu64PCP2LbWd3Z/Laic=
github.com/catppuccin/go v0.3.0 h1:d+0/YicIq+hSTo5oPuRi5kOpqkVA5tAsU6dNhvRu+aY=
github.com/catppuccin/go v0.3.0/go.mod h1:8IHJuMGaUUjQM82qBrGNBv7LFq6JI3NnQCF6MOlZjpc=
github.com/cenkalti/backoff/v4 v4.3.0 h1:MyRJ/UdXutAwSAT+s3wNd7MfTIcy71VQueUuFK343L8=
github.com/cenkalti/backoff/v4 v4.3.0/go.mod h1:Y3VNntkOUPxTVeUxJ/G5vcM//AlwfmyYozVcomhLiZE=
github.com/cespare/xxhash/v2 v2.3.0 h1:UL815xU9SqsFlibzuggzjXhog7bL6oX9BbNZnL2UFvs=
github.com/cespare/xxhash/v2 v2.3.0/go.mod h1:VGX0DQ3Q6kWi7AoAeZDth3/j3BFtOZR5XLFGgcrjCOs=
github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7 h1:JFgG/xnwFfbezlUnFMJy0nusZvytYysV4SCS2cYbvws=
github.com/charmbracelet/bubbles v0.21.1-0.20250623103423-23b8fd6302d7/go.mod h1:ISC1gtLcVilLOf23wvTfoQuYbW2q0JevFxPfUzZ9Ybw=
github.com/charmbracelet/bubbletea v1.3.6 h1:VkHIxPJQeDt0aFJIsVxw8BQdh/F/L2KKZGsK6et5taU=
github.com/charmbracelet/bubbletea v1.3.6/go.mod h1:oQD9VCRQFF8KplacJLo28/jofOI2ToOfGYeFgBBxHOc=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc h1:4pZI35227imm7yK2bGPcfpFEmuY1gc2YSTShr4iJBfs=
github.com/charmbracelet/colorprofile v0.2.3-0.20250311203215-f60798e515dc/go.mod h1:X4/0JoqgTIPSFcRA/P6INZzIuyqdFY5rm8tb41s9okk=
github.com/charmbracelet/huh v0.8.0 h1:Xz/Pm2h64cXQZn/Jvele4J3r7DDiqFCNIVteYukxDvY=
github.com/charmbracelet/huh v0.8.0/go.mod h1:5YVc+SlZ1IhQALxRPpkGwwEKftN/+OlJlnJYlDRFqN4=
github.com/charmbracelet/lipgloss v1.1.0 h1:vYXsiLHVkK7fp74RkV7b2kq9+zDLoEU4MZoFqR/noCY=
github.com/charmbracelet/lipgloss v1.1.0/go.mod h1:/6Q8FR2o+kj8rz4Dq0zQc3vYf7X+B0binUUBwA0aL30=
github.com/charmbracelet/x/ansi v0.9.3 h1:BXt5DHS/MKF+LjuK4huWrC6NCvHtexww7dMayh6GXd0=
github.com/charmbracelet/x/ansi v0.9.3/go.mod h1:3RQDQ6lDnROptfpWuUVIUG64bD2g2BgntdxH0Ya5TeE=
github.com/charmbracelet/x/cellbuf v0.0.13 h1:/KBBKHuVRbq1lYx5BzEHBAFBP8VcQzJejZ/IA3iR28k=
github.com/charmbracelet/x/cellbuf v0.0.13/go.mod h1:xe0nKWGd3eJgtqZRaN9RjMtK7xUYchjzPr7q6kcvCCs=
github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0 h1:qko3AQ4gK1MTS/de7F5hPGx6/k1u0w4TeYmBFwzYVP4=
github.com/charmbracelet/x/exp/strings v0.0.0-20240722160745-212f7b056ed0/go.mod h1:pBhA0ybfXv6hDjQUZ7hk1lVxBiUbupdw5R31yPUViVQ=
github.com/charmbracelet/x/term v0.2.1 h1:AQeHeLZ1OqSXhrAWpYUtZyX1T3zVxfpZuEQMIQaGIAQ=
github.com/charmbracelet/x/term v0.2.1/go.mod h1:oQ4enTYFV7QN4m0i9mzHrViD7TQKvNEEkHUMCmsxdUg=
github.com/chengxilo/virtualterm v1.0.4 h1:Z6IpERbRVlfB8WkOmtbHiDbBANU7cimRIof7mk9/PwM=
github.com/chengxilo/virtualterm v1.0.4/go.mod h1:DyxxBZz/x1iqJjFxTFcr6/x+jSpqN0iwWCOK1q10rlY=
github.com/cloudflare/circl v1.6.1 h1:zqIqSPIndyBh1bjLVVDHMPpVKqp8Su/V+6MeDzzQBQ0=
github.com/cloudflare/circl v1.6.1/go.mod h1:uddAzsPgqdMAYatqJ0lsjX1oECcQLIlRpzZh3pJrofs=
github.com/coreos/go-systemd/v22 v22.5.0/go.mod h1:Y58oyj3AT4RCenI/lSvhwexgC+NSVTIJ3seZv2GcEnc=
github.com/cpuguy83/go-md2man/v2 v2.0.6/go.mod h1:oOW0eioCTA6cOiMLiUPZOpcVxMig6NIQQ7OS05n1F4g=
github.com/cyphar/filepath-securejoin v0.4.1 h1:JyxxyPEaktOD+GAnqIqTf9A8tHyAG22rowi7HkoSU1s=
github.com/cyphar/filepath-securejoin v0.4.1/go.mod h1:Sdj7gXlvMcPZsbhwhQ33GguGLDGQL7h7bg04C/+u9jI=
github.com/davecgh/go-spew v1.1.0/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/davecgh/go-spew v1.1.1 h1:vj9j/u1bqnvCEfJOwUhtlOARqs3+rkHYY13jYWTU97c=
github.com/davecgh/go-spew v1.1.1/go.mod h1:J7Y8YcW2NihsgmVo/mv3lAwl/skON4iLHjSsI+c5H38=
github.com/dgraph-io/badger/v4 v4.8.0 h1:JYph1ChBijCw8SLeybvPINizbDKWZ5n/GYbz2yhN/bs=
github.com/dgraph-io/badger/v4 v4.8.0/go.mod h1:U6on6e8k/RTbUWxqKR0MvugJuVmkxSNc79ap4917h4w=
github.com/dgraph-io/ristretto/v2 v2.2.0 h1:bkY3XzJcXoMuELV8F+vS8kzNgicwQFAaGINAEJdWGOM=
github.com/dgraph-io/ristretto/v2 v2.2.0/go.mod h1:RZrm63UmcBAaYWC1DotLYBmTvgkrs0+XhBd7Npn7/zI=
github.com/dgryski/go-farm v0.0.0-20240924180020-3414d57e47da h1:aIftn67I1fkbMa512G+w+Pxci9hJPB8oMnkcP3iZF38=
github.com/dgryski/go-farm v0.0.0-20240924180020-3414d57e47da/go.mod h1:SqUrOPUnsFjfmXRMNPybcSiG0BgUW2AuFH8PAnS2iTw=
github.com/dustin/go-humanize v1.0.1 h1:GzkhY7T5VNhEkwH0PVJgjz+fX1rhBrR7pRT3mDkpeCY=
github.com/dustin/go-humanize v1.0.1/go.mod h1:Mu1zIs6XwVuF/gI1OepvI0qD18qycQx+mFykh5fBlto=
github.com/elazarl/goproxy v1.7.2 h1:Y2o6urb7Eule09PjlhQRGNsqRfPmYI3KKQLFpCAV3+o=
github.com/elazarl/goproxy v1.7.2/go.mod h1:82vkLNir0ALaW14Rc399OTTjyNREgmdL2cVoIbS6XaE=
github.com/emirpasic/gods v1.18.1 h1:FXtiHYKDGKCW2KzwZKx0iC0PQmdlorYgdFG9jPXJ1Bc=
github.com/emirpasic/gods v1.18.1/go.mod h1:8tpGGwCnJ5H4r6BWwaV6OrWmMoPhUl5jm/FMNAnJvWQ=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f h1:Y/CXytFA4m6baUTXGLOoWe4PQhGxaX0KpnayAqC48p4=
github.com/erikgeiser/coninput v0.0.0-20211004153227-1c3628e74d0f/go.mod h1:vw97MGsxSvLiUE2X8qFplwetxpGLQrlU1Q9AUEIzCaM=
github.com/frankban/quicktest v1.14.6 h1:7Xjx+VpznH+oBnejlPUj8oUpdxnVs4f8XU8WnHkI4W8=
github.com/frankban/quicktest v1.14.6/go.mod h1:4ptaffx2x8+WTWXmUCuVU6aPUX1/Mz7zb5vbUoiM6w0=
github.com/fsnotify/fsnotify v1.9.0 h1:2Ml+OJNzbYCTzsxtv8vKSFD9PbJjmhYF14k/jKC7S9k=
github.com/fsnotify/fsnotify v1.9.0/go.mod h1:8jBTzvmWwFyi3Pb8djgCCO5IBqzKJ/Jwo8TRcHyHii0=
github.com/gliderlabs/ssh v0.3.8 h1:a4YXD1V7xMF9g5nTkdfnja3Sxy1PVDCj1Zg4Wb8vY6c=
github.com/gliderlabs/ssh v0.3.8/go.mod h1:xYoytBv1sV0aL3CavoDuJIQNURXkkfPA/wxQ1pL1fAU=
github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376 h1:+zs/tPmkDkHx3U66DAb0lQFJrpS6731Oaa12ikc+DiI=
github.com/go-git/gcfg v1.5.1-0.20230307220236-3a3c6141e376/go.mod h1:an3vInlBmSxCcxctByoQdvwPiA7DTK7jaaFDBTtu0ic=
github.com/go-git/go-billy/v5 v5.6.2 h1:6Q86EsPXMa7c3YZ3aLAQsMA0VlWmy43r6FHqa/UNbRM=
github.com/go-git/go-billy/v5 v5.6.2/go.mod h1:rcFC2rAsp/erv7CMz9GczHcuD0D32fWzH+MJAU+jaUU=
github.com/go-git/go-git-fixtures/v4 v4.3.2-0.20231010084843-55a94097c399 h1:eMje31YglSBqCdIqdhKBW8lokaMrL3uTkpGYlE2OOT4=
github.com/go-git/go-git-fixtures/v4 v4.3.2-0.20231010084843-55a94097c399/go.mod h1:1OCfN199q1Jm3HZlxleg+Dw/mwps2Wbk9frAWm+4FII=
github.com/go-git/go-git/v5 v5.16.4 h1:7ajIEZHZJULcyJebDLo99bGgS0jRrOxzZG4uCk2Yb2Y=
github.com/go-git/go-git/v5 v5.16.4/go.mod h1:4Ge4alE/5gPs30F2H1esi2gPd69R0C39lolkucHBOp8=
github.com/go-logr/logr v1.2.2/go.mod h1:jdQByPbusPIv2/zmleS9BjJVeZ6kBagPoEUsqbVz/1A=
github.com/go-logr/logr v1.4.3 h1:CjnDlHq8ikf6E492q6eKboGOC0T8CDaOvkHCIg8idEI=
github.com/go-logr/logr v1.4.3/go.mod h1:9T104GzyrTigFIr8wt5mBrctHMim0Nb2HLGrmQ40KvY=
github.com/go-logr/stdr v1.2.2 h1:hSWxHoqTgW2S2qGc0LTAI563KZ5YKYRhT3MFKZMbjag=
github.com/go-logr/stdr v1.2.2/go.mod h1:mMo/vtBO5dYbehREoey6XUKy/eSumjCCveDpRre4VKE=
github.com/go-rod/rod v0.113.0/go.mod h1:aiedSEFg5DwG/fnNbUOTPMTTWX3MRj6vIs/a684Mthw=
github.com/go-rod/rod v0.116.2 h1:A5t2Ky2A+5eD/ZJQr1EfsQSe5rms5Xof/qj296e+ZqA=
github.com/go-rod/rod v0.116.2/go.mod h1:H+CMO9SCNc2TJ2WfrG+pKhITz57uGNYU43qYHh438Mg=
github.com/go-rod/stealth v0.4.9 h1:X2PmQk4DUF2wzw6GOsWjW/glb8K5ebnftbEvLh7MlZ4=
github.com/go-rod/stealth v0.4.9/go.mod h1:eAzyvw8c0iAd5nJJsSWeh0fQ5z94vCIfdi1hUmYDimc=
github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c h1:wpkoddUomPfHiOziHZixGO5ZBS73cKqVzZipfrLmO1w=
github.com/go-shiori/dom v0.0.0-20230515143342-73569d674e1c/go.mod h1:oVDCh3qjJMLVUSILBRwrm+Bc6RNXGZYtoh9xdvf1ffM=
github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0 h1:A3B75Yp163FAIf9nLlFMl4pwIj+T3uKxfI7mbvvY2Ls=
github.com/go-shiori/go-readability v0.0.0-20251205110129-5db1dc9836f0/go.mod h1:suxK0Wpz4BM3/2+z1mnOVTIWHDiMCIOGoKDCRumSsk0=
github.com/go-viper/mapstructure/v2 v2.4.0 h1:EBsztssimR/CONLSZZ04E8qAkxNYq4Qp9LvH92wZUgs=
github.com/go-viper/mapstructure/v2 v2.4.0/go.mod h1:oJDH3BJKyqBA2TXFhDsKDGDTlndYOZ6rGS0BRZIxGhM=
github.com/gobwas/glob v0.2.3 h1:A4xDbljILXROh+kObIiy5kIaPYD8e96x1tgBhUI5J+Y=
github.com/gobwas/glob v0.2.3/go.mod h1:d3Ez4x06l9bZtSvzIay5+Yzi0fmZzPgnTbPcKjJAkT8=
github.com/gocolly/colly/v2 v2.3.0 h1:HSFh0ckbgVd2CSGRE+Y/iA4goUhGROJwyQDCMXGFBWM=
github.com/gocolly/colly/v2 v2.3.0/go.mod h1:Qp54s/kQbwCQvFVx8KzKCSTXVJ1wWT4QeAKEu33x1q8=
github.com/godbus/dbus/v5 v5.0.4/go.mod h1:xhWf0FNVPg57R7Z0UbKHbJfkEywrmjJnf7w5xrFpKfA=
github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f h1:3BSP1Tbs2djlpprl7wCLuiqMaUh5SJkkzI2gDs+FgLs=
github.com/gogs/chardet v0.0.0-20211120154057-b7413eaefb8f/go.mod h1:Pcatq5tYkCW2Q6yrR2VRHlbHpZ/R4/7qyL1TCF7vl14=
github.com/golang/groupcache v0.0.0-20210331224755-41bb18bfe9da/go.mod h1:cIg4eruTrX1D+g88fzRXU5OdNfaM+9IcxsU14FzY7Hc=
github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8 h1:f+oWsMOmNPc8JmEHVZIycC7hBoQxHH9pNKQORJNozsQ=
github.com/golang/groupcache v0.0.0-20241129210726-2c02b8208cf8/go.mod h1:wcDNUvekVysuuOpQKo3191zZyTpiI6se1N1ULghS0sw=
github.com/golang/protobuf v1.5.0/go.mod h1:FsONVRAS9T7sI+LIUmWTfcYkHO4aIWwzhcaSAoJOfIk=
github.com/golang/protobuf v1.5.2/go.mod h1:XVQd3VNwM+JqD3oG2Ue2ip4fOMUkwXdXDdiuN0vRsmY=
github.com/golang/protobuf v1.5.4 h1:i7eJL8qZTpSEXOPTxNKhASYpMn+8e5Q6AdndVa1dWek=
github.com/golang/protobuf v1.5.4/go.mod h1:lnTiLA8Wa4RWRcIUkrtSVa5nRhsEGBg48fD6rSs7xps=
github.com/google/flatbuffers v25.2.10+incompatible h1:F3vclr7C3HpB1k9mxCGRMXq6FdUalZ6H/pNX4FP1v0Q=
github.com/google/flatbuffers v25.2.10+incompatible/go.mod h1:1AeVuKshWv4vARoZatz6mlQ0JxURH0Kv5+zNeJKJCa8=
github.com/google/go-cmp v0.5.5/go.mod h1:v8dTdLbMG2kIc/vJvl+f65V22dbkXbowE6jgT/gNBxE=
github.com/google/go-cmp v0.6.0/go.mod h1:17dUlkBOakJ0+DkrSSNjCkIjxS6bF9zb3elmeNGIjoY=
github.com/google/go-cmp v0.7.0 h1:wk8382ETsv4JYUZwIsn6YpYiWiBsYLSJiTsyBybVuN8=
github.com/google/go-cmp v0.7.0/go.mod h1:pXiqmnSA92OHEEa9HXL2W4E7lf9JzCmGVUdgjX3N/iU=
github.com/inconshreveable/mousetrap v1.1.0 h1:wN+x4NVGpMsO7ErUn/mUI3vEoE6Jt13X2s0bqwp9tc8=
github.com/inconshreveable/mousetrap v1.1.0/go.mod h1:vpF70FUmC8bwa3OWnCshd2FqLfsEA9PFc4w1p2J65bw=
github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99 h1:BQSFePA1RWJOlocH6Fxy8MmwDt+yVQYULKfN0RoTN8A=
github.com/jbenet/go-context v0.0.0-20150711004518-d14ea06fba99/go.mod h1:1lJo3i6rXxKeerYnT8Nvf0QmHCRC1n8sfWVwXF2Frvo=
github.com/kennygrant/sanitize v1.2.4 h1:gN25/otpP5vAsO2djbMhF/LQX6R7+O1TB4yv8NzpJ3o=
github.com/kennygrant/sanitize v1.2.4/go.mod h1:LGsjYYtgxbetdg5owWB2mpgUL6e2nfw2eObZ0u0qvak=
github.com/kevinburke/ssh_config v1.2.0 h1:x584FjTGwHzMwvHx18PXxbBVzfnxogHaAReU4gf13a4=
github.com/kevinburke/ssh_config v1.2.0/go.mod h1:CT57kijsi8u/K/BOFA39wgDQJ9CxiF4nAY/ojJ6r6mM=
github.com/klauspost/compress v1.18.2 h1:iiPHWW0YrcFgpBYhsA6D1+fqHssJscY/Tm/y2Uqnapk=
github.com/klauspost/compress v1.18.2/go.mod h1:R0h/fSBs8DE4ENlcrlib3PsXS61voFxhIs2DeRhCvJ4=
github.com/kr/pretty v0.1.0/go.mod h1:dAy3ld7l9f0ibDNOQOHHMYYIIbhfbHSm3C4ZsoJORNo=
github.com/kr/pretty v0.3.1 h1:flRD4NNwYAUpkphVc1HcthR4KEIFJ65n8Mw5qdRn3LE=
github.com/kr/pretty v0.3.1/go.mod h1:hoEshYVHaxMs3cyo3Yncou5ZscifuDolrwPKZanG3xk=
github.com/kr/pty v1.1.1/go.mod h1:pFQYn66WHrOpPYNljwOMqo10TkYh1fy3cYio2l3bCsQ=
github.com/kr/text v0.1.0/go.mod h1:4Jbv+DJW3UT/LiOwJeYQe1efqtUx/iVham/4vfdArNI=
github.com/kr/text v0.2.0 h1:5Nx0Ya0ZqY2ygV366QzturHI13Jq95ApcVaJBhpS+AY=
github.com/kr/text v0.2.0/go.mod h1:eLer722TekiGuMkidMxC/pM04lWEeraHUUmBw8l2grE=
github.com/lucasb-eyer/go-colorful v1.3.0 h1:2/yBRLdWBZKrf7gB40FoiKfAWYQ0lqNcbuQwVHXptag=
github.com/lucasb-eyer/go-colorful v1.3.0/go.mod h1:R4dSotOR9KMtayYi1e77YzuveK+i7ruzyGqttikkLy0=
github.com/mattn/go-colorable v0.1.13 h1:fFA4WZxdEF4tXPZVKMLwD8oUnCTTo08duU7wxecdEvA=
github.com/mattn/go-colorable v0.1.13/go.mod h1:7S9/ev0klgBDR4GtXTXX8a3vIGJpMovkB8vQcUbaXHg=
github.com/mattn/go-isatty v0.0.16/go.mod h1:kYGgaQfpe5nmfYZH+SKPsOc2e4SrIfOl2e/yFXSvRLM=
github.com/mattn/go-isatty v0.0.19/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-isatty v0.0.20 h1:xfD0iDuEKnDkl03q4limB+vH+GxLEtL/jb4xVJSWWEY=
github.com/mattn/go-isatty v0.0.20/go.mod h1:W+V8PltTTMOvKvAeJH7IuucS94S2C6jfK/D7dTCTo3Y=
github.com/mattn/go-localereader v0.0.1 h1:ygSAOl7ZXTx4RdPYinUpg6W99U8jWvWi9Ye2JC/oIi4=
github.com/mattn/go-localereader v0.0.1/go.mod h1:8fBrzywKY7BI3czFoHkuzRoWE9C+EiG4R1k4Cjx5p88=
github.com/mattn/go-runewidth v0.0.10/go.mod h1:RAqKPSqVFrSLVXbA8x7dzmKdmGzieGRCM46jaSJTDAk=
github.com/mattn/go-runewidth v0.0.16 h1:E5ScNMtiwvlvB5paMFdw9p4kSQzbXFikJ5SQO6TULQc=
github.com/mattn/go-runewidth v0.0.16/go.mod h1:Jdepj2loyihRzMpdS35Xk/zdY8IAYHsh153qUoGf23w=
github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db h1:62I3jR2EmQ4l5rM/4FEfDWcRD+abF5XlKShorW5LRoQ=
github.com/mitchellh/colorstring v0.0.0-20190213212951-d06e56a500db/go.mod h1:l0dey0ia/Uv7NcFFVbCLtqEBQbrT4OCwCSKTEv6enCw=
github.com/mitchellh/hashstructure/v2 v2.0.2 h1:vGKWl0YJqUNxE8d+h8f6NJLcCJrgbhC4NcD46KavDd4=
github.com/mitchellh/hashstructure/v2 v2.0.2/go.mod h1:MG3aRVU/N29oo/V/IhBX8GR/zz4kQkprJgF2EVszyDE=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6 h1:ZK8zHtRHOkbHy6Mmr5D264iyp3TiX5OmNcI5cIARiQI=
github.com/muesli/ansi v0.0.0-20230316100256-276c6243b2f6/go.mod h1:CJlz5H+gyd6CUWT45Oy4q24RdLyn7Md9Vj2/ldJBSIo=
github.com/muesli/cancelreader v0.2.2 h1:3I4Kt4BQjOR54NavqnDogx/MIoWBFa0StPA8ELUXHmA=
github.com/muesli/cancelreader v0.2.2/go.mod h1:3XuTXfFS2VjM+HTLZY9Ak0l6eUKfijIfMUZ4EgX0QYo=
github.com/muesli/termenv v0.16.0 h1:S5AlUN9dENB57rsbnkPyfdGuWIlkmzJjbFf0Tf5FWUc=
github.com/muesli/termenv v0.16.0/go.mod h1:ZRfOIKPFDYQoDFF4Olj7/QJbW60Ol/kL1pU3VfY/Cnk=
github.com/nlnwa/whatwg-url v0.6.2 h1:jU61lU2ig4LANydbEJmA2nPrtCGiKdtgT0rmMd2VZ/Q=
github.com/nlnwa/whatwg-url v0.6.2/go.mod h1:x0FPXJzzOEieQtsBT/AKvbiBbQ46YlL6Xa7m02M1ECk=
github.com/onsi/gomega v1.34.1 h1:EUMJIKUjM8sKjYbtxQI9A4z2o+rruxnzNvpknOXie6k=
github.com/onsi/gomega v1.34.1/go.mod h1:kU1QgUvBDLXBJq618Xvm2LUX6rSAfRaFRTcdOeDLwwY=
github.com/pelletier/go-toml/v2 v2.2.4 h1:mye9XuhQ6gvn5h28+VilKrrPoQVanw5PMw/TB0t5Ec4=
github.com/pelletier/go-toml/v2 v2.2.4/go.mod h1:2gIqNv+qfxSVS7cM2xJQKtLSTLUE9V8t9Stt+h56mCY=
github.com/pjbgf/sha1cd v0.3.2 h1:a9wb0bp1oC2TGwStyn0Umc/IGKQnEgF0vVaZ8QF8eo4=
github.com/pjbgf/sha1cd v0.3.2/go.mod h1:zQWigSxVmsHEZow5qaLtPYxpcKMMQpa09ixqBxuCS6A=
github.com/pkg/errors v0.9.1 h1:FEBLx1zS214owpjy7qsBeixbURkuhQAwrK5UwLGTwt4=
github.com/pkg/errors v0.9.1/go.mod h1:bwawxfHBFNV+L2hUp1rHADufV3IMtnDRdf1r5NINEl0=
github.com/pmezard/go-difflib v1.0.0 h1:4DBwDE0NGyQoBHbLQYPwSUPoCMWR5BEzIk/f1lZbAQM=
github.com/pmezard/go-difflib v1.0.0/go.mod h1:iKH77koFhYxTK1pcRnkKkqfTogsbg7gZNVY4sRDYZ/4=
github.com/quic-go/qpack v0.5.1 h1:giqksBPnT/HDtZ6VhtFKgoLOWmlyo9Ei6u9PqzIMbhI=
github.com/quic-go/qpack v0.5.1/go.mod h1:+PC4XFrEskIVkcLzpEkbLqq1uCoxPhQuvK5rH1ZgaEg=
github.com/rivo/uniseg v0.1.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.2.0/go.mod h1:J6wj4VEh+S6ZtnVlnTBMWIodfgj8LQOQFoIToxlJtxc=
github.com/rivo/uniseg v0.4.7 h1:WUdvkW8uEhrYfLC4ZzdpI2ztxP1I582+49Oc5Mq64VQ=
github.com/rivo/uniseg v0.4.7/go.mod h1:FN3SvrM+Zdj16jyLfmOkMNblXMcoc8DfTHruCPUcx88=
github.com/rogpeppe/go-internal v1.14.1 h1:UQB4HGPB6osV0SQTLymcB4TgvyWu6ZyliaW0tI/otEQ=
github.com/rogpeppe/go-internal v1.14.1/go.mod h1:MaRKkUm5W0goXpeCfT7UZI6fk/L7L7so1lCWt35ZSgc=
github.com/rs/xid v1.6.0/go.mod h1:7XoLgs4eV+QndskICGsho+ADou8ySMSjJKDIan90Nz0=
github.com/rs/zerolog v1.34.0 h1:k43nTLIwcTVQAncfCw4KZ2VY6ukYoZaBPNOE8txlOeY=
github.com/rs/zerolog v1.34.0/go.mod h1:bJsvje4Z08ROH4Nhs5iH600c3IkWhwp44iRc54W6wYQ=
github.com/russross/blackfriday/v2 v2.1.0/go.mod h1:+Rmxgy9KzJVeS9/2gXHxylqXiyQDYRxCVz55jmeOWTM=
github.com/sagikazarmark/locafero v0.11.0 h1:1iurJgmM9G3PA/I+wWYIOw/5SyBtxapeHDcg+AAIFXc=
github.com/sagikazarmark/locafero v0.11.0/go.mod h1:nVIGvgyzw595SUSUE6tvCp3YYTeHs15MvlmU87WwIik=
github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d h1:hrujxIzL1woJ7AwssoOcM/tq5JjjG2yYOc8odClEiXA=
github.com/saintfish/chardet v0.0.0-20230101081208-5e3ef4b5456d/go.mod h1:uugorj2VCxiV1x+LzaIdVa9b4S4qGAcH6cbhh4qVxOU=
github.com/schollz/progressbar/v3 v3.18.0 h1:uXdoHABRFmNIjUfte/Ex7WtuyVslrw2wVPQmCN62HpA=
github.com/schollz/progressbar/v3 v3.18.0/go.mod h1:IsO3lpbaGuzh8zIMzgY3+J8l4C8GjO0Y9S69eFvNsec=
github.com/scylladb/termtables v0.0.0-20191203121021-c4c0b6d42ff4/go.mod h1:C1a7PQSMz9NShzorzCiG2fk9+xuCgLkPeCvMHYR2OWg=
github.com/sebdah/goldie/v2 v2.8.0 h1:dZb9wR8q5++oplmEiJT+U/5KyotVD+HNGCAc5gNr8rc=
github.com/sebdah/goldie/v2 v2.8.0/go.mod h1:oZ9fp0+se1eapSRjfYbsV/0Hqhbuu3bJVvKI/NNtssI=
github.com/sergi/go-diff v1.4.0 h1:n/SP9D5ad1fORl+llWyN+D6qoUETXNZARKjyY2/KVCw=
github.com/sergi/go-diff v1.4.0/go.mod h1:A0bzQcvG0E7Rwjx0REVgAGH58e96+X0MeOfepqsbeW4=
github.com/sirupsen/logrus v1.7.0/go.mod h1:yWOB1SBYBC5VeMP7gHvWumXLIWorT60ONWic61uBYv0=
github.com/skeema/knownhosts v1.3.1 h1:X2osQ+RAjK76shCbvhHHHVl3ZlgDm8apHEHFqRjnBY8=
github.com/skeema/knownhosts v1.3.1/go.mod h1:r7KTdC8l4uxWRyK2TpQZ/1o5HaSzh06ePQNxPwTcfiY=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8 h1:+jumHNA0Wrelhe64i8F6HNlS8pkoyMv5sreGx2Ry5Rw=
github.com/sourcegraph/conc v0.3.1-0.20240121214520-5f936abd7ae8/go.mod h1:3n1Cwaq1E1/1lhQhtRK2ts/ZwZEhjcQeJQ1RuC6Q/8U=
github.com/spf13/afero v1.15.0 h1:b/YBCLWAJdFWJTN9cLhiXXcD7mzKn9Dm86dNnfyQw1I=
github.com/spf13/afero v1.15.0/go.mod h1:NC2ByUVxtQs4b3sIUphxK0NioZnmxgyCrfzeuq8lxMg=
github.com/spf13/cast v1.10.0 h1:h2x0u2shc1QuLHfxi+cTJvs30+ZAHOGRic8uyGTDWxY=
github.com/spf13/cast v1.10.0/go.mod h1:jNfB8QC9IA6ZuY2ZjDp0KtFO2LZZlg4S/7bzP6qqeHo=
github.com/spf13/cobra v1.10.2 h1:DMTTonx5m65Ic0GOoRY2c16WCbHxOOw6xxezuLaBpcU=
github.com/spf13/cobra v1.10.2/go.mod h1:7C1pvHqHw5A4vrJfjNwvOdzYu0Gml16OCs2GRiTUUS4=
github.com/spf13/pflag v1.0.9/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/pflag v1.0.10 h1:4EBh2KAYBwaONj6b2Ye1GiHfwjqyROoF4RwYO+vPwFk=
github.com/spf13/pflag v1.0.10/go.mod h1:McXfInJRrz4CZXVZOBLb0bTZqETkiAhM9Iw0y3An2Bg=
github.com/spf13/viper v1.21.0 h1:x5S+0EU27Lbphp4UKm1C+1oQO+rKx36vfCoaVebLFSU=
github.com/spf13/viper v1.21.0/go.mod h1:P0lhsswPGWD/1lZJ9ny3fYnVqxiegrlNrEmgLjbTCAY=
github.com/stretchr/objx v0.1.0/go.mod h1:HFkY916IF+rwdDfMAkV7OtwuqBVzrE8GR6GFx+wExME=
github.com/stretchr/objx v0.5.2 h1:xuMeJ0Sdp5ZMRXx/aWO6RZxdr3beISkG5/G/aIRr3pY=
github.com/stretchr/objx v0.5.2/go.mod h1:FRsXN1f5AsAjCGJKqEizvkpNtU+EGNCLh3NxZ/8L+MA=
github.com/stretchr/testify v1.2.2/go.mod h1:a8OnRcib4nhh0OaRAV+Yts87kKdq0PP7pXfy6kDkUVs=
github.com/stretchr/testify v1.3.0/go.mod h1:M5WIy9Dh21IEIfnGCwXGc5bZfKNJtfHm1UVUgZn+9EI=
github.com/stretchr/testify v1.4.0/go.mod h1:j7eGeouHqKxXV5pUuKE4zz7dFj8WfuZ+81PSLYec5m4=
github.com/stretchr/testify v1.7.0/go.mod h1:6Fq8oRcR53rry900zMqJjRRixrwX3KX962/h/Wwjteg=
github.com/stretchr/testify v1.11.1 h1:7s2iGBzp5EwR7/aIZr8ao5+dra3wiQyKjjFuvgVKu7U=
github.com/stretchr/testify v1.11.1/go.mod h1:wZwfW3scLgRK+23gO65QZefKpKQRnfz6sD981Nm4B6U=
github.com/subosito/gotenv v1.6.0 h1:9NlTDc1FTs4qu0DDq7AEtTPNw6SVm7uBMsUCUjABIf8=
github.com/subosito/gotenv v1.6.0/go.mod h1:Dk4QP5c2W3ibzajGcXpNraDfq2IrhjMIvMSWPKKo0FU=
github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5 h1:YqAladjX7xpA6BM04leXMWAEjS0mTZ5kUU9KRBriQJc=
github.com/tam7t/hpkp v0.0.0-20160821193359-2b70b4024ed5/go.mod h1:2JjD2zLQYH5HO74y5+aE3remJQvl6q4Sn6aWA2wD1Ng=
github.com/temoto/robotstxt v1.1.2 h1:W2pOjSJ6SWvldyEuiFXNxz3xZ8aiWX5LbfDiOFd7Fxg=
github.com/temoto/robotstxt v1.1.2/go.mod h1:+1AmkuG3IYkh1kv0d2qEB9Le88ehNO0zwOr3ujewlOo=
github.com/xanzy/ssh-agent v0.3.3 h1:+/15pJfg/RsTxqYcX6fHqOXZwwMP+2VyYWJeWM2qQFM=
github.com/xanzy/ssh-agent v0.3.3/go.mod h1:6dzNDKs0J9rVPHPhaGCukekBHKqfl+L3KghI1Bc68Uw=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e h1:JVG44RsyaB9T2KIHavMF/ppJZNG9ZpyihvCd0w101no=
github.com/xo/terminfo v0.0.0-20220910002029-abceb7e1c41e/go.mod h1:RbqR21r5mrJuqunuUZ/Dhy/avygyECGrLceyNeo4LiM=
github.com/xyproto/randomstring v1.0.5 h1:YtlWPoRdgMu3NZtP45drfy1GKoojuR7hmRcnhZqKjWU=
github.com/xyproto/randomstring v1.0.5/go.mod h1:rgmS5DeNXLivK7YprL0pY+lTuhNQW3iGxZ18UQApw/E=
github.com/ysmood/fetchup v0.2.3 h1:ulX+SonA0Vma5zUFXtv52Kzip/xe7aj4vqT5AJwQ+ZQ=
github.com/ysmood/fetchup v0.2.3/go.mod h1:xhibcRKziSvol0H1/pj33dnKrYyI2ebIvz5cOOkYGns=
github.com/ysmood/goob v0.4.0 h1:HsxXhyLBeGzWXnqVKtmT9qM7EuVs/XOgkX7T6r1o1AQ=
github.com/ysmood/goob v0.4.0/go.mod h1:u6yx7ZhS4Exf2MwciFr6nIM8knHQIE22lFpWHnfql18=
github.com/ysmood/gop v0.0.2/go.mod h1:rr5z2z27oGEbyB787hpEcx4ab8cCiPnKxn0SUHt6xzk=
github.com/ysmood/gop v0.2.0 h1:+tFrG0TWPxT6p9ZaZs+VY+opCvHU8/3Fk6BaNv6kqKg=
github.com/ysmood/gop v0.2.0/go.mod h1:rr5z2z27oGEbyB787hpEcx4ab8cCiPnKxn0SUHt6xzk=
github.com/ysmood/got v0.34.1/go.mod h1:yddyjq/PmAf08RMLSwDjPyCvHvYed+WjHnQxpH851LM=
github.com/ysmood/got v0.40.0 h1:ZQk1B55zIvS7zflRrkGfPDrPG3d7+JOza1ZkNxcc74Q=
github.com/ysmood/got v0.40.0/go.mod h1:W7DdpuX6skL3NszLmAsC5hT7JAhuLZhByVzHTq874Qg=
github.com/ysmood/gotrace v0.6.0 h1:SyI1d4jclswLhg7SWTL6os3L1WOKeNn/ZtzVQF8QmdY=
github.com/ysmood/gotrace v0.6.0/go.mod h1:TzhIG7nHDry5//eYZDYcTzuJLYQIkykJzCRIo4/dzQM=
github.com/ysmood/gson v0.7.3 h1:QFkWbTH8MxyUTKPkVWAENJhxqdBa4lYTQWqZCiLG6kE=
github.com/ysmood/gson v0.7.3/go.mod h1:3Kzs5zDl21g5F/BlLTNcuAGAYLKt2lV5G8D1zF3RNmg=
github.com/ysmood/leakless v0.8.0/go.mod h1:R8iAXPRaG97QJwqxs74RdwzcRHT1SWCGTNqY8q0JvMQ=
github.com/ysmood/leakless v0.9.0 h1:qxCG5VirSBvmi3uynXFkcnLMzkphdh3xx5FtrORwDCU=
github.com/ysmood/leakless v0.9.0/go.mod h1:R8iAXPRaG97QJwqxs74RdwzcRHT1SWCGTNqY8q0JvMQ=
github.com/yuin/goldmark v1.4.13/go.mod h1:6yULJ656Px+3vBD8DxQVa3kxgyrAnzto9xy5taEt/CY=
github.com/yuin/goldmark v1.7.13 h1:GPddIs617DnBLFFVJFgpo1aBfe/4xcvMc3SB5t/D0pA=
github.com/yuin/goldmark v1.7.13/go.mod h1:ip/1k0VRfGynBgxOz0yCqHrbZXhcjxyuS66Brc7iBKg=
go.opentelemetry.io/auto/sdk v1.1.0 h1:cH53jehLUN6UFLY71z+NDOiNJqDdPRaXzTel0sJySYA=
go.opentelemetry.io/auto/sdk v1.1.0/go.mod h1:3wSPjt5PWp2RhlCcmmOial7AvC4DQqZb7a7wCow3W8A=
go.opentelemetry.io/otel v1.37.0 h1:9zhNfelUvx0KBfu/gb+ZgeAfAgtWrfHJZcAqFC228wQ=
go.opentelemetry.io/otel v1.37.0/go.mod h1:ehE/umFRLnuLa/vSccNq9oS1ErUlkkK71gMcN34UG8I=
go.opentelemetry.io/otel/metric v1.37.0 h1:mvwbQS5m0tbmqML4NqK+e3aDiO02vsf/WgbsdpcPoZE=
go.opentelemetry.io/otel/metric v1.37.0/go.mod h1:04wGrZurHYKOc+RKeye86GwKiTb9FKm1WHtO+4EVr2E=
go.opentelemetry.io/otel/trace v1.37.0 h1:HLdcFNbRQBE2imdSEgm/kwqmQj1Or1l/7bW6mxVK7z4=
go.opentelemetry.io/otel/trace v1.37.0/go.mod h1:TlgrlQ+PtQO5XFerSPUYG0JSgGyryXewPGyayAWSBS0=
go.uber.org/mock v0.5.0 h1:KAMbZvZPyBPWgD14IrIQ38QCyjwpvVVV6K/bHl1IwQU=
go.uber.org/mock v0.5.0/go.mod h1:ge71pBPLYDk7QIi1LupWxdAykm7KIEFchiOqd6z7qMM=
go.yaml.in/yaml/v3 v3.0.4 h1:tfq32ie2Jv2UxXFdLJdh3jXuOzWiL1fo0bu/FbuKpbc=
go.yaml.in/yaml/v3 v3.0.4/go.mod h1:DhzuOOF2ATzADvBadXxruRBLzYTpT36CKvDb3+aBEFg=
golang.org/x/crypto v0.0.0-20190308221718-c2843e01d9a2/go.mod h1:djNgcEr1/C05ACkg1iLfiJU5Ep61QUkGW8qpdssI0+w=
golang.org/x/crypto v0.0.0-20210921155107-089bfa567519/go.mod h1:GvvjBRRGRdwPK5ydBHafDWAxML/pGHZbMvKqRZ5+Abc=
golang.org/x/crypto v0.0.0-20220622213112-05595931fe9d/go.mod h1:IxCIyHEi3zRg3s0A5j5BB6A9Jmi73HwBIUl50j+osU4=
golang.org/x/crypto v0.13.0/go.mod h1:y6Z2r+Rw4iayiXXAIxJIDAJ1zMW4yaTpebo8fPOliYc=
golang.org/x/crypto v0.19.0/go.mod h1:Iy9bg/ha4yyC70EfRS8jz+B6ybOBKMaSxLj6P6oBDfU=
golang.org/x/crypto v0.23.0/go.mod h1:CKFgDieR+mRhux2Lsu27y0fO304Db0wZe70UKqHu0v8=
golang.org/x/crypto v0.31.0/go.mod h1:kDsLvtWBEx7MV9tJOj9bnXsPbxwJQ6csT/x4KIN4Ssk=
golang.org/x/crypto v0.32.0/go.mod h1:ZnnJkOaASj8g0AjIduWNlq2NRxL0PlBrbKVyZ6V/Ugc=
golang.org/x/crypto v0.44.0 h1:A97SsFvM3AIwEEmTBiaxPPTYpDC47w720rdiiUvgoAU=
golang.org/x/crypto v0.44.0/go.mod h1:013i+Nw79BMiQiMsOPcVCB5ZIJbYkerPrGnOa00tvmc=
golang.org/x/exp v0.0.0-20240719175910-8a7402abbf56 h1:2dVuKD2vS7b0QIHQbpyTISPd0LeHDbnYEryqj5Q1ug8=
golang.org/x/exp v0.0.0-20240719175910-8a7402abbf56/go.mod h1:M4RDyNAINzryxdtnbRXRL/OHtkFuWGRjvuhBJpk2IlY=
golang.org/x/mod v0.6.0-dev.0.20220419223038-86c51ed26bb4/go.mod h1:jJ57K6gSWd91VN4djpZkiMVwK6gcyfeH4XE8wZrZaV4=
golang.org/x/mod v0.8.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/mod v0.12.0/go.mod h1:iBbtSCu2XBx23ZKBPSOrRkjjQPZFPuis4dIYUhu/chs=
golang.org/x/mod v0.15.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
golang.org/x/mod v0.17.0/go.mod h1:hTbmBsO62+eylJbnUtE2MGJUyE7QWk4xUqPFrRgJ+7c=
golang.org/x/mod v0.29.0 h1:HV8lRxZC4l2cr3Zq1LvtOsi/ThTgWnUk/y64QSs8GwA=
golang.org/x/mod v0.29.0/go.mod h1:NyhrlYXJ2H4eJiRy/WDBO6HMqZQ6q9nk4JzS3NuCK+w=
golang.org/x/net v0.0.0-20190620200207-3b0461eec859/go.mod h1:z5CRVTTTmAJ677TzLLGU+0bjPO0LkuOLi4/5GtJWs/s=
golang.org/x/net v0.0.0-20210226172049-e18ecbb05110/go.mod h1:m0MpNAwzfU5UDzcl9v0D8zg8gWTRqZa9RBIspLL5mdg=
golang.org/x/net v0.0.0-20211112202133-69e39bad7dc2/go.mod h1:9nx3DQGgdP8bBQD5qxJ1jj9UTztislL4KSBs9R2vV5Y=
golang.org/x/net v0.0.0-20220722155237-a158d28d115b/go.mod h1:XRhObCWvk6IyKnWLug+ECip1KBveYUHfp+8e9klMJ9c=
golang.org/x/net v0.6.0/go.mod h1:2Tu9+aMcznHK/AK1HMvgo6xiTLG5rD5rZLDS+rp2Bjs=
golang.org/x/net v0.10.0/go.mod h1:0qNGK6F8kojg2nk9dLZ2mShWaEBan6FAoqfSigmmuDg=
golang.org/x/net v0.15.0/go.mod h1:idbUs1IY1+zTqbi8yxTbhexhEEk5ur9LInksu6HrEpk=
golang.org/x/net v0.21.0/go.mod h1:bIjVDfnllIU7BJ2DNgfnXvpSvtn8VRwhlsaeUTyUS44=
golang.org/x/net v0.25.0/go.mod h1:JkAGAh7GEvH74S6FOH42FLoXpXbE/aqXSrIQjXgsiwM=
golang.org/x/net v0.33.0/go.mod h1:HXLR5J+9DxmrqMwG9qjGCxZ+zKXxBru04zlTvWlWuN4=
golang.org/x/net v0.34.0/go.mod h1:di0qlW3YNM5oh6GqDGQr92MyTozJPmybPK4Ev/Gm31k=
golang.org/x/net v0.47.0 h1:Mx+4dIFzqraBXUugkia1OOvlD6LemFo1ALMHjrXDOhY=
golang.org/x/net v0.47.0/go.mod h1:/jNxtkgq5yWUGYkaZGqo27cfGZ1c5Nen03aYrrKpVRU=
golang.org/x/sync v0.0.0-20190423024810-112230192c58/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.0.0-20220722155255-886fb9371eb4/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.1.0/go.mod h1:RxMgew5VJxzue5/jJTE5uejpjVlOe/izrB70Jof72aM=
golang.org/x/sync v0.3.0/go.mod h1:FU7BRWz2tNW+3quACPkgCx/L+uEAv1htQ0V83Z9Rj+Y=
golang.org/x/sync v0.6.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.7.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.10.0/go.mod h1:Czt+wKu1gCyEFDUtn0jG5QVvpJ6rzVqr5aXyt9drQfk=
golang.org/x/sync v0.18.0 h1:kr88TuHDroi+UVf+0hZnirlk8o8T+4MrK6mr60WkH/I=
golang.org/x/sync v0.18.0/go.mod h1:9KTHXmSnoGruLpwFjVSX0lNNA75CykiMECbovNTZqGI=
golang.org/x/sys v0.0.0-20190215142949-d0b11bdaac8a/go.mod h1:STP8DvDyc/dI5b8T5hshtkjS+E42TnysNCUPdjciGhY=
golang.org/x/sys v0.0.0-20191026070338-33540a1f6037/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20201119102817-f84b799fce68/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210124154548-22da62e12c0c/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210423082822-04245dca01da/go.mod h1:h1NjWce9XRLGQEsW7wpKNCjG9DtNlClVuFLEZdDNbEs=
golang.org/x/sys v0.0.0-20210615035016-665e8c7367d1/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20210809222454-d867a43fc93e/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220520151302-bc2c85ada10a/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220715151400-c0bba94af5f8/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220722155257-8c9f86f7a55f/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.0.0-20220811171246-fbc7d0a398ab/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.5.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.6.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.8.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.12.0/go.mod h1:oPkhp1MJrh7nUepCBck5+mAzfO9JrbApNNgaTdGDITg=
golang.org/x/sys v0.17.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.20.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.28.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.29.0/go.mod h1:/VUhepiaJMQUp4+oa/7Zr1D23ma6VTLIYjOOTFZPUcA=
golang.org/x/sys v0.38.0 h1:3yZWxaJjBmCWXqhN1qh02AkOnCQ1poK6oF+a7xWL6Gc=
golang.org/x/sys v0.38.0/go.mod h1:OgkHotnGiDImocRcuBABYBEXf8A9a87e/uXjp9XT3ks=
golang.org/x/telemetry v0.0.0-20240228155512-f48c80bd79b2/go.mod h1:TeRTkGYfJXctD9OcfyVLyj2J3IxLnKwHJR8f4D8a3YE=
golang.org/x/term v0.0.0-20201126162022-7de9c90e9dd1/go.mod h1:bj7SfCRtBDWHUb9snDiAeCFNEtKQo2Wmx5Cou7ajbmo=
golang.org/x/term v0.0.0-20210927222741-03fcf44c2211/go.mod h1:jbD1KX2456YbFQfuXm/mYQcufACuNUgVhRMnK/tPxf8=
golang.org/x/term v0.5.0/go.mod h1:jMB1sMXY+tzblOD4FWmEbocvup2/aLOaQEp7JmGp78k=
golang.org/x/term v0.8.0/go.mod h1:xPskH00ivmX89bAKVGSKKtLOWNx2+17Eiy94tnKShWo=
golang.org/x/term v0.12.0/go.mod h1:owVbMEjm3cBLCHdkQu9b1opXd4ETQWc3BhuQGKgXgvU=
golang.org/x/term v0.17.0/go.mod h1:lLRBjIVuehSbZlaOtGMbcMncT+aqLLLmKrsjNrUguwk=
golang.org/x/term v0.20.0/go.mod h1:8UkIAJTvZgivsXaD6/pH6U9ecQzZ45awqEOzuCvwpFY=
golang.org/x/term v0.27.0/go.mod h1:iMsnZpn0cago0GOrHO2+Y7u7JPn5AylBrcoWkElMTSM=
golang.org/x/term v0.28.0/go.mod h1:Sw/lC2IAUZ92udQNf3WodGtn4k/XoLyZoh8v/8uiwek=
golang.org/x/term v0.37.0 h1:8EGAD0qCmHYZg6J17DvsMy9/wJ7/D/4pV/wfnld5lTU=
golang.org/x/term v0.37.0/go.mod h1:5pB4lxRNYYVZuTLmy8oR2BH8dflOR+IbTYFD8fi3254=
golang.org/x/text v0.3.0/go.mod h1:NqM8EUOU14njkJ3fqMW+pc6Ldnwhi/IjpwHt7yyuwOQ=
golang.org/x/text v0.3.3/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.6/go.mod h1:5Zoc/QRtKVWzQhOtBMvqHzDpF6irO9z98xDceosuGiQ=
golang.org/x/text v0.3.7/go.mod h1:u+2+/6zg+i71rQMx5EYifcz6MCKuco9NR6JIITiCfzQ=
golang.org/x/text v0.3.8/go.mod h1:E6s5w1FMmriuDzIBO73fBruAKo1PCIq6d2Q6DHfQ8WQ=
golang.org/x/text v0.7.0/go.mod h1:mrYo+phRRbMaCq/xk9113O4dZlRixOauAjOtrjsXDZ8=
golang.org/x/text v0.9.0/go.mod h1:e1OnstbJyHTd6l/uOt8jFFHp6TRDWZR/bV3emEE/zU8=
golang.org/x/text v0.13.0/go.mod h1:TvPlkZtksWOMsz7fbANvkp4WM8x/WCo/om8BMLbz+aE=
golang.org/x/text v0.14.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=
golang.org/x/text v0.15.0/go.mod h1:18ZOQIKpY8NJVqYksKHtTdi31H5itFRjB5/qKTNYzSU=
golang.org/x/text v0.21.0/go.mod h1:4IBbMaMmOPCJ8SecivzSH54+73PCFmPWxNTLm+vZkEQ=
golang.org/x/text v0.31.0 h1:aC8ghyu4JhP8VojJ2lEHBnochRno1sgL6nEi9WGFGMM=
golang.org/x/text v0.31.0/go.mod h1:tKRAlv61yKIjGGHX/4tP1LTbc13YSec1pxVEWXzfoeM=
golang.org/x/tools v0.0.0-20180917221912-90fa682c2a6e/go.mod h1:n7NCudcB/nEzxVGmLbDWY5pfWTLqBcC2KZ6jyYvM4mQ=
golang.org/x/tools v0.0.0-20191119224855-298f0cb1881e/go.mod h1:b+2E5dAYhXwXZwtnZ6UAqBI28+e2cm9otk0dWdXHAEo=
golang.org/x/tools v0.1.12/go.mod h1:hNGJHUnrk76NpqgfD5Aqm5Crs+Hm0VOH/i9J2+nxYbc=
golang.org/x/tools v0.6.0/go.mod h1:Xwgl3UAJ/d3gWutnCtw505GrjyAbvKui8lOU390QaIU=
golang.org/x/tools v0.13.0/go.mod h1:HvlwmtVNQAhOuCjW7xxvovg8wbNq7LwfXh/k7wXUl58=
golang.org/x/tools v0.21.1-0.20240508182429-e35e4ccd0d2d/go.mod h1:aiJjzUbINMkxbQROHiO6hDPo2LHcIPhhQsa9DLh0yGk=
golang.org/x/tools v0.38.0 h1:Hx2Xv8hISq8Lm16jvBZ2VQf+RLmbd7wVUsALibYI/IQ=
golang.org/x/tools v0.38.0/go.mod h1:yEsQ/d/YK8cjh0L6rZlY8tgtlKiBNTL14pGDJPJpYQs=
golang.org/x/xerrors v0.0.0-20190717185122-a985d3407aa7/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
golang.org/x/xerrors v0.0.0-20191204190536-9bdfabe68543/go.mod h1:I/5z698sn9Ka8TeJc9MKroUUfqBBauWjQqLJ2OPfmY0=
google.golang.org/appengine v1.6.8 h1:IhEN5q69dyKagZPYMSdIjS2HqprW324FRQZJcGqPAsM=
google.golang.org/appengine v1.6.8/go.mod h1:1jJ3jBArFh5pcgW8gCtRJnepW8FzD1V44FJffLiz/Ds=
google.golang.org/protobuf v1.26.0-rc.1/go.mod h1:jlhhOSvTdKEhbULTjvd4ARK9grFBp09yW+WbY/TyQbw=
google.golang.org/protobuf v1.26.0/go.mod h1:9q0QmTI4eRPtz6boOQmLYwt+qCgq0jsYwAQnmE0givc=
google.golang.org/protobuf v1.36.10 h1:AYd7cD/uASjIL6Q9LiTjz8JLcrh/88q5UObnmY3aOOE=
google.golang.org/protobuf v1.36.10/go.mod h1:HTf+CrKn2C3g5S8VImy6tdcUvCska2kB7j23XfzDpco=
gopkg.in/check.v1 v0.0.0-20161208181325-20d25e280405/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20190902080502-41f04d3bba15/go.mod h1:Co6ibVJAznAaIkqp8huTwlJQCZ016jof/cbN4VW5Yz0=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c h1:Hei/4ADfdWqJk1ZMxUNpqntNwaWcugrBjAiHlqqRiVk=
gopkg.in/check.v1 v1.0.0-20201130134442-10cb98267c6c/go.mod h1:JHkPIbrfpd72SG/EVd6muEfDQjcINNoR0C8j2r3qZ4Q=
gopkg.in/warnings.v0 v0.1.2 h1:wFXVbFY8DY5/xOe1ECiWdKCzZlxgshcYVNkBHstARME=
gopkg.in/warnings.v0 v0.1.2/go.mod h1:jksf8JmL6Qr/oQM2OXTHunEvvTAsrWBLb6OOjuVWRNI=
gopkg.in/yaml.v2 v2.2.2/go.mod h1:hI93XBmqTisBFMUTm0b8Fm+jr3Dg1NNxqwp+5A1VGuI=
gopkg.in/yaml.v2 v2.4.0/go.mod h1:RDklbk79AGWmwhnvt/jBztapEOGDOx6ZbXqjP6csGnQ=
gopkg.in/yaml.v3 v3.0.0-20200313102051-9f266ea9e77c/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
gopkg.in/yaml.v3 v3.0.1 h1:fxVm/GzAzEWqLHuvctI91KS9hhNmmWOoWu0XTYJS7CA=
gopkg.in/yaml.v3 v3.0.1/go.mod h1:K4uyk7z7BCEPqu6E+C64Yfv1cQ7kz7rIZviUmN+EgEM=
</file>
<file path="LICENSE">
Creative Commons Attribution-NonCommercial 4.0 International

Copyright (c) 2025-2026

This work is licensed under the Creative Commons Attribution-NonCommercial 4.0
International License.

You are free to:
- Share — copy and redistribute the material in any medium or format
- Adapt — remix, transform, and build upon the material

Under the following terms:
- Attribution — You must give appropriate credit, provide a link to the license,
  and indicate if changes were made.
- NonCommercial — You may not use the material for commercial purposes.

No additional restrictions — You may not apply legal terms or technological measures
  that legally restrict others from doing anything the license permits.

To view a copy of this license, visit:
http://creativecommons.org/licenses/by-nc/4.0/

---

SUMMARY: This license allows you to use, modify, and distribute this software for
non-commercial purposes, as long as you credit the original author. Commercial use
is not permitted.
</file>
<file path="Makefile">
# Makefile

BINARY_NAME=repodocs
VERSION=$(shell git describe --tags --always --dirty 2>/dev/null || echo "dev")
BUILD_TIME=$(shell date -u '+%Y-%m-%dT%H:%M:%SZ')
COMMIT=$(shell git rev-parse --short HEAD 2>/dev/null || echo "unknown")
LDFLAGS=-ldflags "-X github.com/quantmind-br/repodocs-go/pkg/version.Version=$(VERSION) -X github.com/quantmind-br/repodocs-go/pkg/version.BuildTime=$(BUILD_TIME) -X github.com/quantmind-br/repodocs-go/pkg/version.Commit=$(COMMIT) -s -w"

BUILD_DIR=./build
INSTALL_DIR=$(HOME)/.local/bin
CONFIG_DIR=$(HOME)/.repodocs

.PHONY: build test test-all coverage lint deps install uninstall release release-dry clean help

build: ## Build the binary
	@mkdir -p $(BUILD_DIR)
	CGO_ENABLED=0 go build $(LDFLAGS) -o $(BUILD_DIR)/$(BINARY_NAME) ./cmd/repodocs

test: ## Run unit tests
	go test -v -race -short ./...

test-all: ## Run all tests (unit + integration + e2e)
	go test -v -race ./...

coverage: ## Generate coverage report (HTML in ./coverage/)
	@mkdir -p ./coverage
	go test -coverprofile=./coverage/coverage.out -covermode=atomic ./...
	go tool cover -html=./coverage/coverage.out -o ./coverage/coverage.html
	@go tool cover -func=./coverage/coverage.out | tail -1
	@echo "Report: ./coverage/coverage.html"

lint: ## Run linters and format code
	@which golangci-lint > /dev/null || go install github.com/golangci/golangci-lint/v2/cmd/golangci-lint@latest
	gofmt -s -w .
	golangci-lint run ./...

deps: ## Download and tidy dependencies
	go mod download
	go mod tidy

install: build ## Build and install to ~/.local/bin
	@mkdir -p $(INSTALL_DIR) $(CONFIG_DIR)
	@install -m 755 $(BUILD_DIR)/$(BINARY_NAME) $(INSTALL_DIR)/
	@test -f $(CONFIG_DIR)/config.yaml || cp ./configs/config.yaml.template $(CONFIG_DIR)/config.yaml
	@echo "✓ Installed to $(INSTALL_DIR)/$(BINARY_NAME)"

uninstall: ## Remove from ~/.local/bin
	@rm -f $(INSTALL_DIR)/$(BINARY_NAME)
	@echo "✓ Uninstalled"

release: ## Create GitHub release (interactive)
	@./scripts/release.sh

release-dry: ## Test release build locally (creates ./dist/)
	@which goreleaser > /dev/null || go install github.com/goreleaser/goreleaser/v2@latest
	goreleaser release --snapshot --clean

clean: ## Remove build artifacts
	@rm -rf $(BUILD_DIR) ./coverage ./dist

help: ## Show this help
	@grep -E '^[a-zA-Z_-]+:.*?## .*$$' $(MAKEFILE_LIST) | awk 'BEGIN {FS = ":.*?## "}; {printf "\033[36m%-12s\033[0m %s\n", $$1, $$2}'

.DEFAULT_GOAL := help
</file>
<file path="opencode.json">
{
  "$schema": "https://opencode.ai/config.json",
  "mcp": {
    "serena": {
      "type": "local",
      "command": [
        "uvx",
        "--from", "git+https://github.com/oraios/serena",
        "serena",
        "start-mcp-server",
        "--context", "ide",
        "--project-from-cwd"
      ],
      "enabled": true
    }
  }
}
</file>
<file path="README.md">
# RepoDocs

RepoDocs is a powerful Go-based CLI tool and library designed to extract documentation from diverse sources—including websites, Git repositories, Sitemaps, and Wikis—and convert them into clean, structured Markdown. It is built to facilitate the creation of high-quality datasets for LLM training, RAG (Retrieval-Augmented Generation) pipelines, or local documentation mirrors.

## Features

-   **Multi-Source Extraction**: Automatically detects and handles various source types:
    -   **Web Crawler**: Recursive crawling of documentation sites.
    -   **Git/GitHub**: Cloning repositories or fetching specific paths.
    -   **Sitemaps**: Systematic discovery via `sitemap.xml`.
    -   **llms.txt**: Support for the emerging `llms.txt` standard for LLM-friendly discovery.
    -   **Package Docs**: Specialized handling for `pkg.go.dev`.
-   **Advanced Processing**:
    -   **HTML to Markdown**: Converts complex HTML into clean Markdown using a multi-stage pipeline.
    -   **Content Extraction**: Uses "readability" logic and CSS selectors to isolate main content and remove noise (navbars, footers, scripts).
    -   **JS Rendering**: Headless browser support (via `go-rod`) for Single Page Applications (SPAs) and JavaScript-heavy sites.
-   **Stealth & Robustness**:
    -   **Bot Avoidance**: User-Agent rotation and TLS fingerprinting to bypass basic bot detection.
    -   **Caching**: Persistent caching using BadgerDB to minimize network load and respect rate limits.
    -   **Retries**: Exponential backoff for transient network errors.
-   **AI Integration**: Optional metadata enrichment using LLMs (OpenAI, Anthropic, Google) to generate summaries, tags, and categories.
-   **Structured Output**: Generates Markdown files with YAML frontmatter and a consolidated `repodocs.json` index.

## Installation

### Prerequisites

-   **Go**: 1.21 or later.
-   **Chrome/Chromium**: Required if using the `--render-js` feature for JavaScript rendering.

### From Source

```bash
git clone https://github.com/yourusername/repodocs.git
cd repodocs
go build -o repodocs ./cmd/repodocs
```

### Dependency Check
Use the built-in "doctor" command to verify your environment:
```bash
./repodocs doctor
```

## Testing

RepoDocs has comprehensive test coverage with **64.8% overall coverage** and **9 packages above 90%**.

### Running Tests

```bash
# Unit tests (fast)
make test
# or
go test ./... -short

# Integration tests
make test-integration
# or
go test ./tests/integration/... -tags=integration

# Full test suite
go test ./...

# Coverage report
go test ./... -coverprofile=coverage.out
go tool cover -html=coverage.html
```

### Test Coverage

| Package | Coverage | Status |
|---------|----------|--------|
| git | 100.0% | ✅ Excellent |
| domain | 97.1% | ✅ Excellent |
| manifest | 97.0% | ✅ Excellent |
| state | 95.5% | ✅ Excellent |
| output | 94.4% | ✅ Excellent |
| llm | 93.0% | ✅ Excellent |
| cache | 91.3% | ✅ Excellent |
| utils | 90.1% | ✅ Excellent |
| renderer | 89.1% | ✅ Excellent |
| converter | 87.1% | ✅ Good |
| fetcher | 84.1% | ✅ Good |
| strategies/git | 79.5% | ✅ Good |
| config | 76.0% | ✅ Good |

See [TESTING.md](TESTING.md) for detailed testing guidelines, patterns, and best practices.

## Quick Start

Extract documentation from a URL to the default `./docs` directory:
```bash
repodocs https://docs.example.com
```

Extract a specific GitHub repository with a maximum depth of 2:
```bash
repodocs https://github.com/user/repo --max-depth 2
```

Force JavaScript rendering for a React-based documentation site:
```bash
repodocs https://spa-docs.com --render-js
```

Generate JSON metadata and limit to 10 pages:
```bash
repodocs https://example.com --json-meta --limit 10
```

Process multiple sources from a manifest file:
```bash
repodocs --manifest sources.yaml
```

## Batch Processing with Manifests

For processing multiple documentation sources, RepoDocs supports manifest files in YAML or JSON format. This enables reproducible, one-command data ingestion for complex RAG pipelines.

### Creating a Manifest

Create a `sources.yaml` file:

```yaml
sources:
  - url: https://docs.example.com
    strategy: crawler
    content_selector: "article.main"
    max_depth: 3

  - url: https://github.com/org/repo
    strategy: git
    include:
      - "docs/**/*.md"
      - "README.md"

options:
  output: ./knowledge-base
  continue_on_error: true
```

### Running with a Manifest

```bash
repodocs --manifest sources.yaml
```

### Manifest Schema

#### Sources

Each source defines a documentation URL and optional configuration:

| Field | Type | Required | Description |
|-------|------|----------|-------------|
| `url` | string | Yes | URL to extract documentation from |
| `strategy` | string | No | Force a specific strategy (`crawler`, `git`, `sitemap`, etc.) |
| `content_selector` | string | No | CSS selector for main content |
| `exclude_selector` | string | No | CSS selector for elements to remove |
| `exclude` | array | No | URL/path patterns to skip |
| `include` | array | No | Path patterns to include (git strategy) |
| `max_depth` | int | No | Maximum crawl depth |
| `render_js` | bool | No | Force JavaScript rendering |
| `limit` | int | No | Maximum pages from this source |

#### Options

Global options that apply to the entire manifest:

| Field | Type | Default | Description |
|-------|------|---------|-------------|
| `continue_on_error` | bool | `false` | Continue processing if a source fails |
| `output` | string | `./docs` | Output directory for all sources |
| `concurrency` | int | `5` | Number of concurrent workers |

### Error Handling

By default, execution stops on the first source failure. Use `continue_on_error: true` to process all sources regardless of individual failures:

```yaml
options:
  continue_on_error: true
```

With this option:
- Failed sources are logged but don't stop execution
- All sources are attempted
- Summary shows success/failure counts
- Exit code is non-zero if any source failed

### Example Manifests

See the `examples/manifests/` directory for sample manifest files:
- `simple.yaml` - Basic single-source manifest
- `multi-source.yaml` - Multiple sources with different strategies
- `error-tolerant.yaml` - Continues on errors
- `full-options.yaml` - All available options documented

## Architecture

RepoDocs follows a decoupled, interface-driven architecture structured as a processing pipeline:

1.  **Detection**: The `Orchestrator` uses a `Strategy Factory` to identify the correct approach (Git, Crawler, Sitemap, etc.) based on the input URL.
2.  **Execution**: The selected `Strategy` orchestrates fetching or cloning content.
3.  **Processing**: The `Converter Pipeline` transforms raw content:
    -   **Encoding**: Normalizes text to UTF-8.
    -   **Sanitization**: Removes unwanted HTML tags and noise.
    -   **Conversion**: Transforms cleaned HTML into Markdown.
4.  **Enhancement**: The `MetadataEnhancer` (optional) uses LLMs to enrich the document with summaries and tags.
5.  **Output**: The `Writer` persists the final Markdown and metadata to the local filesystem.

### Core Components

-   **Internal Domain**: Defines core models (`Document`, `Page`) and interfaces (`Fetcher`, `Renderer`, `Cache`).
-   **Fetcher**: High-level HTTP client with stealth capabilities and caching.
-   **Renderer**: Manages a pool of headless browser tabs for dynamic content.
-   **Strategies**: Specialized logic for different documentation sources.

## Configuration

RepoDocs can be configured via CLI flags, environment variables, or a configuration file (default: `~/.repodocs/config.yaml`).

### Interactive Configuration

Use the interactive TUI to configure RepoDocs:

```bash
repodocs config
```

This opens an interactive terminal interface where you can navigate through configuration categories, edit values with validation, and save changes.

### Configuration Commands

| Command | Description |
|---------|-------------|
| `repodocs config` | Open interactive configuration TUI |
| `repodocs config edit` | Open interactive configuration TUI |
| `repodocs config show` | Display current configuration as YAML |
| `repodocs config init` | Create default config file at ~/.repodocs/config.yaml |
| `repodocs config path` | Show configuration file path |

### Accessibility

For screen reader support, enable accessible mode:

```bash
# Using environment variable
ACCESSIBLE=1 repodocs config

# Using flag
repodocs config --accessible
```

### Configuration Categories

The interactive TUI organizes settings into categories:

- **Output**: Directory, flat structure, overwrite behavior, JSON metadata
- **Concurrency**: Workers, timeout, max crawl depth
- **Cache**: Enable/disable, TTL, cache directory
- **Rendering**: JavaScript rendering, JS timeout, scroll behavior
- **Stealth**: User-Agent, random delays
- **Logging**: Log level, log format
- **LLM**: Provider, API key, model, temperature, metadata enhancement

### Common Flags

| Flag | Short | Description | Default |
| :--- | :--- | :--- | :--- |
| `--manifest` | | Path to manifest file (YAML/JSON) for batch processing | |
| `--output` | `-o` | Output directory | `./docs` |
| `--concurrency` | `-j` | Number of concurrent workers | `5` |
| `--max-depth` | `-d` | Maximum crawl depth | `4` |
| `--limit` | `-l` | Maximum number of pages to process | `0` (unlimited) |
| `--render-js` | | Force JavaScript rendering | `false` |
| `--no-cache` | | Disable the BadgerDB caching layer | `false` |
| `--exclude` | | Regex patterns to exclude specific paths | |
| `--json-meta` | | Generate individual `.json` metadata files | `false` |

## Development

### Running Tests
Execute the test suite:
```bash
go test ./...
```

### Linting
The project follows standard Go formatting. Run the linter:
```bash
go vet ./...
```

### Building
Build the binary for your local architecture:
```bash
go build -o repodocs ./cmd/repodocs
```

## Contributing

1.  Ensure all core services are defined via interfaces in the `internal/domain` package.
2.  When adding new extraction logic, implement the `Strategy` interface in `internal/strategies`.
3.  Add unit tests for new components using `go.uber.org/mock` for dependency mocking.
4.  Ensure any changes to the HTML-to-Markdown pipeline are reflected in the `internal/converter` package.

## License

Refer to the `LICENSE` file for details.
</file>
<file path="TESTING.md">
# Testing Guide

This document provides comprehensive information about testing practices in the repodocs-go project.

## Test Coverage Overview

**Current Coverage**: 64.8% overall

### Coverage by Package

| Package | Coverage | Status |
|---------|----------|--------|
| git | 100.0% | ✅ Excellent |
| domain | 97.1% | ✅ Excellent |
| manifest | 97.0% | ✅ Excellent |
| state | 95.5% | ✅ Excellent |
| output | 94.4% | ✅ Excellent |
| llm | 92.9% | ✅ Excellent |
| utils | 90.9% | ✅ Excellent |
| cache | 91.3% | ✅ Excellent |
| config | 91.4% | ✅ Excellent |
| converter | 87.3% | ✅ Good |
| fetcher | 84.1% | ✅ Good |
| strategies/git | 79.5% | ✅ Good |
| app | 58.8% | ⚠️ Needs improvement |
| strategies | 40.0% | ⚠️ Needs improvement |
| renderer | 30.3% | ⚠️ Needs work |

## Running Tests

### Unit Tests (Fast)
```bash
make test
# or
go test ./... -short
```

### Integration Tests
```bash
make test-integration
# or
go test ./tests/integration/... -tags=integration
```

### E2E Tests
```bash
make test-e2e
# or
go test ./tests/e2e/... -tags=e2e
```

### Coverage Report
```bash
go test ./... -coverprofile=coverage.out
go tool cover -html=coverage.html
open coverage.html
```

## Test Structure

```
tests/
├── unit/               # Fast unit tests with mocks
│   ├── app/           # Application orchestrator tests
│   ├── cache/         # Cache implementation tests
│   ├── config/        # Configuration tests
│   ├── converter/     # HTML/Markdown conversion tests
│   ├── domain/        # Domain models and interfaces
│   ├── fetcher/       # HTTP client tests
│   ├── git/           # Git operations tests
│   ├── llm/           # LLM provider tests
│   ├── manifest/      # Manifest parsing tests
│   ├── output/        # Output writer tests
│   ├── renderer/      # Browser renderer tests
│   ├── state/         # State manager tests
│   └── strategies/    # Strategy pattern tests
├── integration/       # Integration tests with real services
│   ├── fetcher/
│   ├── llm/
│   ├── renderer/
│   └── strategies/
└── e2e/              # End-to-end tests
```

## Testing Patterns

### 1. Table-Driven Tests

Preferred for testing multiple scenarios:

```go
func TestFunction(t *testing.T) {
    tests := []struct {
        name    string
        input   string
        wantErr bool
    }{
        {"valid input", "test", false},
        {"invalid input", "", true},
    }

    for _, tt := range tests {
        t.Run(tt.name, func(t *testing.T) {
            err := YourFunction(tt.input)
            if tt.wantErr {
                assert.Error(t, err)
            } else {
                assert.NoError(t, err)
            }
        })
    }
}
```

### 2. Mock Testing

Using `go.uber.org/mock` for interfaces:

```go
// Generate mock
//go:generate mockgen -source=domain/cache.go -destination=../../mocks/mock_cache.go

func TestWithMock(t *testing.T) {
    ctrl := gomock.NewController(t)
    defer ctrl.Finish()

    mockCache := mocks.NewMockCache(ctrl)
    mockCache.EXPECT().Get(gomock.Any(), "key").Return([]byte("value"), nil)
}
```

### 3. Test Helpers

Create reusable test helpers:

```go
// tests/testutil/testutil.go
func SetupTestServer(t *testing.T) *httptest.Server {
    server := httptest.NewServer(http.HandlerFunc(func(w http.ResponseWriter, r *http.Request) {
        w.WriteHeader(http.StatusOK)
        w.Write([]byte("OK"))
    }))
    t.Cleanup(server.Close)
    return server
}
```

### 4. Fixture Organization

```
tests/
└── fixtures/
    ├── html/           # HTML samples for testing
    │   ├── simple.html
    │   ├── spa.html
    │   └── markdown.html
    └── sitemap/        # Sitemap samples
        ├── small.xml
        └── index.xml
```

## Coverage Targets

- **Minimum**: 80% per package
- **Target**: 90% per package
- **Excellent**: 95%+ per package

Packages below 80% require attention and additional test coverage.

## CI/CD Integration

Tests run automatically on:
- Pull request creation
- Push to main branch
- Scheduled nightly builds

Coverage badges are updated automatically based on test results.

## Best Practices

1. **Write tests first** (TDD when possible)
2. **Keep tests fast** - Use `-short` flag for quick feedback
3. **Use descriptive test names** - `TestFunction_Scenario`
4. **Table-driven for multiple cases** - Better maintainability
5. **Mock external dependencies** - Tests should be deterministic
6. **Clean up resources** - Use `t.Cleanup()` and `defer`
7. **Test error paths** - Not just happy paths
8. **Add benchmarks** - For performance-critical code

## Example: Adding New Tests

```go
// 1. Identify function to test
func MyFunction(input string) (string, error)

// 2. Create test file: tests/unit/mypackage/mypackage_test.go
package mypackage_test

import (
    "testing"
    "github.com/stretchr/testify/assert"
    "github.com/quantmind-br/repodocs-go/internal/mypackage"
)

// 3. Write test
func TestMyFunction(t *testing.T) {
    result, err := mypackage.MyFunction("test")
    assert.NoError(t, err)
    assert.Equal(t, "expected", result)
}

// 4. Run test
// go test ./tests/unit/mypackage/ -v
```

## Coverage Reports

Generate detailed coverage reports:

```bash
# Function coverage
go test ./internal/... -coverprofile=coverage.out
go tool cover -func=coverage.out | grep -v "100.0%"

# HTML report
go tool cover -html=coverage.out -o coverage.html

# By package
go test ./internal/... -coverprofile=coverage.out
go tool cover -func=coverage.out | grep "^github.com"
```

## Troubleshooting

### Tests Timing Out
- Use `-short` flag to skip integration tests
- Check for infinite loops in goroutines
- Add explicit timeouts to contexts

### Browser Tests Failing
- Ensure Chrome/Chromium is installed
- Check `renderer.IsAvailable()` before running
- Use `testing.Short()` to skip when browser unavailable

### Cache Issues
- Clear cache: `rm -rf ~/.repodocs-cache`
- Run with cache disabled: `REPODOCS_CACHE=off ./repodocs`

### Mock Generation
```bash
# Install mockgen
go install go.uber.org/mock/mockgen@latest

# Generate mocks
go generate ./...
```

## Additional Resources

- [Effective Go Testing](https://go.dev/doc/effective_go.html#testing)
- [Table-Driven Tests](https://dave.cheney.net/2019/03/04/table-driven-tests-in-go)
- [Test Coverage](https://blog.golang.org/cover/go14)
- [Go Mock](https://github.com/golang/mock)
</file>


---

## Analysis Categories

### 1. Bundle Size
- Large dependencies that could be replaced
- Unused exports and dead code
- Missing tree-shaking opportunities
- Duplicate dependencies
- Unoptimized assets (images, fonts)

### 2. Runtime Performance
- Inefficient algorithms (O(n^2) when O(n) possible)
- Unnecessary computations in hot paths
- Blocking operations on main thread
- Missing memoization opportunities
- Expensive regular expressions
- Synchronous I/O operations

### 3. Memory Usage
- Memory leaks (event listeners, closures, timers)
- Unbounded caches or collections
- Large object retention
- Missing cleanup in components
- Inefficient data structures

### 4. Database Performance
- N+1 query problems
- Missing indexes
- Unoptimized queries
- Over-fetching data
- Missing query result limits

### 5. Network Optimization
- Missing request caching
- Unnecessary API calls
- Large payload sizes
- Missing compression
- Sequential requests that could be parallel

### 6. Rendering Performance
- Unnecessary re-renders
- Missing React.memo / useMemo / useCallback
- Large component trees
- Missing virtualization for lists
- Layout thrashing

### 7. Caching Opportunities
- Repeated expensive computations
- Cacheable API responses
- Static asset caching
- Build-time computation opportunities

---

## Impact Classification

| Impact | Description | User Experience |
|--------|-------------|-----------------|
| **high** | Major improvement visible to users | Significantly faster load/interaction |
| **medium** | Noticeable improvement | Moderately improved responsiveness |
| **low** | Minor improvement | Subtle, developer benefit |

---

## Output Format

Provide your analysis as structured JSON:

```json
{
  "performance_optimizations": [
    {
      "id": "perf-001",
      "title": "Replace moment.js with date-fns for 90% bundle reduction",
      "description": "The project uses moment.js (300KB) for simple date formatting. date-fns is tree-shakeable and would reduce the date utility footprint to ~30KB.",
      "rationale": "moment.js is the largest dependency and only 3 functions are used. Low-hanging fruit for bundle size reduction.",
      "category": "bundle_size|runtime|memory|database|network|rendering|caching",
      "impact": "high|medium|low",
      "affected_areas": ["src/utils/date.ts", "package.json"],
      "current_metric": "Bundle includes 300KB for moment.js",
      "expected_improvement": "~270KB reduction in bundle size, ~20% faster initial load",
      "implementation": "1. Install date-fns\n2. Replace moment imports\n3. Update format strings\n4. Remove moment.js",
      "tradeoffs": "date-fns format strings differ from moment.js",
      "estimated_effort": "trivial|small|medium|large"
    }
  ],
  "summary": {
    "files_analyzed": 0,
    "issues_by_category": {},
    "issues_by_impact": {},
    "estimated_total_savings": ""
  }
}
```

---

## Common Anti-Patterns

### Bundle Size
```javascript
// BAD: Importing entire library
import _ from 'lodash';
_.map(arr, fn);

// GOOD: Import only what's needed
import map from 'lodash/map';
map(arr, fn);
```

### Runtime Performance
```javascript
// BAD: O(n^2) when O(n) is possible
users.forEach(user => {
  const match = allPosts.find(p => p.userId === user.id);
});

// GOOD: O(n) with map lookup
const postsByUser = new Map(allPosts.map(p => [p.userId, p]));
users.forEach(user => {
  const match = postsByUser.get(user.id);
});
```

### React Rendering
```jsx
// BAD: New function on every render
<Button onClick={() => handleClick(id)} />

// GOOD: Memoized callback
const handleButtonClick = useCallback(() => handleClick(id), [id]);
<Button onClick={handleButtonClick} />
```

### Database Queries
```sql
-- BAD: N+1 query pattern
SELECT * FROM users;
-- Then for each user:
SELECT * FROM posts WHERE user_id = ?;

-- GOOD: Single query with JOIN
SELECT u.*, p.* FROM users u
LEFT JOIN posts p ON p.user_id = u.id;
```

---

## Performance Budgets

Consider these common targets:
- Time to Interactive: < 3.8s
- First Contentful Paint: < 1.8s
- Largest Contentful Paint: < 2.5s
- Total Blocking Time: < 200ms
- Bundle size: < 200KB gzipped (initial)

---

## Guidelines

1. **Measure First**: Suggest profiling before and after when possible
2. **Quantify Impact**: Include expected improvements (%, ms, KB)
3. **Consider Tradeoffs**: Note any downsides (complexity, maintenance)
4. **Prioritize User Impact**: Focus on user-facing performance
5. **Avoid Premature Optimization**: Don't suggest micro-optimizations

---

## Instructions

1. Analyze package dependencies for bundle bloat
2. Search for common anti-patterns in the code
3. Identify algorithmic inefficiencies
4. Look for React/component optimization opportunities
5. Check for caching and memoization gaps
6. Output the structured JSON with your findings

Begin your analysis now.
