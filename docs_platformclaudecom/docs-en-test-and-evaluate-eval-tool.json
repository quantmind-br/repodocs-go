{
  "url": "https://platform.claude.com/docs/en/test-and-evaluate/eval-tool.md",
  "title": "Using the Evaluation Tool",
  "description": "The [Claude Console](/dashboard) features an **Evaluation tool** that allows you to test your prompts under various scenarios.",
  "fetched_at": "2026-01-01T02:08:44.585239264-03:00",
  "content_hash": "991c735f889158503321480f0e42c637be9a0974c060c04555dfcf92930fca27",
  "word_count": 428,
  "char_count": 2893,
  "links": [
    "https://platform.claude.com/dashboard",
    "https://platform.claude.com/docs/images/access_evaluate.png",
    "https://platform.claude.com/docs/en/build-with-claude/prompt-engineering/prompt-generator",
    "https://platform.claude.com/docs/images/promptgenerator.png",
    "https://platform.claude.com/docs/images/eval_populated.png"
  ],
  "headers": {
    "h1": [
      "Using the Evaluation Tool"
    ],
    "h2": [
      "Accessing the Evaluate Feature",
      "Generating Prompts",
      "Creating Test Cases",
      "Tips for Effective Evaluation",
      "Understanding and comparing results"
    ]
  },
  "rendered_with_js": false,
  "source_strategy": "llms",
  "cache_hit": true,
  "summary": "Document explains how to use Claude's Evaluation tool for testing prompts with dynamic variables, generating test cases, and refining AI responses through structured prompt engineering.",
  "tags": [
    "evaluation-tool",
    "prompt-engineering",
    "dynamic-variables",
    "test-case-generation",
    "ai-response-refinement"
  ],
  "category": "guide"
}